{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMzKbwugepgQJ1CR8FeueIC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/DQN_Merc_9_Avr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xD7IsKgXYb2k",
        "outputId": "ec6a5fdb-92d0-4261-e561-ed70ead3d8c8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6M92YNE9Nna"
      },
      "source": [
        "## Configuration for Colab"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lN_8Aor1eXSp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GyYmCpH89Nnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7564527-3543-43be-a46a-e2a7ea5751ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium==1.0.0\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gymnasium\n",
            "  Attempting uninstall: gymnasium\n",
            "    Found existing installation: gymnasium 1.1.1\n",
            "    Uninstalling gymnasium-1.1.1:\n",
            "      Successfully uninstalled gymnasium-1.1.1\n",
            "Successfully installed gymnasium-1.0.0\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwLhlnim9Nnc"
      },
      "source": [
        "## import module"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ViHWc_8Fern_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/Neilus03/DRL_comparative_study"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "svi6umkee3r4",
        "outputId": "2056e421-67f5-4952-c53c-8f5858efb0b0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-4-2d3d2f31762b>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-2d3d2f31762b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https://github.com/Neilus03/DRL_comparative_study\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Breakout_sb3_DQN"
      ],
      "metadata": {
        "id": "zp8I0qPYfn-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Breakout_sb3_DQN/config.py"
      ],
      "metadata": {
        "id": "K30J6Q_Bftot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.common.utils import get_latest_run_id\n",
        "import torch\n",
        "\n",
        "\n",
        "'''FILE TO STORE ALL THE CONFIGURATION VARIABLES'''\n",
        "\n",
        "#pretrained is a boolean that indicates if a pretrained model will be loaded\n",
        "pretrained = False # Set to True if you want to load a pretrained model\n",
        "\n",
        "#check_freq is the frequency at which the callback is called, in this case, the callback is called every 2000 timesteps\n",
        "check_freq = 2000\n",
        "\n",
        "#save_path is the path where the best model will be saved\n",
        "save_path = \"./breakout_DQN_1M_save_path\"\n",
        "\n",
        "#log_dir is the path where the logs will be saved\n",
        "log_dir = \"./log_dir\"\n",
        "\n",
        "\n",
        "'''\n",
        "Hyperparameters of the model {policy, learning_rate, buffer_size, learning_starts, batch_size, tau, gamma, train_freq, gradient_steps, replay_buffer_class, replay_buffer_kwargs, optimize_memory_usage, target_update_interval, exploration_fraction, exploration_initial_eps, exploration_final_eps, max_grad_norm, stats_window_size, tensorboard_log, policy_kwargs, verbose, seed, device, _init_setup_model, total_timesteps, log_interval}\n",
        "'''\n",
        "#policy is the policy of the model, in this case, the model will use a convolutional neural network\n",
        "policy = \"CnnPolicy\"\n",
        "\n",
        "#learning_rate is the learning rate of the model\n",
        "learning_rate =5e-4  #first trial: 5e-4   #second trial: 1e-4  #third trial: 1e-3  #fourth trial: 5e-5 #fifth trial: 5e-5 gamma = 0.90 #sixth trial: 1e-4 gamma = 0.90 #seventh trial: 5e-4 gamma = 0.90\n",
        "\n",
        "# buffer_size is the size of the replay buffer\n",
        "buffer_size=100000\n",
        "\n",
        "#learning_starts is the number of timesteps before the first interaction with the environment\n",
        "learning_starts=50000\n",
        "\n",
        "#batch_size is the number of samples that will be taken from the replay buffer for training the model\n",
        "batch_size=64\n",
        "\n",
        "#tau is the soft update coefficient for updating the target network if set to 1 then the target network is hard updated every target_update_interval timesteps\n",
        "tau= 1.0\n",
        "\n",
        "#gamma is the discount factor\n",
        "gamma=0.99\n",
        "\n",
        "#train_freq is the number of timesteps between each training step\n",
        "train_freq=4\n",
        "\n",
        "#gradient_steps is the number of gradient steps to take after each rollout\n",
        "gradient_steps=1\n",
        "\n",
        "#replay_buffer_class is the class of the replay buffer\n",
        "replay_buffer_class=None\n",
        "\n",
        "#replay_buffer_kwargs is the keyword arguments for the replay buffer\n",
        "replay_buffer_kwargs=None\n",
        "\n",
        "#optimize_memory_usage is a boolean that indicates if the memory usage will be optimized\n",
        "optimize_memory_usage=False\n",
        "\n",
        "#target_update_interval is the number of timesteps between each target network update\n",
        "target_update_interval=10000\n",
        "\n",
        "#exploration_fraction is the fraction of the total number of timesteps that the exploration rate will be annealed\n",
        "exploration_fraction=0.1\n",
        "\n",
        "#exploration_initial_eps is the initial value of the exploration rate\n",
        "exploration_initial_eps=1.0\n",
        "\n",
        "#exploration_final_eps is the final value of the exploration rate\n",
        "exploration_final_eps=0.05\n",
        "\n",
        "#max_grad_norm is the maximum value of the gradient norm\n",
        "max_grad_norm=0.5\n",
        "\n",
        "#stats_window_size is the size of the window for computing the stats\n",
        "stats_window_size=100\n",
        "\n",
        "#tensorboard_log is the path to the folder where the tensorboard logs will be saved\n",
        "tensorboard_log=log_dir\n",
        "\n",
        "#policy_kwargs is the keyword arguments for the policy\n",
        "policy_kwargs=None\n",
        "\n",
        "#verbose is the verbosity level: 0 no output, 1 info, 2 debug\n",
        "verbose=2\n",
        "\n",
        "#seed is the seed for the pseudo random number generator\n",
        "seed=None\n",
        "\n",
        "#device is the device on which the model will be trained\n",
        "device= \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#_init_setup_model is a boolean that indicates if the model will be initialized\n",
        "_init_setup_model=True\n",
        "\n",
        "#total_timesteps is the total number of timesteps that the model will be trained. In this case, the model will be trained for 1e7 timesteps\n",
        "#Take into account that the number of timesteps is not the number of episodes, in a game like breakout, the agent takes an action every frame,\n",
        "# then the number of timesteps is the number of frames, which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e7 timesteps is 1000 games more or less.\n",
        "total_timesteps = int(3e7)\n",
        "\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "log_interval = 100\n",
        "\n",
        "'''\n",
        "Saved model path\n",
        "'''\n",
        "\n",
        "#for the path to be shorter just put \"./a2c_Breakout_1M.zip\" instead of the full path\n",
        "saved_model_path = \"./DQN_Breakout_30M_lr_5e-4_gamma_90.zip\"\n",
        "unzip_file_path =  \"./DQN_Breakout_30M_lr_5e-4_gamma_90_unzipped\"\n",
        "\n",
        "'''\n",
        "Environment variables\n",
        "'''\n",
        "#n_stack is the number of frames stacked together to form the input to the model\n",
        "n_stack = 4\n",
        "#n_envs is the number of environments that will be run in parallel\n",
        "n_envs = 4\n",
        "\n",
        "'''\n",
        "Wandb configuration\n",
        "'''\n",
        "#log_to_wandb is a boolean that indicates if the training process will be logged to wandb\n",
        "log_to_wandb = False\n",
        "\n",
        "# project is the name of the project in wandb\n",
        "project_train = \"BREAKOUT_SB3_BENCHMARK\"\n",
        "project_test = \"breakout-sb3-DQN-test\"\n",
        "\n",
        "#entity is the name of the team in wandb\n",
        "entity = \"ai42\"\n",
        "\n",
        "#name is the name of the run in wandb\n",
        "name_train = \"DQN_breakout_lr_5e-4_gamma_90\"\n",
        "name_test = \"DQN_breakout_test\"\n",
        "#notes is a description of the run\n",
        "notes = \"DQN_breakout with parameters: {}\".format(locals()) #locals() returns a dictionary with all the local variables, in this case, all the variables in this file\n",
        "#sync_tensorboard is a boolean that indicates if the tensorboard logs will be synced to wandb\n",
        "sync_tensorboard = True\n",
        "\n",
        "\n",
        "'''\n",
        "Test configuration\n",
        "'''\n",
        "test_episodes = 100"
      ],
      "metadata": {
        "id": "8Yx2cQQCgKqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Breakout_sb3_DQN/train_DQN.py"
      ],
      "metadata": {
        "id": "OmuXuUAkh76x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack\n",
        "import torch\n",
        "import config\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from utils import unzip_file, CustomWandbCallback\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the appropriate directories for logging and saving the model\n",
        "'''\n",
        "os.makedirs(config.log_dir, exist_ok=True)\n",
        "os.makedirs(config.save_path, exist_ok=True)\n",
        "\n",
        "#Create the callback that logs the mean reward of the last 100 episodes to wandb\n",
        "custom_callback = CustomWandbCallback(config.check_freq, config.save_path)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up loging to wandb\n",
        "'''\n",
        "#Set wandb to log the training process\n",
        "if config.log_to_wandb:\n",
        "    #Set wandb to log the training process\n",
        "    wandb.init(project=config.project_train, entity = config.entity, name=config.name_train, notes=config.notes, sync_tensorboard=config.sync_tensorboard)\n",
        "    #wandb_callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "    wandb_callback = WandbCallback()\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment\n",
        "'''\n",
        "# Create multiple environments and wrap them correctly\n",
        "env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=config.n_envs, seed=config.seed)\n",
        "env = VecFrameStack(env, n_stack=config.n_stack)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the model\n",
        "'''\n",
        "#Create the model with the parameters specified in config.py, go to config.py to see the meaning of each parameter in detail\n",
        "model = DQN(policy=config.policy,\n",
        "            env=env,\n",
        "            learning_rate=config.learning_rate,\n",
        "            buffer_size=config.buffer_size,\n",
        "            learning_starts=config.learning_starts,\n",
        "            batch_size=config.batch_size,\n",
        "            tau=config.tau,\n",
        "            gamma=config.gamma,\n",
        "            train_freq=config.train_freq,\n",
        "            gradient_steps=config.gradient_steps,\n",
        "            replay_buffer_class=config.replay_buffer_class,\n",
        "            replay_buffer_kwargs=config.replay_buffer_kwargs,\n",
        "            optimize_memory_usage=config.optimize_memory_usage,\n",
        "            target_update_interval=config.target_update_interval,\n",
        "            exploration_fraction=config.exploration_fraction,\n",
        "            exploration_initial_eps=config.exploration_initial_eps,\n",
        "            exploration_final_eps=config.exploration_final_eps,\n",
        "            max_grad_norm=config.max_grad_norm,\n",
        "            tensorboard_log=config.log_dir,\n",
        "            policy_kwargs=config.policy_kwargs,\n",
        "            verbose=config.verbose,\n",
        "            seed=config.seed,\n",
        "            device=config.device,\n",
        "            _init_setup_model=config._init_setup_model)\n",
        "\n",
        "\n",
        "print(\"model in device: \", model.device)\n",
        "\n",
        "#Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    model = DQN.load(config.saved_model_path, env=env, verbose=config.verbose, tensorboard_log=config.log_dir)\n",
        "    #Unzip the file a2c_Breakout_1M.zip and store the unzipped files in the folder DQN_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "'''\n",
        "Train the model and save it\n",
        "'''\n",
        "#model.learn will train the model for 1e6 timesteps, timestep is the number of actions taken by the agent,\n",
        "# in a game like breakout, the agent takes an action every frame, then the number of timesteps is the number of frames,\n",
        "# which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e6 timesteps is 1000 games more or less.\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "#callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "\n",
        "if config.log_to_wandb:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=[wandb_callback, custom_callback], progress_bar=True)\n",
        "else:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=[custom_callback], progress_bar=True)\n",
        "\n",
        "#Save the model\n",
        "model.save(config.saved_model_path[:-4]) #remove the .zip extension from the path\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "suKaLD1tiFz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Breakout_sb3_DQN/test_DQN.py"
      ],
      "metadata": {
        "id": "oRt73Mwnic0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import config\n",
        "from utils import make_env, unzip_file\n",
        "\n",
        "\n",
        "'''\n",
        "Set up wandb\n",
        "'''\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.name_test, entity= config.entity, sync_tensorboard=config.sync_tensorboard, name=config.name_test, notes=config.notes)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment and the model to test\n",
        "'''\n",
        "if config.pretrained:\n",
        "    #Unzip the file DQN_Breakout_1M.zip and store the unzipped files in the folder DQN_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "\n",
        "#We start with a single environment for Breakout with render mode set to human\n",
        "env = make_env(\"BreakoutNoFrameskip-v4\")\n",
        "#We then wrap the environment with the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "env = DummyVecEnv([env]) # Output shape: (1, 84, 84)\n",
        "#Finally, we wrap the environment with the VecFrameStack wrapper which stacks the observations over the last 4 frames\n",
        "env = VecFrameStack(env, n_stack=config.n_stack) # Output shape: (4, 84, 84)\n",
        "\n",
        "# Create the model\n",
        "model = DQN(policy = config.policy\n",
        "            ,env = env\n",
        "            ,verbose = config.verbose)\n",
        "\n",
        "\n",
        "# Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    # Load the model components, including the policy network and the value network\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "'''\n",
        "Test the model in the environment and log the results to wandb\n",
        "'''\n",
        "# Run the episodes and render the gameplay\n",
        "for episode in range(config.test_episodes):\n",
        "    # Reset the environment and stack the initial state 4 times\n",
        "    obs = env.reset()#obs = np.stack([obs] * 4, axis=0)  # Initial state stack\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        # Take an action in the environment according to the policy of the trained agent\n",
        "        action, _ = model.predict(obs)\n",
        "        # Take the action in the environment and store the results in the variables\n",
        "        obs, reward, done, info = env.step(action) # obs shape: (1, 84, 84), reward shape: (1,), done shape: (1,), info shape: (1,)\n",
        "        # Update the total reward\n",
        "        episode_reward += reward[0]\n",
        "        # Render the environment to visualize the gameplay of the trained agent\n",
        "        env.render()\n",
        "    # Log the total reward of the episode to wandb\n",
        "    if config.log_to_wandb:\n",
        "        wandb.log({'test_episode_reward': episode_reward, 'test_episode': episode})\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "GQ6wMWwLi2M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Breakout_sb3_DQN/utils.py"
      ],
      "metadata": {
        "id": "A5IO7HLojCVP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "import os\n",
        "import zipfile\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import wandb\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "'''\n",
        "The RewardLogger wrapper is used to log the rewards of each episode to wandb\n",
        "It makes sure that the rewards of each episode are stored in a list and that the current episode reward is reset\n",
        "'''\n",
        "class RewardLogger(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(RewardLogger, self).__init__(env)\n",
        "        # Store the rewards of each episode\n",
        "        self.episode_rewards = []\n",
        "        # Store the current episode reward\n",
        "        self.current_episode_reward = 0\n",
        "\n",
        "    # The step function is called every time the agent takes an action in the environment\n",
        "    def step(self, action):\n",
        "        # Call the step function of the environment and store the results\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # Update the current episode reward\n",
        "        self.current_episode_reward += reward\n",
        "        # If the episode is done, store the episode reward and reset the current episode reward\n",
        "        if done:\n",
        "            self.episode_rewards.append(self.current_episode_reward)\n",
        "            self.current_episode_reward = 0\n",
        "        # Return the results as in a normal step function\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    # The reset function is called every time the environment is reset (at the beginning of each episode)\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    # The get_episode_rewards function returns the rewards of each episode\n",
        "    def get_episode_rewards(self):\n",
        "        return self.episode_rewards\n",
        "\n",
        "'''\n",
        "The CustomWandbCallback is a callback* that logs the mean reward of the last 100 episodes to wandb.\n",
        "A callback is a function that is called at the end of each episode to perform some action,\n",
        "in this case, the action is logging the mean reward of the last 100 episodes to wandb.\n",
        "'''\n",
        "class CustomWandbCallback(BaseCallback):\n",
        "    def __init__(self, check_freq, save_path, verbose=1):\n",
        "        super(CustomWandbCallback, self).__init__(verbose)\n",
        "        # Define the frequency at which the callback is called\n",
        "        self.check_freq = check_freq\n",
        "        # Define the path where the best model will be saved\n",
        "        self.save_path = save_path\n",
        "        # Define the best mean reward as -inf\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        '''\n",
        "        The _on_step function is called at the end of each episode.\n",
        "        It returns True if the callback should be called again, and False otherwise.\n",
        "        To do this, it checks if the number of calls to the callback is a multiple of the check_freq.\n",
        "        If it is, it computes the mean reward of the last 100 episodes and logs it to wandb.\n",
        "        It also saves the model if the mean reward is greater than the best mean reward.\n",
        "        '''\n",
        "        # Check if the number of calls to the callback is a multiple of the check_freq\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            # Gather rewards from all environments, by all environments we mean all the environments in the vectorized environment, usually there is only 1 environment in the vectorized environment\n",
        "            all_rewards = []\n",
        "            for env in self.training_env.envs: # self.training_env is the vectorized environment\n",
        "                # logger_env is the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "                logger_env = env.envs[0] if isinstance(env, DummyVecEnv) else env # env.envs[0] is the AtariWrapper which wraps the environment correctly\n",
        "                #Check if the logger_env is the RewardLogger wrapper\n",
        "                if isinstance(logger_env, RewardLogger):\n",
        "                    # If it is, get the rewards of each episode and store them in all_rewards\n",
        "                    all_rewards.extend(logger_env.get_episode_rewards())#extend is used to add the elements of a list to another list\n",
        "\n",
        "            #If there are rewards in all_rewards, compute the mean reward of the last 100 episodes and log it to wandb\n",
        "            if all_rewards:\n",
        "                # Compute the mean reward of the last 100 episodes\n",
        "                mean_reward = np.mean(all_rewards[-self.check_freq:])\n",
        "                # Log the mean reward of the last 100 episodes to wandb\n",
        "                wandb.log({'mean_reward': mean_reward, 'steps': self.num_timesteps})\n",
        "\n",
        "                # Save the best model\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    self.model.save(os.path.join(self.save_path, 'best_model'))\n",
        "        # Return True if the callback should be called again, and False otherwise\n",
        "        return True"
      ],
      "metadata": {
        "id": "LU4Ypt2-jTGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "README.md"
      ],
      "metadata": {
        "id": "YB6mok_LjXA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "CTaebj5wjllh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0vNOqtI2UcR9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python train_DQN.py"
      ],
      "metadata": {
        "id": "SQIpEjp-kBTB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config.py"
      ],
      "metadata": {
        "id": "o-m9y13aU2-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_DQN.py"
      ],
      "metadata": {
        "id": "ockpHf10Uogd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "utils.py"
      ],
      "metadata": {
        "id": "HVf0285PUwAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RewardLogger"
      ],
      "metadata": {
        "id": "a4lulcHSUZDV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
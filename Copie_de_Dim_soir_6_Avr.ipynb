{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdCzoWdF4GWmX8yvvXy3sl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/Copie_de_Dim_soir_6_Avr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6M92YNE9Nna"
      },
      "source": [
        "## Configuration for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "GyYmCpH89Nnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "871f4880-2ce2-4492-a784-ba9065402d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwLhlnim9Nnc"
      },
      "source": [
        "## import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from cpu_thread import cpu_thread\n",
        "from gpu_thread import gpu_thread\n",
        "import os\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "mp.set_sharing_strategy('file_system')\n",
        "\n",
        "\n",
        "def str2bool(v):\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(description='PPO training')\n",
        "parser.add_argument('--load', default=False,\n",
        "                    help='Whether or not to load pretrained weights. '\n",
        "                         'You must have started an alread trained net for it to work',\n",
        "                    dest='load', type=str2bool)\n",
        "parser.add_argument('--render', default=True, help='Show the game running in a separate process. '\n",
        "                                                   'This slows the training a bit',\n",
        "                    dest='render', type=str2bool)\n",
        "parser.add_argument('--gif', default=False, help='Create a gif of a game every once in a while',\n",
        "                    dest='gif', type=str2bool)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    assert args.render or not args.gif, 'If you want to display a gif, you must set render to true'\n",
        "    if args.load is False and os.path.isfile('./model/breakout.pt'):\n",
        "        while True:\n",
        "            load = input('Are you sure you want to erase the previous training? (y/n) ')\n",
        "            if load.lower() in ('y', 'yes', '1'):\n",
        "                break\n",
        "            elif load.lower() in ('n', 'no', '0'):\n",
        "                import sys\n",
        "                sys.exit()\n",
        "\n",
        "    # create shared variables between all the processes\n",
        "    manager = mp.Manager()\n",
        "    # used to send the results of the net\n",
        "    common_dict = manager.dict()\n",
        "    # a queue of batches to be fed to the training net\n",
        "    mem_queue = manager.Queue(1500 * mp.cpu_count())\n",
        "    # a queue of operations pending\n",
        "    process_queue = manager.Queue(mp.cpu_count()-1)\n",
        "\n",
        "    with mp.Pool() as pool:\n",
        "        try:\n",
        "            workers: int = pool._processes\n",
        "            print(f\"Running pool with {workers//2} workers\")\n",
        "            pool.apply_async(gpu_thread, (args.load, mem_queue, process_queue, common_dict, [0, 1]))\n",
        "            if args.render:\n",
        "                pool.apply_async(cpu_thread, (2 if args.gif else 1, mem_queue, process_queue, common_dict, [2, 3]))\n",
        "            for i in range(2*(1+args.render), workers, 2):\n",
        "                pool.apply_async(cpu_thread, (0, mem_queue, process_queue, common_dict, [i, i+1]))\n",
        "\n",
        "            # Wait for children to finish\n",
        "            pool.close()\n",
        "            pool.join()\n",
        "        except KeyboardInterrupt:\n",
        "            pool.join()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parser.parse_args()\n",
        "    mp.set_start_method('spawn')\n",
        "    main(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "uzKgKaf6wsVp",
        "outputId": "6e007d6d-da17-4726-bb33-393cc4f2fc2f"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'cpu_thread'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-3bc5609d7ab7>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcpu_thread\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcpu_thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgpu_thread\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgpu_thread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmultiprocessing\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cpu_thread'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gym\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from parameters import parameters\n",
        "\n",
        "\n",
        "def sample(action):\n",
        "    tresh = random.random()\n",
        "    p = 0.\n",
        "    for i, prob in enumerate(action):\n",
        "        p += prob\n",
        "        if p > tresh:\n",
        "            return i, prob\n",
        "\n",
        "\n",
        "def process_frame(frame):\n",
        "    return (frame/255).transpose((2, 0, 1))\n",
        "\n",
        "\n",
        "def process_reward(reward):\n",
        "    return 0.01*reward\n",
        "\n",
        "\n",
        "def generate_game(env, pid, process_queue, common_dict):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    reward_list = []\n",
        "    action_list = []\n",
        "    prob_list = []\n",
        "    observation_list = []\n",
        "    frame_count = 0\n",
        "    live = 5\n",
        "    while not done:\n",
        "        observation = process_frame(observation)\n",
        "        observation_list.append(observation)\n",
        "        process_queue.put((pid, observation))\n",
        "        while pid not in common_dict:\n",
        "            time.sleep(0.0001)\n",
        "        action_prob = common_dict[pid]\n",
        "        del common_dict[pid]\n",
        "        action, prob = sample(action_prob)\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        if info['ale.lives'] < live:\n",
        "            live = info['ale.lives']\n",
        "            reward = -1\n",
        "        action_list.append(action)\n",
        "        prob_list.append(prob)\n",
        "        reward_list.append(process_reward(reward))\n",
        "        frame_count += 1\n",
        "    print(\"Score: {:4.0f}\".format(100*sum(reward_list)))\n",
        "    for i in range(len(reward_list) - 2, -1, -1):\n",
        "        reward_list[i] += reward_list[i + 1] * parameters.GAMMA  # compute the discounted obtained reward for each step\n",
        "    return observation_list, reward_list, action_list, prob_list\n",
        "\n",
        "\n",
        "def play(env, pid, process_queue, common_dict):\n",
        "    while True:\n",
        "        counter = 0\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            counter += 1\n",
        "            if counter >= 2000:\n",
        "                break\n",
        "            process_queue.put((pid, process_frame(observation)))\n",
        "            while pid not in common_dict:\n",
        "                time.sleep(0.0001)\n",
        "            action_prob = common_dict[pid]\n",
        "            del common_dict[pid]\n",
        "            action = sample(action_prob)[0]\n",
        "            observation, _, done, _ = env.step(action)\n",
        "            env.render()\n",
        "\n",
        "\n",
        "def play_to_gif(env, pid, process_queue, common_dict):\n",
        "    display = False\n",
        "    episode = 0\n",
        "    while 'epoch' not in common_dict:\n",
        "        time.sleep(0.001)\n",
        "    while True:\n",
        "        if common_dict['epoch'] % 25 == 0 and not display:\n",
        "            display = True\n",
        "            episode = common_dict['epoch']\n",
        "        observation = env.reset()\n",
        "        frames = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            process_queue.put((pid, process_frame(observation)))\n",
        "            while pid not in common_dict:\n",
        "                time.sleep(0.0001)\n",
        "            action_prob = common_dict[pid]\n",
        "            del common_dict[pid]\n",
        "            action = sample(action_prob)[0]\n",
        "            observation, _, done, _ = env.step(action)\n",
        "            if display:\n",
        "                frames.append(env.render(mode='rgb_array'))\n",
        "            else:\n",
        "                env.render()\n",
        "        if display:\n",
        "            display_frames_as_gif(frames, 'Episode {}.gif'.format(episode))\n",
        "            display = False\n",
        "\n",
        "\n",
        "def display_frames_as_gif(frames, name):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=33)\n",
        "    try:\n",
        "        anim.save('gifs/' + name)\n",
        "    except:\n",
        "        anim.save('gifs/' + name, writer=animation.PillowWriter(fps=40))\n",
        "\n",
        "\n",
        "def cpu_thread(render, memory_queue, process_queue, common_dict, workers):\n",
        "    import psutil\n",
        "    p = psutil.Process()\n",
        "    p.cpu_affinity(workers)\n",
        "    import signal\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    try:\n",
        "        env = gym.make('Breakout-v0')\n",
        "        pid = os.getpid()\n",
        "        print('process started with pid: {} on core(s) {}'.format(os.getpid(), workers), flush=True)\n",
        "        if render == 1:\n",
        "            play(env, pid, process_queue, common_dict)\n",
        "        elif render == 2:\n",
        "            play_to_gif(env, pid, process_queue, common_dict)\n",
        "        else:\n",
        "            while True:\n",
        "                observation_list, reward_list, action_list, prob_list = generate_game(env, pid, process_queue, common_dict)\n",
        "                memory_queue.put((observation_list, reward_list, action_list, prob_list))\n",
        "    except Exception as e:\n",
        "        print(e, flush=True)"
      ],
      "metadata": {
        "id": "xqc9GmTzxEUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from model import PPO\n",
        "import torch.optim as optim\n",
        "from parameters import parameters\n",
        "\n",
        "\n",
        "def process_observations(observation, model):\n",
        "    with torch.no_grad():\n",
        "        action = model.forward(observation)\n",
        "        return action.cpu().squeeze().numpy()\n",
        "\n",
        "\n",
        "def destack_process(model, process_queue, common_dict):\n",
        "    if process_queue.qsize() > 0:\n",
        "        model.eval()\n",
        "        for _ in range(process_queue.qsize()):  # for instead of while to not get stuck\n",
        "            pid, obs = process_queue.get(True)\n",
        "            action = process_observations(torch.Tensor(obs).unsqueeze(0).to(parameters.DEVICE), model)\n",
        "            common_dict[pid] = action\n",
        "\n",
        "\n",
        "def destack_memory(memory_queue, observations, rewards, actions, probs):\n",
        "    while memory_queue.qsize() > 0 and len(observations) <= parameters.MAXLEN:\n",
        "        try:\n",
        "            _, __, ___, ____ = memory_queue.get(True)\n",
        "            observations = torch.cat((observations, torch.Tensor(_).to(parameters.DEVICE)))\n",
        "            rewards = torch.cat((rewards, torch.Tensor(__).to(parameters.DEVICE)))\n",
        "            actions = torch.cat((actions, torch.LongTensor(___).to(parameters.DEVICE)))\n",
        "            probs = torch.cat((probs, torch.Tensor(____).to(parameters.DEVICE)))\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "            return True, observations, rewards, actions, probs\n",
        "    return False, observations, rewards, actions, probs\n",
        "\n",
        "\n",
        "def run_epoch(epochs, model, optimizer, observations, rewards, actions, probs):\n",
        "    model.train()\n",
        "    for _ in range(parameters.EPOCH_STEPS):\n",
        "        perm = torch.randperm(len(probs))\n",
        "        for i in range(0, len(probs), parameters.BATCH_SIZE):\n",
        "            if i + parameters.BATCH_SIZE > len(probs):\n",
        "                break\n",
        "            optimizer.zero_grad()\n",
        "            lossactor, losscritic = model.loss(observations[perm[i:i+parameters.BATCH_SIZE]], rewards[perm[i:i+parameters.BATCH_SIZE]], actions[perm[i:i+parameters.BATCH_SIZE]], probs[perm[i:i+parameters.BATCH_SIZE]])\n",
        "            if epochs > 10:\n",
        "                (lossactor + losscritic).backward()\n",
        "            else:\n",
        "                losscritic.backward()\n",
        "            optimizer.step()\n",
        "        print('Loss actor: {0:7.3f}  Loss critic: {1:7.3f}'.format(1000 * lossactor, 1000 * losscritic))\n",
        "\n",
        "\n",
        "def gpu_thread(load, memory_queue, process_queue, common_dict, workers):\n",
        "    # the only thread that has an access to the gpu, it will then perform all the NN computation\n",
        "    import psutil\n",
        "    p = psutil.Process()\n",
        "    p.cpu_affinity(workers)\n",
        "    import signal\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    try:\n",
        "        print('process started with pid: {} on core(s) {}'.format(os.getpid(), workers), flush=True)\n",
        "        model = PPO()\n",
        "        model.to(parameters.DEVICE)\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=parameters.LEARNING_RATE)\n",
        "        epochs = 0\n",
        "        if load:\n",
        "            checkpoint = torch.load('./model/breakout.pt')\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            epochs = checkpoint['epochs']\n",
        "        observations = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        rewards = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        actions = torch.LongTensor([]).to(parameters.DEVICE)\n",
        "        probs = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        common_dict['epoch'] = epochs\n",
        "        while True:\n",
        "            memory_full, observations, rewards, actions, probs = \\\n",
        "                destack_memory(memory_queue, observations, rewards, actions, probs)\n",
        "            destack_process(model, process_queue, common_dict)\n",
        "            if len(observations) > parameters.MAXLEN or memory_full:\n",
        "                epochs += 1\n",
        "                print('-' * 60 + '\\n        epoch ' + str(epochs) + '\\n' + '-' * 60)\n",
        "                run_epoch(epochs, model, optimizer, observations, rewards, actions, probs)\n",
        "                observations = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                rewards = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                actions = torch.LongTensor([]).to(parameters.DEVICE)\n",
        "                probs = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'epochs': epochs\n",
        "                }, './model/breakout.pt')\n",
        "                common_dict['epoch'] = epochs\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('saving before interruption', flush=True)\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epochs': epochs\n",
        "        }, './model/breakout.pt')"
      ],
      "metadata": {
        "id": "mcERaIhBxc9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.ACTOR_COEFF = 1.\n",
        "        self.LOSS_CLIPPING = 0.15\n",
        "        self.GAMMA = 0.98\n",
        "        self.DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCH_STEPS = 10\n",
        "        self.MAXLEN = 1000\n",
        "        self.LEARNING_RATE = 1e-4\n",
        "\n",
        "\n",
        "parameters = Parameters()"
      ],
      "metadata": {
        "id": "HmsiU_hmx4Aa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from parameters import parameters\n",
        "\n",
        "\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return torch.eye(num_classes)[y].to(parameters.DEVICE)\n",
        "\n",
        "\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPO, self).__init__()\n",
        "        self.vision = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, 3),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(6, 16, 3),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(16, 24, 5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(9384, 120),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(84, 4)\n",
        "        )\n",
        "\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(9384, 120),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(84, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        vision = self.vision(x).view(-1, 9384)\n",
        "        actor = self.actor(vision).view(-1, 4).softmax(-1)\n",
        "        if self.training:\n",
        "            critic = self.critic(vision)\n",
        "            return actor, critic.squeeze()\n",
        "        return actor\n",
        "\n",
        "    def loss(self, observations, rewards, actions, old_prob):\n",
        "        prob_distribution, reward_predicted = self.forward(observations)\n",
        "        r = (torch.sum(torch.eye(4)[actions].to(parameters.DEVICE) * prob_distribution, -1) + 1e-10) / (old_prob + 1e-10)\n",
        "        advantage = (rewards - reward_predicted).detach()\n",
        "        lossactor = - parameters.ACTOR_COEFF \\\n",
        "                    * torch.mean(torch.min(r * advantage,\n",
        "                                           torch.clamp(r,\n",
        "                                                       min=(1. - parameters.LOSS_CLIPPING),\n",
        "                                                       max=(1. + parameters.LOSS_CLIPPING))\n",
        "                                           * advantage))\n",
        "        losscritic = F.mse_loss(reward_predicted, rewards)\n",
        "        return lossactor, losscritic"
      ],
      "metadata": {
        "id": "vTSryE5DyPvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Concept :\n",
        "\n",
        "Forward :\n",
        "\n",
        "Découper l'écran en ~AxA parties\n",
        "\n",
        "Pour chaque, faire tourner un classifier sur B classes (AxAxB)\n",
        "+ des \"coordonnées\" (probablement un vecteur peu contraint de AxAxC)\n",
        "\n",
        "Sampler aléatoirement le link en dur choisi, on obtient AxA items\n",
        "\n",
        "Embedding B -> D (AxAxD)\n",
        "\n",
        "Coordonnées concat Embedding (AxAx(C+D))\n",
        "\n",
        "Actor critic après quelques couches linéaires + Leaky\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Difficultés à prévoir :\n",
        "\n",
        "Classification à plotter pour vérifier qu'il merde pas trop\n",
        "Chercher une façon de s'assurer une certaine distance entre les objets (?)\n",
        "\n",
        "Pistes d'améliorations :\n",
        "\n",
        "Chercher un moyen de découvrir les objets autrement que par AxA :\n",
        "\n",
        "RCNN Actor/Critic\n",
        "ce serait mieux si parfaitement exécuté (ce qui reste à PoC, mais me paraît impossible)\n",
        "\n",
        "Prédire direct coordonnées + classification + vecteur\n",
        "comment s'assurer que les coordonnées soient \"dans le bon ordre\" ? En donnant une seule conv peut-être\n",
        "comment s'assurer de l'unicité des classes ? Probablement un énorme problème\n",
        "\n",
        "Chaque classe cherches ses coordonnées + vecteur + \"importance\" elle-même\n",
        "comment linker 2 fois la même classe ? Probablement pas un énorme problème\n",
        "comment s'assurer de l'unicité des classes ? Probablement un énorme problème\n",
        "\n",
        "\n",
        "RCNN actor/critic :\n",
        "\n",
        "CNN\n",
        "\n",
        "ROI en mode actor/critic (PoC ?) Je peux me tromper, mais les anchors ne peuvent pas marcher en A/C\n",
        "\n",
        "ROI pooling\n",
        "\n",
        "coord (~bbox recentering) classification\n",
        "\n",
        "actor/critic\n",
        "\n",
        "\n",
        "RPN est impossible je pense car :\n",
        "- pas de IoU à feed en A/C\n",
        "- pas de continuité avec un nombre de pixel entier pour la bbox\n",
        "\n",
        "Fast RCNN au lieu de Faster RCNN ?\n",
        "ce serait lent mais probablement pas déconnant. Voir comment le ROI fonctionne."
      ],
      "metadata": {
        "id": "Iwp91Jh0yr70"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Pytorch multiprocessing PPO implementation playing Breakout\n",
        "How it works\n",
        "The optimization is a standard PPO implementation, however the point was to push the limits of what a limited computer could do in reinforcement learning. Thus I use multiple processes to play the game and gather experiences. However, if multiples processes try to access a single gpu, most of the computation time will be lost to each process waiting for their turn on the gpu, rather than actually playing the game, resulting in a very limited speedup between multiprocessed and not multiprocessed algorithms. Furthermore it necessitated the net to be copied on multiple processes, wich was very VRAM consuming.\n",
        "\n",
        "This algorithm works differently:\n",
        "\n",
        "multiple processes play the game\n",
        "a single process has access to the gpu\n",
        "when a playing process requires the gpu, it sends the operation to execute to the gpu process, and the gpu process sends back the result\n",
        "This way, the training can be around twice as fast for a computer with a single GPU compared to a naive multiprocessed PPO\n",
        "\n",
        "Requirements\n",
        "Pytorch\n",
        "Numpy\n",
        "gym (Atari)\n",
        "a few standard libraries such as argparse, time, os\n",
        "There is no guarantee this will work in python 2, or without a GPU\n",
        "around 2Gb of RAM for each core of your CPU with the recommended number of workers\n",
        "How to begin the training\n",
        "Clone this repository: git clone https://github.com/CSautier/Breakout\n",
        "Launch the game in a shell: python Breakout.py\n",
        "If you'd prefer a faster training, you can deactivate the visualization: python Breakout.py --render False\n",
        "Useful resources\n",
        "https://openai.com/\n",
        "https://arxiv.org/pdf/1707.06347.pdf\n",
        "Feel free to use as much of this code as you want but mention my github if you found this useful.\n",
        "For more information, you can contact me on my github."
      ],
      "metadata": {
        "id": "jhI3vHAWy-NF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/model/*.pt\n",
        "__pycache__/\n",
        ".*/"
      ],
      "metadata": {
        "id": "D9ThkmVQzcTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "+18\n",
        "-13\n",
        "\n",
        "\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -2,31 +2,36 @@\n",
        "\n",
        "## How it works\n",
        "\n",
        "The optimization is a standard PPO implementation, however the point was to push the limits of what a limited computer could do in reinforcement learning. Thus I use multiple processes to play the game and gather experiences. However, as my previous experiences taught me, if multiples processes try to access a single gpu, most of the computation time will be lost waiting for their turn on the gpu, rather than actually playing the game, resulting in a very limited speedup between multiprocessed and not multiprocessed algorithms. Furthermore it necessitated the net to be copied on multiple processes, wich was very VRAM consuming.\n",
        "This algorithm works differently.\n",
        "The optimization is a standard PPO implementation, however the point was to push the limits of what a limited computer\n",
        "could do in reinforcement learning. Thus I use multiple processes to play the game and gather experiences. However, if\n",
        "multiples processes try to access a single gpu, most of the computation time will be lost to each process waiting for\n",
        "their turn on the gpu, rather than actually playing the game, resulting in a very limited speedup between multiprocessed\n",
        "and not multiprocessed algorithms. Furthermore it necessitated the net to be copied on multiple processes, wich was very\n",
        "VRAM consuming. \\\n",
        "\\\n",
        "This algorithm works differently:\n",
        "* multiple processes play the game\n",
        "* a single process has access to the gpu\n",
        "* when a playing process requires the gpu, it sends the operation to execute to the gpu process, and the gpu process sends back the result\n",
        "* when a playing process requires the gpu, it sends the operation to execute to the gpu process, and the gpu process\n",
        "sends back the result\n",
        "\n",
        "This way, the speed limitation will go one step further, on your gpu or ram most likely\n",
        "This way, the training can be around twice as fast for a computer with a single GPU compared to a naive multiprocessed\n",
        "PPO\n",
        "\n",
        "## Requirements\n",
        "\n",
        "* Pytorch (gpu highly recommanded)\n",
        "* Pytorch\n",
        "* Numpy\n",
        "* gym (Atari)\n",
        "* a few standard libraries such as argparse, time, os (you most likely already have them)\n",
        "there might be a few modifications to make to run it in python 2\n",
        "# Before any execution\n",
        "Be careful, this requires a lot of ram, especially with many processes, so keep your ram in check and kill the program before it freezes your computer if necessary\n",
        "* a few standard libraries such as argparse, time, os\n",
        "* There is no guarantee this will work in python 2, or without a GPU\n",
        "* around 2Gb of RAM for each core of your CPU with the recommended number of workers\n",
        "\n",
        "## How to begin the training\n",
        "\n",
        "* Clone this repository: `git clone https://github.com/CSautier/Breakout`\n",
        "* Launch the game in bash: `python Breakout.py`\n",
        "* Launch the game in a shell: `python Breakout.py`\n",
        "* If you'd prefer a faster training, you can deactivate the visualization: `python Breakout.py --render False`\n",
        "\n",
        "## Useful resources\n",
        "\n",
        "‎breakout.py\n",
        "+72\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -0,0 +1,72 @@\n",
        "from cpu_thread import cpu_thread\n",
        "from gpu_thread import gpu_thread\n",
        "import os\n",
        "import argparse\n",
        "import torch.multiprocessing as mp\n",
        "mp.set_sharing_strategy('file_system')\n",
        "def str2bool(v):\n",
        "    if isinstance(v, bool):\n",
        "        return v\n",
        "    if v.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif v.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected.')\n",
        "parser = argparse.ArgumentParser(description='PPO training')\n",
        "parser.add_argument('--load', default=False,\n",
        "                    help='Whether or not to load pretrained weights. '\n",
        "                         'You must have started an alread trained net for it to work',\n",
        "                    dest='load', type=str2bool)\n",
        "parser.add_argument('--render', default=True, help='Show the game running in a separate process. '\n",
        "                                                   'This slows the training a bit',\n",
        "                    dest='render', type=str2bool)\n",
        "parser.add_argument('--gif', default=False, help='Create a gif of a game every once in a while',\n",
        "                    dest='gif', type=str2bool)\n",
        "def main(args):\n",
        "    assert args.render or not args.gif, 'If you want to display a gif, you must set render to true'\n",
        "    if args.load is False and os.path.isfile('./model/breakout.pt'):\n",
        "        while True:\n",
        "            load = input('Are you sure you want to erase the previous training? (y/n) ')\n",
        "            if load.lower() in ('y', 'yes', '1'):\n",
        "                break\n",
        "            elif load.lower() in ('n', 'no', '0'):\n",
        "                import sys\n",
        "                sys.exit()\n",
        "    # create shared variables between all the processes\n",
        "    manager = mp.Manager()\n",
        "    # used to send the results of the net\n",
        "    common_dict = manager.dict()\n",
        "    # a queue of batches to be fed to the training net\n",
        "    mem_queue = manager.Queue(1500 * mp.cpu_count())\n",
        "    # a queue of operations pending\n",
        "    process_queue = manager.Queue(mp.cpu_count()-1)\n",
        "    with mp.Pool() as pool:\n",
        "        try:\n",
        "            workers: int = pool._processes\n",
        "            print(f\"Running pool with {workers//2} workers\")\n",
        "            pool.apply_async(gpu_thread, (args.load, mem_queue, process_queue, common_dict, [0, 1]))\n",
        "            if args.render:\n",
        "                pool.apply_async(cpu_thread, (2 if args.gif else 1, mem_queue, process_queue, common_dict, [2, 3]))\n",
        "            for i in range(2*(1+args.render), workers, 2):\n",
        "                pool.apply_async(cpu_thread, (0, mem_queue, process_queue, common_dict, [i, i+1]))\n",
        "            # Wait for children to finish\n",
        "            pool.close()\n",
        "            pool.join()\n",
        "        except KeyboardInterrupt:\n",
        "            pool.join()\n",
        "if __name__ == \"__main__\":\n",
        "    args = parser.parse_args()\n",
        "    mp.set_start_method('spawn')\n",
        "    main(args)\n",
        "‎cpu_thread.py\n",
        "+141\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -0,0 +1,141 @@\n",
        "import os\n",
        "import gym\n",
        "import time\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation\n",
        "from parameters import parameters\n",
        "def sample(action):\n",
        "    tresh = random.random()\n",
        "    p = 0.\n",
        "    for i, prob in enumerate(action):\n",
        "        p += prob\n",
        "        if p > tresh:\n",
        "            return i, prob\n",
        "def process_frame(frame):\n",
        "    return (frame/255).transpose((2, 0, 1))\n",
        "def process_reward(reward):\n",
        "    return 0.01*reward\n",
        "def generate_game(env, pid, process_queue, common_dict):\n",
        "    observation = env.reset()\n",
        "    done = False\n",
        "    reward_list = []\n",
        "    action_list = []\n",
        "    prob_list = []\n",
        "    observation_list = []\n",
        "    frame_count = 0\n",
        "    while not done:\n",
        "        observation = process_frame(observation)\n",
        "        observation_list.append(observation)\n",
        "        process_queue.put((pid, observation))\n",
        "        while pid not in common_dict:\n",
        "            time.sleep(0.0001)\n",
        "        action_prob = common_dict[pid]\n",
        "        del common_dict[pid]\n",
        "        action, prob = sample(action_prob)\n",
        "        observation, reward, done, info = env.step(action)\n",
        "        action_list.append(action)\n",
        "        prob_list.append(prob)\n",
        "        reward_list.append(process_reward(reward))\n",
        "        frame_count += 1\n",
        "    print(\"Score: {:4.0f}\".format(100*sum(reward_list)))\n",
        "    for i in range(len(reward_list) - 2, -1, -1):\n",
        "        reward_list[i] += reward_list[i + 1] * parameters.GAMMA  # compute the discounted obtained reward for each step\n",
        "    return observation_list, reward_list, action_list, prob_list\n",
        "def play(env, pid, process_queue, common_dict):\n",
        "    while True:\n",
        "        counter = 0\n",
        "        observation = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            counter += 1\n",
        "            if counter >= 2000:\n",
        "                break\n",
        "            process_queue.put((pid, process_frame(observation)))\n",
        "            while pid not in common_dict:\n",
        "                time.sleep(0.0001)\n",
        "            action_prob = common_dict[pid]\n",
        "            del common_dict[pid]\n",
        "            action = sample(action_prob)[0]\n",
        "            observation, _, done, _ = env.step(action)\n",
        "            env.render()\n",
        "def play_to_gif(env, pid, process_queue, common_dict):\n",
        "    display = False\n",
        "    episode = 0\n",
        "    while 'epoch' not in common_dict:\n",
        "        time.sleep(0.001)\n",
        "    while True:\n",
        "        if common_dict['epoch'] % 25 == 0 and not display:\n",
        "            display = True\n",
        "            episode = common_dict['epoch']\n",
        "        observation = env.reset()\n",
        "        frames = []\n",
        "        done = False\n",
        "        while not done:\n",
        "            process_queue.put((pid, process_frame(observation)))\n",
        "            while pid not in common_dict:\n",
        "                time.sleep(0.0001)\n",
        "            action_prob = common_dict[pid]\n",
        "            del common_dict[pid]\n",
        "            action = sample(action_prob)[0]\n",
        "            observation, _, done, _ = env.step(action)\n",
        "            if display:\n",
        "                frames.append(env.render(mode='rgb_array'))\n",
        "            else:\n",
        "                env.render()\n",
        "        if display:\n",
        "            display_frames_as_gif(frames, 'Episode {}.gif'.format(episode))\n",
        "            display = False\n",
        "def display_frames_as_gif(frames, name):\n",
        "    \"\"\"\n",
        "    Displays a list of frames as a gif, with controls\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi=72)\n",
        "    patch = plt.imshow(frames[0])\n",
        "    plt.axis('off')\n",
        "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
        "    def animate(i):\n",
        "        patch.set_data(frames[i])\n",
        "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=33)\n",
        "    try:\n",
        "        anim.save('gifs/' + name)\n",
        "    except:\n",
        "        anim.save('gifs/' + name, writer=animation.PillowWriter(fps=40))\n",
        "def cpu_thread(render, memory_queue, process_queue, common_dict, workers):\n",
        "    import psutil\n",
        "    p = psutil.Process()\n",
        "    p.cpu_affinity(workers)\n",
        "    import signal\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    try:\n",
        "        env = gym.make('Breakout-v0')\n",
        "        pid = os.getpid()\n",
        "        print('process started with pid: {} on core(s) {}'.format(os.getpid(), workers), flush=True)\n",
        "        if render == 1:\n",
        "            play(env, pid, process_queue, common_dict)\n",
        "        elif render == 2:\n",
        "            play_to_gif(env, pid, process_queue, common_dict)\n",
        "        else:\n",
        "            while True:\n",
        "                observation_list, reward_list, action_list, prob_list = generate_game(env, pid, process_queue, common_dict)\n",
        "                memory_queue.put((observation_list, reward_list, action_list, prob_list))\n",
        "    except Exception as e:\n",
        "        print(e, flush=True)\n",
        "‎gpu_thread.py\n",
        "+102\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -0,0 +1,102 @@\n",
        "import os\n",
        "import torch\n",
        "from model import PPO\n",
        "import torch.optim as optim\n",
        "from parameters import parameters\n",
        "def process_observations(observation, model):\n",
        "    with torch.no_grad():\n",
        "        action = model.forward(observation)\n",
        "        return action.cpu().squeeze().numpy()\n",
        "def destack_process(model, process_queue, common_dict):\n",
        "    if process_queue.qsize() > 0:\n",
        "        model.eval()\n",
        "        for _ in range(process_queue.qsize()):  # for instead of while to not get stuck\n",
        "            pid, obs = process_queue.get(True)\n",
        "            action = process_observations(torch.Tensor(obs).unsqueeze(0).to(parameters.DEVICE), model)\n",
        "            common_dict[pid] = action\n",
        "def destack_memory(memory_queue, observations, rewards, actions, probs):\n",
        "    while memory_queue.qsize() > 0 and len(observations) <= parameters.MAXLEN:\n",
        "        try:\n",
        "            _, __, ___, ____ = memory_queue.get(True)\n",
        "            observations = torch.cat((observations, torch.Tensor(_).to(parameters.DEVICE)))\n",
        "            rewards = torch.cat((rewards, torch.Tensor(__).to(parameters.DEVICE)))\n",
        "            actions = torch.cat((actions, torch.LongTensor(___).to(parameters.DEVICE)))\n",
        "            probs = torch.cat((probs, torch.Tensor(____).to(parameters.DEVICE)))\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "            return True, observations, rewards, actions, probs\n",
        "    return False, observations, rewards, actions, probs\n",
        "def run_epoch(epochs, model, optimizer, observations, rewards, actions, probs):\n",
        "    model.train()\n",
        "    for _ in range(parameters.EPOCH_STEPS):\n",
        "        perm = torch.randperm(len(probs))\n",
        "        for i in range(0, len(probs), parameters.BATCH_SIZE):\n",
        "            if i + parameters.BATCH_SIZE > len(probs):\n",
        "                break\n",
        "            optimizer.zero_grad()\n",
        "            lossactor, losscritic = model.loss(observations[perm[i:i+parameters.BATCH_SIZE]], rewards[perm[i:i+parameters.BATCH_SIZE]], actions[perm[i:i+parameters.BATCH_SIZE]], probs[perm[i:i+parameters.BATCH_SIZE]])\n",
        "            if epochs > 10:\n",
        "                (lossactor + losscritic).backward()\n",
        "            else:\n",
        "                losscritic.backward()\n",
        "            optimizer.step()\n",
        "        print('Loss actor: {0:7.3f}  Loss critic: {1:7.3f}'.format(1000 * lossactor, 1000 * losscritic))\n",
        "def gpu_thread(load, memory_queue, process_queue, common_dict, workers):\n",
        "    # the only thread that has an access to the gpu, it will then perform all the NN computation\n",
        "    import psutil\n",
        "    p = psutil.Process()\n",
        "    p.cpu_affinity(workers)\n",
        "    import signal\n",
        "    signal.signal(signal.SIGINT, signal.SIG_IGN)\n",
        "    try:\n",
        "        print('process started with pid: {} on core(s) {}'.format(os.getpid(), workers), flush=True)\n",
        "        model = PPO()\n",
        "        model.to(parameters.DEVICE)\n",
        "        optimizer = optim.RMSprop(model.parameters(), lr=parameters.LEARNING_RATE)\n",
        "        epochs = 0\n",
        "        if load:\n",
        "            checkpoint = torch.load('./model/mariobot.pt')\n",
        "            model.load_state_dict(checkpoint['model_state_dict'])\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "            epochs = checkpoint['epochs']\n",
        "        observations = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        rewards = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        actions = torch.LongTensor([]).to(parameters.DEVICE)\n",
        "        probs = torch.Tensor([]).to(parameters.DEVICE)\n",
        "        common_dict['epoch'] = epochs\n",
        "        while True:\n",
        "            memory_full, observations, rewards, actions, probs = \\\n",
        "                destack_memory(memory_queue, observations, rewards, actions, probs)\n",
        "            destack_process(model, process_queue, common_dict)\n",
        "            if len(observations) > parameters.MAXLEN or memory_full:\n",
        "                epochs += 1\n",
        "                print('-' * 60 + '\\n        epoch ' + str(epochs) + '\\n' + '-' * 60)\n",
        "                run_epoch(epochs, model, optimizer, observations, rewards, actions, probs)\n",
        "                observations = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                rewards = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                actions = torch.LongTensor([]).to(parameters.DEVICE)\n",
        "                probs = torch.Tensor([]).to(parameters.DEVICE)\n",
        "                torch.save({\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'epochs': epochs\n",
        "                }, './model/mariobot.pt')\n",
        "                common_dict['epoch'] = epochs\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print('saving before interruption', flush=True)\n",
        "        torch.save({\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'epochs': epochs\n",
        "        }, './model/mariobot.pt')\n",
        "‎model.py\n",
        "+59\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -0,0 +1,59 @@\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from parameters import parameters\n",
        "def to_categorical(y, num_classes):\n",
        "    \"\"\" 1-hot encodes a tensor \"\"\"\n",
        "    return torch.eye(num_classes)[y].to(parameters.DEVICE)\n",
        "class PPO(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PPO, self).__init__()\n",
        "        self.vision = nn.Sequential(\n",
        "            nn.Conv2d(3, 6, 5),\n",
        "            nn.MaxPool2d(2, 2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Conv2d(6, 16, 5),\n",
        "            nn.MaxPool2d(2),\n",
        "            nn.LeakyReLU(0.1),\n",
        "        )\n",
        "        self.actor = nn.Sequential(\n",
        "            nn.Linear(16 * 49 * 37, 120),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(84, 4)\n",
        "        )\n",
        "        self.critic = nn.Sequential(\n",
        "            nn.Linear(16 * 49 * 37, 120),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(120, 84),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(84, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        vision = self.vision(x).view(-1, 29008)\n",
        "        actor = self.actor(vision).view(-1, 4).softmax(-1)\n",
        "        if self.training:\n",
        "            critic = self.critic(vision)\n",
        "            return actor, critic.squeeze()\n",
        "        return actor\n",
        "    def loss(self, observations, rewards, actions, old_prob):\n",
        "        prob_distribution, reward_predicted = self.forward(observations)\n",
        "        r = (torch.sum(torch.eye(4)[actions].to(parameters.DEVICE) * prob_distribution, -1) + 1e-10) / (old_prob + 1e-10)\n",
        "        advantage = (rewards - reward_predicted).detach()\n",
        "        lossactor = - parameters.ACTOR_COEFF \\\n",
        "                    * torch.mean(torch.min(r * advantage,\n",
        "                                           torch.clamp(r,\n",
        "                                                       min=(1. - parameters.LOSS_CLIPPING),\n",
        "                                                       max=(1. + parameters.LOSS_CLIPPING))\n",
        "                                           * advantage))\n",
        "        losscritic = F.mse_loss(reward_predicted, rewards)\n",
        "        return lossactor, losscritic\n",
        "‎parameters.py\n",
        "+16\n",
        "Original file line number\tDiff line number\tDiff line change\n",
        "@@ -0,0 +1,16 @@\n",
        "import torch\n",
        "class Parameters:\n",
        "    def __init__(self):\n",
        "        self.ACTOR_COEFF = 1.\n",
        "        self.LOSS_CLIPPING = 0.15\n",
        "        self.GAMMA = 0.98\n",
        "        self.DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.BATCH_SIZE = 32\n",
        "        self.EPOCH_STEPS = 10\n",
        "        self.MAXLEN = 1000\n",
        "        self.LEARNING_RATE = 1e-4\n",
        "parameters = Parameters()\n"
      ],
      "metadata": {
        "id": "SwAcmmJi2-Yw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/01_A2C.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r6M92YNE9Nna"
      },
      "source": [
        "## Configuration for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyYmCpH89Nnb"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkcr9XKc9Nnc"
      },
      "source": [
        "## 01. A2C\n",
        "[Mnih, Volodymyr, et al. \"Asynchronous methods for deep reinforcement learning.\" International conference on machine learning. 2016.](http://proceedings.mlr.press/v48/mniha16.pdf)\n",
        "\n",
        "### Actor-Critic\n",
        "\n",
        "*Actor critic* method is one of the popular *policy optimization* algorithms. This approach maximizes the expected return by pushing up the probabilities of actions that receive higher returns. Let $\\pi_\\theta$ denote a policy with parameters $\\theta$. The policy gradient of performance $\\mathcal{J}(\\pi_\\theta)$ is\n",
        "\n",
        "$$ \\nabla_\\theta \\mathcal{J}(\\pi_\\theta) = \\underset{\\tau\\sim\\pi_\\theta}{\\mathbb{E}}\\left[ \\sum^T_{t=0} \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t)A^{\\pi_\\theta}(s_t, a_t) \\right],$$\n",
        "\n",
        "where $\\tau$ is a trajectory and $A^{\\pi_\\theta}$ is the advantage function for reducing variance of values. The *policy gradient algorithm* updates the parameters by adding this gradient.\n",
        "\n",
        "$$\\theta_{k+1} = \\theta_k + \\alpha \\nabla_\\theta \\mathcal{J}(\\pi_{\\theta_k}),$$\n",
        "\n",
        "where $\\alpha$ is a learning rate. The agent is trained in an on-policy way because the parameters are updated by the current policy. We call the policy *Actor* which predicts probabilities of actions in each state, and call the value function *Critic* that predicts values of all state-action pairs.\n",
        "\n",
        "\n",
        "### Advantage Function\n",
        "\n",
        "The advantage function effectively reduces the variance of values and is defined as follows.\n",
        "\n",
        "$$ A(s,a) = Q(s,a) - V(s) $$\n",
        "\n",
        "From this formula, we can replace Q with $r+\\gamma V(s')$ and redefine Advantage function without using Action-Value function.\n",
        "\n",
        "$$ A(s,a) = r + \\gamma V(s') - V(s) $$\n",
        "\n",
        "\n",
        "### Maximization Entropy\n",
        "\n",
        "Entropy is a measure of unpredictability or a measure of randomness. If we have actions with almost equal probabilities, the entropy over the actions will be the largest because it's completely unpredictable which action will be chosen. In view of the fact, we can encourage exploration by adding entropy maximization term to the loss function. The entropy $H$ with respect to the probability $p$ over actions is\n",
        "\n",
        "$$ H(P) = - \\sum_a p(a) \\log p(a) $$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwLhlnim9Nnc"
      },
      "source": [
        "## import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GP4TXOk9Nnd"
      },
      "source": [
        "## Network\n",
        "We will use two separated networks for actor and critic respectively. The actor network consists of one fully connected hidden layer with ReLU branched out two fully connected output layers for mean and standard deviation of Normal distribution. Pendulum-v0 has only one action which has a range from -2 to 2. In order to fit the range, the actor outputs the mean value that is multiplied by 2 after tanh. On the one hand, the critic network has two fully connected layers as a hidden layer (ReLU) and an output layer. One thing to note is that we initialize the last layers' weights and biases as uniformly distributed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t1MBQFoT9Nne"
      },
      "outputs": [],
      "source": [
        "def initialize_uniformly(layer: nn.Linear, init_w: float = 3e-3):\n",
        "    \"\"\"Initialize the weights and bias in [-init_w, init_w].\"\"\"\n",
        "    layer.weight.data.uniform_(-init_w, init_w)\n",
        "    layer.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self, in_dim: int, out_dim: int):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Actor, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.mu_layer = nn.Linear(128, out_dim)\n",
        "        self.log_std_layer = nn.Linear(128, out_dim)\n",
        "\n",
        "        initialize_uniformly(self.mu_layer)\n",
        "        initialize_uniformly(self.log_std_layer)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = F.relu(self.hidden1(state))\n",
        "\n",
        "        mu = torch.tanh(self.mu_layer(x)) * 2\n",
        "        log_std = F.softplus(self.log_std_layer(x))\n",
        "        std = torch.exp(log_std)\n",
        "\n",
        "        dist = Normal(mu, std)\n",
        "        action = dist.sample()\n",
        "\n",
        "        return action, dist\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, in_dim: int):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        super(Critic, self).__init__()\n",
        "\n",
        "        self.hidden1 = nn.Linear(in_dim, 128)\n",
        "        self.out = nn.Linear(128, 1)\n",
        "\n",
        "        initialize_uniformly(self.out)\n",
        "\n",
        "    def forward(self, state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Forward method implementation.\"\"\"\n",
        "        x = F.relu(self.hidden1(state))\n",
        "        value = self.out(x)\n",
        "\n",
        "        return value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX1H8ylr9Nne"
      },
      "source": [
        "## A2C Agent\n",
        "Here is a summary of A2CAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "|---               |---                                                   |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|plot              | plot the training progresses.                        |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mrUtbRh29Nne"
      },
      "outputs": [],
      "source": [
        "class A2CAgent:\n",
        "    \"\"\"A2CAgent interacting with environment.\n",
        "\n",
        "    Atribute:\n",
        "        env (gym.Env): openAI Gym environment\n",
        "        gamma (float): discount factor\n",
        "        entropy_weight (float): rate of weighting entropy into the loss function\n",
        "        device (torch.device): cpu / gpu\n",
        "        actor (nn.Module): target actor model to select actions\n",
        "        critic (nn.Module): critic model to predict state values\n",
        "        actor_optimizer (optim.Optimizer) : optimizer of actor\n",
        "        critic_optimizer (optim.Optimizer) : optimizer of critic\n",
        "        transition (list): temporory storage for the recent transition\n",
        "        total_step (int): total step numbers\n",
        "        is_test (bool): flag to show the current mode (train / test)\n",
        "        seed (int): random seed\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env: gym.Env, gamma: float, entropy_weight: float, seed: int = 777):\n",
        "        \"\"\"Initialize.\"\"\"\n",
        "        self.env = env\n",
        "        self.gamma = gamma\n",
        "        self.entropy_weight = entropy_weight\n",
        "        self.seed = seed\n",
        "\n",
        "        # device: cpu / gpu\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(self.device)\n",
        "\n",
        "        # networks\n",
        "        obs_dim = env.observation_space.shape[0]\n",
        "        action_dim = env.action_space.shape[0]\n",
        "        self.actor = Actor(obs_dim, action_dim).to(self.device)\n",
        "        self.critic = Critic(obs_dim).to(self.device)\n",
        "\n",
        "        # optimizer\n",
        "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=1e-4)\n",
        "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=1e-3)\n",
        "\n",
        "        # transition (state, log_prob, next_state, reward, done)\n",
        "        self.transition: list = list()\n",
        "\n",
        "        # total steps count\n",
        "        self.total_step = 0\n",
        "\n",
        "        # mode: train / test\n",
        "        self.is_test = False\n",
        "\n",
        "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"Select an action from the input state.\"\"\"\n",
        "        state = torch.FloatTensor(state).to(self.device)\n",
        "        action, dist = self.actor(state)\n",
        "        selected_action = dist.mean if self.is_test else action\n",
        "\n",
        "        if not self.is_test:\n",
        "            log_prob = dist.log_prob(selected_action).sum(dim=-1)\n",
        "            self.transition = [state, log_prob]\n",
        "\n",
        "        return selected_action.clamp(-2.0, 2.0).cpu().detach().numpy()\n",
        "\n",
        "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
        "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
        "        next_state, reward, terminated, truncated, _ = self.env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        if not self.is_test:\n",
        "            self.transition.extend([next_state, reward, done])\n",
        "\n",
        "        return next_state, reward, done\n",
        "\n",
        "    def update_model(self) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Update the model by gradient descent.\"\"\"\n",
        "        state, log_prob, next_state, reward, done = self.transition\n",
        "\n",
        "        # Q_t   = r + gamma * V(s_{t+1})  if state != Terminal\n",
        "        #       = r                       otherwise\n",
        "        mask = 1 - done\n",
        "        next_state = torch.FloatTensor(next_state).to(self.device)\n",
        "        pred_value = self.critic(state)\n",
        "        targ_value = reward + self.gamma * self.critic(next_state) * mask\n",
        "        value_loss = F.smooth_l1_loss(pred_value, targ_value.detach())\n",
        "\n",
        "        # update value\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        value_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "\n",
        "        # advantage = Q_t - V(s_t)\n",
        "        advantage = (targ_value - pred_value).detach()  # not backpropagated\n",
        "        policy_loss = -advantage * log_prob\n",
        "        policy_loss += self.entropy_weight * -log_prob  # entropy maximization\n",
        "\n",
        "        # update policy\n",
        "        self.actor_optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        self.actor_optimizer.step()\n",
        "\n",
        "        return policy_loss.item(), value_loss.item()\n",
        "\n",
        "    def train(self, num_frames: int, plotting_interval: int = 200):\n",
        "        \"\"\"Train the agent.\"\"\"\n",
        "        self.is_test = False\n",
        "\n",
        "        actor_losses, critic_losses, scores = [], [], []\n",
        "        state, _ = self.env.reset(seed=self.seed)\n",
        "        score = 0\n",
        "\n",
        "        for self.total_step in range(1, num_frames + 1):\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            actor_loss, critic_loss = self.update_model()\n",
        "            actor_losses.append(actor_loss)\n",
        "            critic_losses.append(critic_loss)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "            # if episode ends\n",
        "            if done:\n",
        "                state, _ = self.env.reset(seed=self.seed)\n",
        "                scores.append(score)\n",
        "                score = 0\n",
        "\n",
        "            # plot\n",
        "            if self.total_step % plotting_interval == 0:\n",
        "                self._plot(self.total_step, scores, actor_losses, critic_losses)\n",
        "        self.env.close()\n",
        "\n",
        "    def test(self, video_folder: str):\n",
        "        \"\"\"Test the agent.\"\"\"\n",
        "        self.is_test = True\n",
        "\n",
        "        tmp_env = self.env\n",
        "        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)\n",
        "\n",
        "        state, _ = self.env.reset(seed=self.seed)\n",
        "        done = False\n",
        "        score = 0\n",
        "\n",
        "        while not done:\n",
        "            action = self.select_action(state)\n",
        "            next_state, reward, done = self.step(action)\n",
        "\n",
        "            state = next_state\n",
        "            score += reward\n",
        "\n",
        "        print(\"score: \", score)\n",
        "        self.env.close()\n",
        "\n",
        "        self.env = tmp_env\n",
        "\n",
        "    def _plot(\n",
        "        self,\n",
        "        frame_idx: int,\n",
        "        scores: List[float],\n",
        "        actor_losses: List[float],\n",
        "        critic_losses: List[float],\n",
        "    ):\n",
        "        \"\"\"Plot the training progresses.\"\"\"\n",
        "\n",
        "        def subplot(loc: int, title: str, values: List[float]):\n",
        "            plt.subplot(loc)\n",
        "            plt.title(title)\n",
        "            plt.plot(values)\n",
        "\n",
        "        subplot_params = [\n",
        "            (131, f\"frame {frame_idx}. score: {np.mean(scores[-10:])}\", scores),\n",
        "            (132, \"actor_loss\", actor_losses),\n",
        "            (133, \"critic_loss\", critic_losses),\n",
        "        ]\n",
        "\n",
        "        clear_output(True)\n",
        "        plt.figure(figsize=(30, 5))\n",
        "        for loc, title, values in subplot_params:\n",
        "            subplot(loc, title, values)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNvD7ppH9Nnf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReNSsWTt9Nnf"
      },
      "source": [
        "You can see [the code](https://github.com/Farama-Foundation/Gymnasium/blob/main/gymnasium/envs/classic_control/pendulum.py) and [configurations](https://github.com/Farama-Foundation/Gymnasium/blob/e73245912087d47b538dcdb45fa9a9d185b805c5/gymnasium/envs/__init__.py#L41) of Pendulum-v1 from Gymnasyim repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PK9d7SBQ9Nnf"
      },
      "outputs": [],
      "source": [
        "# environment\n",
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q59eeKYO-FtG"
      },
      "source": [
        "## Set random seed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UP-KFxGQ9Nnf"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PQl2ZfS9Nnf"
      },
      "outputs": [],
      "source": [
        "num_frames = 100000\n",
        "gamma = 0.90\n",
        "entropy_weight = 1e-2\n",
        "\n",
        "agent = A2CAgent(env, gamma, entropy_weight, seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sr5N63DL9Nng"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FcSfuw9h9Nng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "dc414b01-6c24-438c-c84f-c933ac423d88"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-4326fee8443c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "agent.train(num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kz1mY15w9Nng"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KuGSvYK-9Nng",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "d4ebea59-6584-4249-f1de-101a2b7f2660"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-b1709cc082a5>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideo_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"videos/a2c\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# test\n",
        "video_folder = \"videos/a2c\"\n",
        "agent.test(video_folder=video_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfNxDFpe9Nng"
      },
      "source": [
        "## Render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVyGhnuj9Nng"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def ipython_show_video(path: str) -> None:\n",
        "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "    video = io.open(path, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display(\n",
        "        HTML(\n",
        "            data=\"\"\"\n",
        "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "        </video>\n",
        "        \"\"\".format(\n",
        "                encoded.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def show_latest_video(video_folder: str) -> str:\n",
        "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
        "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    ipython_show_video(latest_file)\n",
        "    return latest_file\n",
        "\n",
        "\n",
        "latest_file = show_latest_video(video_folder=video_folder)\n",
        "print(\"Played:\", latest_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IBa8y3Vm9Nnh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "pgayn",
      "language": "python",
      "name": "pgayn"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
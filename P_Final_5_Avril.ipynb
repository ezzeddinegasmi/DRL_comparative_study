{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKA2nBjTc0Y7ddZowbNA3c",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/P_Final_5_Avril.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r exigences.txt"
      ],
      "metadata": {
        "id": "gvdfbGA3CZxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import config\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from utils import make_env, unzip_file, CustomWandbCallback, RewardLogger\n",
        "import os\n",
        "from stable_baselines3.common.utils import get_latest_run_id"
      ],
      "metadata": {
        "id": "KHrutFStldd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUBGWSc2l3tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyYmCpH89Nnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9393777e-2b87-4fb0-e271-8eb4c31220f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U85J8mNwmMAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gh repo clone https://github.com/Neilus03/DRL_comparative_study/BreakOut_sb3_PPO\n",
        "/config.py TO https://github.com/ezzeddinegasmi/git-clone-https-github.com-Neilus03-DRL_comparative_study-BreakOut_sb3_-PPO-config.py-MyFirstProject"
      ],
      "metadata": {
        "id": "jhA62C1ymbEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/config.py"
      ],
      "metadata": {
        "id": "lTrVs6q-oREW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from stable_baselines3.common.utils import get_latest_run_id\n",
        "import torch\n",
        "\n",
        "\n",
        "'''FILE TO STORE ALL THE CONFIGURATION VARIABLES'''\n",
        "\n",
        "#pretrained is a boolean that indicates if a pretrained model will be loaded\n",
        "pretrained = False # Set to True if you want to load a pretrained model\n",
        "\n",
        "#check_freq is the frequency at which the callback is called, in this case, the callback is called every 2000 timesteps\n",
        "check_freq = 2000\n",
        "\n",
        "#save_path is the path where the best model will be saved\n",
        "save_path = \"./breakout_ppo_1M_save_path\"\n",
        "\n",
        "#log_dir is the path where the logs will be saved\n",
        "log_dir = \"./log_dir\"\n",
        "\n",
        "\n",
        "'''\n",
        "Hyperparameters of the model {learning_rate, gamma, device, n_steps, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage}\n",
        "'''\n",
        "#policy is the policy of the model, in this case, the model will use a convolutional neural network\n",
        "policy = \"CnnPolicy\"\n",
        "\n",
        "#learning_rate is the learning rate of the model\n",
        "learning_rate =5e-4  #first trial: 5e-4   #second trial: 1e-4  #third trial: 1e-3  #fourth trial: 5e-5 #fifth trial: 5e-5 gamma = 0.90 #sixth trial: 1e-4 gamma = 0.90 #seventh trial: 5e-4 gamma = 0.90\n",
        "\n",
        "#gamma is the discount factor\n",
        "gamma = 0.99\n",
        "\n",
        "#device is the device where the model will be trained, if cuda is available, the model will be trained in the gpu, otherwise, it will be trained in the cpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#n_steps is the number of steps taken by the model before updating the parameters\n",
        "n_steps = 24\n",
        "\n",
        "#batch_size is the number of samples used in each update\n",
        "batch_size = 96\n",
        "\n",
        "#n_epochs is the number of epochs when optimizing the surrogate loss\n",
        "n_epochs = 6\n",
        "\n",
        "#gae_lambda is the lambda parameter of the generalized advantage estimation, set to 1 to disable it\n",
        "gae_lambda = 0.95\n",
        "\n",
        "#clip_range is the clipping parameter of the surrogate loss\n",
        "clip_range = 0.2\n",
        "\n",
        "#clip_range_vf is the clipping parameter of the value function\n",
        "clip_range_vf = 1\n",
        "\n",
        "#ent_coef is the entropy coefficient, set to 0 to disable it\n",
        "ent_coef = 0.01\n",
        "\n",
        "#vf_coef is the value function coefficient, If we set it to 0.5, then the value function loss will be half the policy loss\n",
        "vf_coef = 0.5\n",
        "\n",
        "#max_grad_norm is the maximum value for the gradient clipping\n",
        "max_grad_norm = 0.5\n",
        "\n",
        "#use_sde is a boolean that indicates if the stochastic differential equation will be used\n",
        "#The stochastic differential equation is a method to add noise to the actions taken by the agent to improve exploration\n",
        "use_sde = False\n",
        "\n",
        "#sde_sample_freq is the frequency at which the noise is added to the actions. If set to -1, the noise will be added every timestep\n",
        "sde_sample_freq = -1\n",
        "\n",
        "#rollout_buffer_class is the class of the rollout buffer, in this case, the model will use the RolloutBuffer class\n",
        "rollout_buffer_class = None\n",
        "\n",
        "#rollout_buffer_kwargs is a dictionary with the keyword arguments for the rollout buffer. If None, it will use the default arguments\n",
        "rollout_buffer_kwargs = None\n",
        "\n",
        "#target_kl is the target value for the KL divergence between the old and updated policy\n",
        "target_kl = 0.5\n",
        "\n",
        "#normalize_advantage is a boolean that indicates if the advantage will be normalized, by normalizing the advantage,\n",
        "# the variance of the advantage is reduced, this is done to improve the training process because the advantage is used to calculate the policy loss\n",
        "normalize_advantage = False\n",
        "\n",
        "#stats_window_size is the size of the window used to calculate the mean and standard deviation of the advantage\n",
        "stats_window_size = 100\n",
        "\n",
        "#tensorboard_log is the path where the tensorboard logs will be saved, in our case, the logs will be saved in the log_dir\n",
        "tensorboard_log = log_dir\n",
        "\n",
        "#policy_kwargs is a dictionary with the keyword arguments for the policy. If None, it will use the default arguments\n",
        "policy_kwargs = None\n",
        "\n",
        "#verbose is the verbosity level: 0 no output, 1 info, 2 debug\n",
        "verbose = 2\n",
        "\n",
        "#seed is the seed for the pseudo random number generator used by the model. It is set to None to use a random seed,\n",
        "# and set to 0 to use a fixed seed for reproducibility\n",
        "seed = None\n",
        "\n",
        "#_init_setup_model is a boolean that indicates if the model will be initialized after being created, set to True to initialize the model\n",
        "_init_setup_model = True\n",
        "\n",
        "#total_timesteps is the total number of timesteps that the model will be trained. In this case, the model will be trained for 1e7 timesteps\n",
        "#Take into account that the number of timesteps is not the number of episodes, in a game like breakout, the agent takes an action every frame,\n",
        "# then the number of timesteps is the number of frames, which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e7 timesteps is 1000 games more or less.\n",
        "total_timesteps = int(3e7)\n",
        "\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "log_interval = 100\n",
        "\n",
        "'''\n",
        "Saved model path\n",
        "'''\n",
        "\n",
        "#for the path to be shorter just put \"./a2c_Breakout_1M.zip\" instead of the full path\n",
        "saved_model_path = \"./PPO_Breakout_30M_lr_5e-4_gamma_90.zip\"\n",
        "unzip_file_path =  \"./PPO_Breakout_30M_lr_5e-4_gamma_90_unzipped\"\n",
        "\n",
        "'''\n",
        "Environment variables\n",
        "'''\n",
        "#n_stack is the number of frames stacked together to form the input to the model\n",
        "n_stack = 4\n",
        "#n_envs is the number of environments that will be run in parallel\n",
        "n_envs = 4\n",
        "\n",
        "'''\n",
        "Wandb configuration\n",
        "'''\n",
        "#log_to_wandb is a boolean that indicates if the training process will be logged to wandb\n",
        "log_to_wandb = False\n",
        "\n",
        "# project is the name of the project in wandb\n",
        "project_train = \"BREAKOUT_SB3_BENCHMARK\"\n",
        "project_test = \"breakout-PPO2-test\"\n",
        "\n",
        "#entity is the name of the team in wandb\n",
        "entity = \"ai42\"\n",
        "\n",
        "#name is the name of the run in wandb\n",
        "name_train = \"PPO_breakout_lr_5e-4_gamma_90\"\n",
        "name_test = \"PPO2_breakout_test\"\n",
        "#notes is a description of the run\n",
        "notes = \"PPO2_breakout with parameters: {}\".format(locals()) #locals() returns a dictionary with all the local variables, in this case, all the variables in this file\n",
        "#sync_tensorboard is a boolean that indicates if the tensorboard logs will be synced to wandb\n",
        "sync_tensorboard = True\n",
        "\n",
        "\n",
        "'''\n",
        "Test configuration\n",
        "'''\n",
        "test_episodes = 100"
      ],
      "metadata": {
        "id": "5VtBnQwhomZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/train_PPO.py"
      ],
      "metadata": {
        "id": "vUsuyr9upJRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import config\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from utils import make_env, unzip_file, CustomWandbCallback, RewardLogger\n",
        "import os\n",
        "from stable_baselines3.common.utils import get_latest_run_id\n",
        "\n",
        "'''\n",
        "Set up the appropriate directories for logging and saving the model\n",
        "'''\n",
        "os.makedirs(config.log_dir, exist_ok=True)\n",
        "os.makedirs(config.save_path, exist_ok=True)\n",
        "\n",
        "#Create the callback that logs the mean reward of the last 100 episodes to wandb\n",
        "custom_callback = CustomWandbCallback(config.check_freq, config.save_path)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up loging to wandb\n",
        "'''\n",
        "\n",
        "#Set wandb to log the training process\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.project_train, entity = config.entity, name=config.name_train, notes=config.notes, sync_tensorboard=config.sync_tensorboard)\n",
        "    #wandb_callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "    wandb_callback = WandbCallback()\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment\n",
        "'''\n",
        "# Create multiple environments and wrap them correctly\n",
        "env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=config.n_envs, seed=config.seed)\n",
        "env = VecFrameStack(env, n_stack=config.n_stack)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the model\n",
        "'''\n",
        "#Create the model with the parameters specified in config.py, go to config.py to see the meaning of each parameter in detail\n",
        "model = PPO(policy=config.policy\n",
        "            ,env=env\n",
        "            ,learning_rate=config.learning_rate\n",
        "            ,n_steps=config.n_steps\n",
        "            ,batch_size=config.batch_size\n",
        "            ,n_epochs=config.n_epochs\n",
        "            ,gamma=config.gamma\n",
        "            ,gae_lambda=config.gae_lambda\n",
        "            ,clip_range=config.clip_range\n",
        "            ,clip_range_vf=config.clip_range_vf\n",
        "            ,normalize_advantage=config.normalize_advantage\n",
        "            ,ent_coef=config.ent_coef\n",
        "            ,vf_coef=config.vf_coef\n",
        "            ,max_grad_norm=config.max_grad_norm\n",
        "            ,use_sde=config.use_sde\n",
        "            ,sde_sample_freq=config.sde_sample_freq\n",
        "            #,rollout_buffer_class=config.rollout_buffer_class\n",
        "            #,rollout_buffer_kwargs=config.rollout_buffer_kwargs\n",
        "            ,target_kl=config.target_kl\n",
        "            ,stats_window_size=config.stats_window_size\n",
        "            ,tensorboard_log=config.log_dir\n",
        "            ,policy_kwargs=config.policy_kwargs\n",
        "            ,verbose=config.verbose\n",
        "            ,seed=config.seed\n",
        "            ,device=config.device\n",
        "            ,_init_setup_model=config._init_setup_model\n",
        "            )\n",
        "\n",
        "print(\"model in device: \", model.device)\n",
        "\n",
        "#Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    model = PPO.load(config.saved_model_path, env=env, verbose=config.verbose, tensorboard_log=config.log_dir)\n",
        "    #Unzip the file a2c_Breakout_1M.zip and store the unzipped files in the folder a2c_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Train the model and save it\n",
        "'''\n",
        "#model.learn will train the model for 1e6 timesteps, timestep is the number of actions taken by the agent,\n",
        "# in a game like breakout, the agent takes an action every frame, then the number of timesteps is the number of frames,\n",
        "# which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e6 timesteps is 1000 games more or less.\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "#callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "\n",
        "if config.log_to_wandb:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=[wandb_callback, custom_callback], progress_bar=True)\n",
        "else:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=custom_callback, progress_bar=True)\n",
        "#Save the model\n",
        "model.save(config.saved_model_path[:-4]) #remove the .zip extension from the path\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "CUKWeiq5pOrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/test_PPO.py"
      ],
      "metadata": {
        "id": "fFZgzKXIpudp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import tensorboard as tb\n",
        "import config\n",
        "from utils import make_env, unzip_file\n",
        "\n",
        "\n",
        "'''\n",
        "Set up wandb\n",
        "'''\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.name_test, entity= config.entity, sync_tensorboard=config.sync_tensorboard, name=config.name_test, notes=config.notes)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment and the model to test\n",
        "'''\n",
        "# Unzip the saved model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    #Unzip the file PPO_Breakout_1M.zip and store the unzipped files in the folder PPO_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "\n",
        "#We start with a single environment for Breakout with render mode set to human\n",
        "env = make_env(\"BreakoutNoFrameskip-v4\")\n",
        "#We then wrap the environment with the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "env = DummyVecEnv([env]) # Output shape: (1, 84, 84)\n",
        "#Finally, we wrap the environment with the VecFrameStack wrapper which stacks the observations over the last 4 frames\n",
        "env = VecFrameStack(env, n_stack=config.n_stack) # Output shape: (4, 84, 84)\n",
        "\n",
        "# Create the model\n",
        "model = PPO(policy = config.policy\n",
        "            ,env = env\n",
        "            ,verbose = config.verbose)\n",
        "\n",
        "# Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    # Load the model components, including the policy network and the value network\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "'''\n",
        "Test the model in the environment and log the results to wandb\n",
        "'''\n",
        "# Run the episodes and render the gameplay\n",
        "for episode in range(config.test_episodes):\n",
        "    # Reset the environment and stack the initial state 4 times\n",
        "    obs = env.reset()#obs = np.stack([obs] * 4, axis=0)  # Initial state stack\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        # Take an action in the environment according to the policy of the trained agent\n",
        "        action, _ = model.predict(obs)\n",
        "        # Take the action in the environment and store the results in the variables\n",
        "        obs, reward, done, info = env.step(action) # obs shape: (1, 84, 84), reward shape: (1,), done shape: (1,), info shape: (1,)\n",
        "        # Update the total reward\n",
        "        episode_reward += reward[0]\n",
        "        # Render the environment to visualize the gameplay of the trained agent\n",
        "        env.render()\n",
        "    if config.log_to_wandb:\n",
        "        # Log the total reward of the episode to wandb\n",
        "        wandb.log({'test_episode_reward': episode_reward, 'test_episode': episode})\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "NBDm8lKypzx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/utils.py"
      ],
      "metadata": {
        "id": "3Z13SThlqO6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import tensorboard as tb\n",
        "import config\n",
        "from utils import make_env, unzip_file\n",
        "\n",
        "\n",
        "'''\n",
        "Set up wandb\n",
        "'''\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.name_test, entity= config.entity, sync_tensorboard=config.sync_tensorboard, name=config.name_test, notes=config.notes)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment and the model to test\n",
        "'''\n",
        "# Unzip the saved model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    #Unzip the file PPO_Breakout_1M.zip and store the unzipped files in the folder PPO_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "\n",
        "#We start with a single environment for Breakout with render mode set to human\n",
        "env = make_env(\"BreakoutNoFrameskip-v4\")\n",
        "#We then wrap the environment with the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "env = DummyVecEnv([env]) # Output shape: (1, 84, 84)\n",
        "#Finally, we wrap the environment with the VecFrameStack wrapper which stacks the observations over the last 4 frames\n",
        "env = VecFrameStack(env, n_stack=config.n_stack) # Output shape: (4, 84, 84)\n",
        "\n",
        "# Create the model\n",
        "model = PPO(policy = config.policy\n",
        "            ,env = env\n",
        "            ,verbose = config.verbose)\n",
        "\n",
        "# Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    # Load the model components, including the policy network and the value network\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "'''\n",
        "Test the model in the environment and log the results to wandb\n",
        "'''\n",
        "# Run the episodes and render the gameplay\n",
        "for episode in range(config.test_episodes):\n",
        "    # Reset the environment and stack the initial state 4 times\n",
        "    obs = env.reset()#obs = np.stack([obs] * 4, axis=0)  # Initial state stack\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        # Take an action in the environment according to the policy of the trained agent\n",
        "        action, _ = model.predict(obs)\n",
        "        # Take the action in the environment and store the results in the variables\n",
        "        obs, reward, done, info = env.step(action) # obs shape: (1, 84, 84), reward shape: (1,), done shape: (1,), info shape: (1,)\n",
        "        # Update the total reward\n",
        "        episode_reward += reward[0]\n",
        "        # Render the environment to visualize the gameplay of the trained agent\n",
        "        env.render()\n",
        "    if config.log_to_wandb:\n",
        "        # Log the total reward of the episode to wandb\n",
        "        wandb.log({'test_episode_reward': episode_reward, 'test_episode': episode})\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "ZgItLw1Uqeoi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X_30pSfrqvBb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
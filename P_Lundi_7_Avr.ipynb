{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNFhY0+/0IKkEetpiUzIqlm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/P_Lundi_7_Avr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "GyYmCpH89Nnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d72d7f5-3359-4bfc-cacf-512f1bd66913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install 'stable-baselines3[extra]'"
      ],
      "metadata": {
        "id": "yxBVYUNrEFMV",
        "outputId": "8103b2bd-3b20-4343-d149-0895cc86abac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: gymnasium<1.2.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.2.0,>=0.29.1->stable-baselines3[extra]) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ale-py"
      ],
      "metadata": {
        "id": "zEpmC3tyEZFM",
        "outputId": "5c1804f6-933d-43b3-805b-96d17e021cf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 176,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ale' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-176-95950f1c5c51>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0male\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ale' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable-baselines3"
      ],
      "metadata": {
        "id": "ijGbFW07EhWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)\n",
        "\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "    vec_env.render()\n",
        "    # VecEnv resets automatically\n",
        "    # if done:\n",
        "    #   obs = env.reset()\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "wyz6U0UIEx77"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)"
      ],
      "metadata": {
        "id": "p1UTKv9yE7yF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -e .[docs,tests,extra]"
      ],
      "metadata": {
        "id": "1PB3pROrF7R9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make pytest"
      ],
      "metadata": {
        "id": "oVW3veg3GJzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python3 -m pytest -v tests/test_env_checker.py"
      ],
      "metadata": {
        "id": "lkEATAq5GXeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@article{stable-baselines3,\n",
        "  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n",
        "  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n",
        "  journal = {Journal of Machine Learning Research},\n",
        "  year    = {2021},\n",
        "  volume  = {22},\n",
        "  number  = {268},\n",
        "  pages   = {1-8},\n",
        "  url     = {http://jmlr.org/papers/v22/20-1364.html}"
      ],
      "metadata": {
        "id": "M3dD6ZSxGnX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "python3 -m pytest -v -k 'test_check_env_dict_action'"
      ],
      "metadata": {
        "id": "4AiJGVfLG1E3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install mypy\n",
        "make type"
      ],
      "metadata": {
        "id": "zuKjp8ukG94M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ruff\n",
        "make lint"
      ],
      "metadata": {
        "id": "HHXJrpU8HGuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwLhlnim9Nnc"
      },
      "source": [
        "## import module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "git clone https://github.com/stable-baselines3/ppo\n"
      ],
      "metadata": {
        "id": "ATVwREJwwtPm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stable_baselines3/ppo/__init__.py"
      ],
      "metadata": {
        "id": "UIld4EnTxuzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3.ppo.policies import CnnPolicy, MlpPolicy, MultiInputPolicy\n",
        "from stable_baselines3.ppo.ppo import PPO\n",
        "\n",
        "__all__ = [\"PPO\", \"CnnPolicy\", \"MlpPolicy\", \"MultiInputPolicy\"]"
      ],
      "metadata": {
        "id": "fGLigaoa5U3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stable_baselines3/ppo/policies.py"
      ],
      "metadata": {
        "id": "BUOzJzIn5v1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This file is here just to define MlpPolicy/CnnPolicy\n",
        "# that work for PPO\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, MultiInputActorCriticPolicy\n",
        "\n",
        "MlpPolicy = ActorCriticPolicy\n",
        "CnnPolicy = ActorCriticCnnPolicy\n",
        "MultiInputPolicy = MultiInputActorCriticPolicy"
      ],
      "metadata": {
        "id": "9Dmm-1Ne52Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stable_baselines3/ppo/ppo.py"
      ],
      "metadata": {
        "id": "_fHu0OOt6bO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "from typing import Any, ClassVar, Optional, TypeVar, Union\n",
        "\n",
        "import numpy as np\n",
        "import torch as th\n",
        "from gymnasium import spaces\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from stable_baselines3.common.buffers import RolloutBuffer\n",
        "from stable_baselines3.common.on_policy_algorithm import OnPolicyAlgorithm\n",
        "from stable_baselines3.common.policies import ActorCriticCnnPolicy, ActorCriticPolicy, BasePolicy, MultiInputActorCriticPolicy\n",
        "from stable_baselines3.common.type_aliases import GymEnv, MaybeCallback, Schedule\n",
        "from stable_baselines3.common.utils import explained_variance, get_schedule_fn\n",
        "\n",
        "SelfPPO = TypeVar(\"SelfPPO\", bound=\"PPO\")\n",
        "\n",
        "\n",
        "class PPO(OnPolicyAlgorithm):\n",
        "    \"\"\"\n",
        "    Proximal Policy Optimization algorithm (PPO) (clip version)\n",
        "\n",
        "    Paper: https://arxiv.org/abs/1707.06347\n",
        "    Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
        "    https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
        "    Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "    Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
        "\n",
        "    :param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
        "    :param env: The environment to learn from (if registered in Gym, can be str)\n",
        "    :param learning_rate: The learning rate, it can be a function\n",
        "        of the current progress remaining (from 1 to 0)\n",
        "    :param n_steps: The number of steps to run for each environment per update\n",
        "        (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
        "        NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
        "        See https://github.com/pytorch/pytorch/issues/29372\n",
        "    :param batch_size: Minibatch size\n",
        "    :param n_epochs: Number of epoch when optimizing the surrogate loss\n",
        "    :param gamma: Discount factor\n",
        "    :param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
        "    :param clip_range: Clipping parameter, it can be a function of the current progress\n",
        "        remaining (from 1 to 0).\n",
        "    :param clip_range_vf: Clipping parameter for the value function,\n",
        "        it can be a function of the current progress remaining (from 1 to 0).\n",
        "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
        "        no clipping will be done on the value function.\n",
        "        IMPORTANT: this clipping depends on the reward scaling.\n",
        "    :param normalize_advantage: Whether to normalize or not the advantage\n",
        "    :param ent_coef: Entropy coefficient for the loss calculation\n",
        "    :param vf_coef: Value function coefficient for the loss calculation\n",
        "    :param max_grad_norm: The maximum value for the gradient clipping\n",
        "    :param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
        "        instead of action noise exploration (default: False)\n",
        "    :param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
        "        Default: -1 (only sample at the beginning of the rollout)\n",
        "    :param rollout_buffer_class: Rollout buffer class to use. If ``None``, it will be automatically selected.\n",
        "    :param rollout_buffer_kwargs: Keyword arguments to pass to the rollout buffer on creation\n",
        "    :param target_kl: Limit the KL divergence between updates,\n",
        "        because the clipping is not enough to prevent large update\n",
        "        see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
        "        By default, there is no limit on the kl div.\n",
        "    :param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
        "        the reported success rate, mean episode length, and mean reward over\n",
        "    :param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
        "    :param policy_kwargs: additional arguments to be passed to the policy on creation. See :ref:`ppo_policies`\n",
        "    :param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
        "        debug messages\n",
        "    :param seed: Seed for the pseudo random generators\n",
        "    :param device: Device (cpu, cuda, ...) on which the code should be run.\n",
        "        Setting it to auto, the code will be run on the GPU if possible.\n",
        "    :param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
        "    \"\"\"\n",
        "\n",
        "    policy_aliases: ClassVar[dict[str, type[BasePolicy]]] = {\n",
        "        \"MlpPolicy\": ActorCriticPolicy,\n",
        "        \"CnnPolicy\": ActorCriticCnnPolicy,\n",
        "        \"MultiInputPolicy\": MultiInputActorCriticPolicy,\n",
        "    }\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        policy: Union[str, type[ActorCriticPolicy]],\n",
        "        env: Union[GymEnv, str],\n",
        "        learning_rate: Union[float, Schedule] = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 64,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        clip_range: Union[float, Schedule] = 0.2,\n",
        "        clip_range_vf: Union[None, float, Schedule] = None,\n",
        "        normalize_advantage: bool = True,\n",
        "        ent_coef: float = 0.0,\n",
        "        vf_coef: float = 0.5,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        use_sde: bool = False,\n",
        "        sde_sample_freq: int = -1,\n",
        "        rollout_buffer_class: Optional[type[RolloutBuffer]] = None,\n",
        "        rollout_buffer_kwargs: Optional[dict[str, Any]] = None,\n",
        "        target_kl: Optional[float] = None,\n",
        "        stats_window_size: int = 100,\n",
        "        tensorboard_log: Optional[str] = None,\n",
        "        policy_kwargs: Optional[dict[str, Any]] = None,\n",
        "        verbose: int = 0,\n",
        "        seed: Optional[int] = None,\n",
        "        device: Union[th.device, str] = \"auto\",\n",
        "        _init_setup_model: bool = True,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            policy,\n",
        "            env,\n",
        "            learning_rate=learning_rate,\n",
        "            n_steps=n_steps,\n",
        "            gamma=gamma,\n",
        "            gae_lambda=gae_lambda,\n",
        "            ent_coef=ent_coef,\n",
        "            vf_coef=vf_coef,\n",
        "            max_grad_norm=max_grad_norm,\n",
        "            use_sde=use_sde,\n",
        "            sde_sample_freq=sde_sample_freq,\n",
        "            rollout_buffer_class=rollout_buffer_class,\n",
        "            rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
        "            stats_window_size=stats_window_size,\n",
        "            tensorboard_log=tensorboard_log,\n",
        "            policy_kwargs=policy_kwargs,\n",
        "            verbose=verbose,\n",
        "            device=device,\n",
        "            seed=seed,\n",
        "            _init_setup_model=False,\n",
        "            supported_action_spaces=(\n",
        "                spaces.Box,\n",
        "                spaces.Discrete,\n",
        "                spaces.MultiDiscrete,\n",
        "                spaces.MultiBinary,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        # Sanity check, otherwise it will lead to noisy gradient and NaN\n",
        "        # because of the advantage normalization\n",
        "        if normalize_advantage:\n",
        "            assert (\n",
        "                batch_size > 1\n",
        "            ), \"`batch_size` must be greater than 1. See https://github.com/DLR-RM/stable-baselines3/issues/440\"\n",
        "\n",
        "        if self.env is not None:\n",
        "            # Check that `n_steps * n_envs > 1` to avoid NaN\n",
        "            # when doing advantage normalization\n",
        "            buffer_size = self.env.num_envs * self.n_steps\n",
        "            assert buffer_size > 1 or (\n",
        "                not normalize_advantage\n",
        "            ), f\"`n_steps * n_envs` must be greater than 1. Currently n_steps={self.n_steps} and n_envs={self.env.num_envs}\"\n",
        "            # Check that the rollout buffer size is a multiple of the mini-batch size\n",
        "            untruncated_batches = buffer_size // batch_size\n",
        "            if buffer_size % batch_size > 0:\n",
        "                warnings.warn(\n",
        "                    f\"You have specified a mini-batch size of {batch_size},\"\n",
        "                    f\" but because the `RolloutBuffer` is of size `n_steps * n_envs = {buffer_size}`,\"\n",
        "                    f\" after every {untruncated_batches} untruncated mini-batches,\"\n",
        "                    f\" there will be a truncated mini-batch of size {buffer_size % batch_size}\\n\"\n",
        "                    f\"We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\\n\"\n",
        "                    f\"Info: (n_steps={self.n_steps} and n_envs={self.env.num_envs})\"\n",
        "                )\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.clip_range = clip_range\n",
        "        self.clip_range_vf = clip_range_vf\n",
        "        self.normalize_advantage = normalize_advantage\n",
        "        self.target_kl = target_kl\n",
        "\n",
        "        if _init_setup_model:\n",
        "            self._setup_model()\n",
        "\n",
        "    def _setup_model(self) -> None:\n",
        "        super()._setup_model()\n",
        "\n",
        "        # Initialize schedules for policy/value clipping\n",
        "        self.clip_range = get_schedule_fn(self.clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            if isinstance(self.clip_range_vf, (float, int)):\n",
        "                assert self.clip_range_vf > 0, \"`clip_range_vf` must be positive, \" \"pass `None` to deactivate vf clipping\"\n",
        "\n",
        "            self.clip_range_vf = get_schedule_fn(self.clip_range_vf)\n",
        "\n",
        "    def train(self) -> None:\n",
        "        \"\"\"\n",
        "        Update policy using the currently gathered rollout buffer.\n",
        "        \"\"\"\n",
        "        # Switch to train mode (this affects batch norm / dropout)\n",
        "        self.policy.set_training_mode(True)\n",
        "        # Update optimizer learning rate\n",
        "        self._update_learning_rate(self.policy.optimizer)\n",
        "        # Compute current clip range\n",
        "        clip_range = self.clip_range(self._current_progress_remaining)  # type: ignore[operator]\n",
        "        # Optional: clip range for the value function\n",
        "        if self.clip_range_vf is not None:\n",
        "            clip_range_vf = self.clip_range_vf(self._current_progress_remaining)  # type: ignore[operator]\n",
        "\n",
        "        entropy_losses = []\n",
        "        pg_losses, value_losses = [], []\n",
        "        clip_fractions = []\n",
        "\n",
        "        continue_training = True\n",
        "        # train for n_epochs epochs\n",
        "        for epoch in range(self.n_epochs):\n",
        "            approx_kl_divs = []\n",
        "            # Do a complete pass on the rollout buffer\n",
        "            for rollout_data in self.rollout_buffer.get(self.batch_size):\n",
        "                actions = rollout_data.actions\n",
        "                if isinstance(self.action_space, spaces.Discrete):\n",
        "                    # Convert discrete action from float to long\n",
        "                    actions = rollout_data.actions.long().flatten()\n",
        "\n",
        "                values, log_prob, entropy = self.policy.evaluate_actions(rollout_data.observations, actions)\n",
        "                values = values.flatten()\n",
        "                # Normalize advantage\n",
        "                advantages = rollout_data.advantages\n",
        "                # Normalization does not make sense if mini batchsize == 1, see GH issue #325\n",
        "                if self.normalize_advantage and len(advantages) > 1:\n",
        "                    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
        "\n",
        "                # ratio between old and new policy, should be one at the first iteration\n",
        "                ratio = th.exp(log_prob - rollout_data.old_log_prob)\n",
        "\n",
        "                # clipped surrogate loss\n",
        "                policy_loss_1 = advantages * ratio\n",
        "                policy_loss_2 = advantages * th.clamp(ratio, 1 - clip_range, 1 + clip_range)\n",
        "                policy_loss = -th.min(policy_loss_1, policy_loss_2).mean()\n",
        "\n",
        "                # Logging\n",
        "                pg_losses.append(policy_loss.item())\n",
        "                clip_fraction = th.mean((th.abs(ratio - 1) > clip_range).float()).item()\n",
        "                clip_fractions.append(clip_fraction)\n",
        "\n",
        "                if self.clip_range_vf is None:\n",
        "                    # No clipping\n",
        "                    values_pred = values\n",
        "                else:\n",
        "                    # Clip the difference between old and new value\n",
        "                    # NOTE: this depends on the reward scaling\n",
        "                    values_pred = rollout_data.old_values + th.clamp(\n",
        "                        values - rollout_data.old_values, -clip_range_vf, clip_range_vf\n",
        "                    )\n",
        "                # Value loss using the TD(gae_lambda) target\n",
        "                value_loss = F.mse_loss(rollout_data.returns, values_pred)\n",
        "                value_losses.append(value_loss.item())\n",
        "\n",
        "                # Entropy loss favor exploration\n",
        "                if entropy is None:\n",
        "                    # Approximate entropy when no analytical form\n",
        "                    entropy_loss = -th.mean(-log_prob)\n",
        "                else:\n",
        "                    entropy_loss = -th.mean(entropy)\n",
        "\n",
        "                entropy_losses.append(entropy_loss.item())\n",
        "\n",
        "                loss = policy_loss + self.ent_coef * entropy_loss + self.vf_coef * value_loss\n",
        "\n",
        "                # Calculate approximate form of reverse KL Divergence for early stopping\n",
        "                # see issue #417: https://github.com/DLR-RM/stable-baselines3/issues/417\n",
        "                # and discussion in PR #419: https://github.com/DLR-RM/stable-baselines3/pull/419\n",
        "                # and Schulman blog: http://joschu.net/blog/kl-approx.html\n",
        "                with th.no_grad():\n",
        "                    log_ratio = log_prob - rollout_data.old_log_prob\n",
        "                    approx_kl_div = th.mean((th.exp(log_ratio) - 1) - log_ratio).cpu().numpy()\n",
        "                    approx_kl_divs.append(approx_kl_div)\n",
        "\n",
        "                if self.target_kl is not None and approx_kl_div > 1.5 * self.target_kl:\n",
        "                    continue_training = False\n",
        "                    if self.verbose >= 1:\n",
        "                        print(f\"Early stopping at step {epoch} due to reaching max kl: {approx_kl_div:.2f}\")\n",
        "                    break\n",
        "\n",
        "                # Optimization step\n",
        "                self.policy.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                # Clip grad norm\n",
        "                th.nn.utils.clip_grad_norm_(self.policy.parameters(), self.max_grad_norm)\n",
        "                self.policy.optimizer.step()\n",
        "\n",
        "            self._n_updates += 1\n",
        "            if not continue_training:\n",
        "                break\n",
        "\n",
        "        explained_var = explained_variance(self.rollout_buffer.values.flatten(), self.rollout_buffer.returns.flatten())\n",
        "\n",
        "        # Logs\n",
        "        self.logger.record(\"train/entropy_loss\", np.mean(entropy_losses))\n",
        "        self.logger.record(\"train/policy_gradient_loss\", np.mean(pg_losses))\n",
        "        self.logger.record(\"train/value_loss\", np.mean(value_losses))\n",
        "        self.logger.record(\"train/approx_kl\", np.mean(approx_kl_divs))\n",
        "        self.logger.record(\"train/clip_fraction\", np.mean(clip_fractions))\n",
        "        self.logger.record(\"train/loss\", loss.item())\n",
        "        self.logger.record(\"train/explained_variance\", explained_var)\n",
        "        if hasattr(self.policy, \"log_std\"):\n",
        "            self.logger.record(\"train/std\", th.exp(self.policy.log_std).mean().item())\n",
        "\n",
        "        self.logger.record(\"train/n_updates\", self._n_updates, exclude=\"tensorboard\")\n",
        "        self.logger.record(\"train/clip_range\", clip_range)\n",
        "        if self.clip_range_vf is not None:\n",
        "            self.logger.record(\"train/clip_range_vf\", clip_range_vf)\n",
        "\n",
        "    def learn(\n",
        "        self: SelfPPO,\n",
        "        total_timesteps: int,\n",
        "        callback: MaybeCallback = None,\n",
        "        log_interval: int = 1,\n",
        "        tb_log_name: str = \"PPO\",\n",
        "        reset_num_timesteps: bool = True,\n",
        "        progress_bar: bool = False,\n",
        "    ) -> SelfPPO:\n",
        "        return super().learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            callback=callback,\n",
        "            log_interval=log_interval,\n",
        "            tb_log_name=tb_log_name,\n",
        "            reset_num_timesteps=reset_num_timesteps,\n",
        "            progress_bar=progress_bar,\n",
        "        )\n"
      ],
      "metadata": {
        "id": "9BS1xMP96gJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "README.md"
      ],
      "metadata": {
        "id": "la6CBPUL7Z9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "<!-- [![pipeline status](https://gitlab.com/araffin/stable-baselines3/badges/master/pipeline.svg)](https://gitlab.com/araffin/stable-baselines3/-/commits/master) -->\n",
        "[![CI](https://github.com/DLR-RM/stable-baselines3/workflows/CI/badge.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)\n",
        "[![Documentation Status](https://readthedocs.org/projects/stable-baselines/badge/?version=master)](https://stable-baselines3.readthedocs.io/en/master/?badge=master) [![coverage report](https://gitlab.com/araffin/stable-baselines3/badges/master/coverage.svg)](https://github.com/DLR-RM/stable-baselines3/actions/workflows/ci.yml)\n",
        "[![codestyle](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)\n",
        "\n",
        "\n",
        "# Stable Baselines3\n",
        "\n",
        "<img src=\"docs/\\_static/img/logo.png\" align=\"right\" width=\"40%\"/>\n",
        "\n",
        "Stable Baselines3 (SB3) is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "You can read a detailed presentation of Stable Baselines3 in the [v1.0 blog post](https://araffin.github.io/post/sb3/) or our [JMLR paper](https://jmlr.org/papers/volume22/20-1364/20-1364.pdf).\n",
        "\n",
        "\n",
        "These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "**Note: Despite its simplicity of use, Stable Baselines3 (SB3) assumes you have some knowledge about Reinforcement Learning (RL).** You should not utilize this library without some practice. To that extent, we provide good resources in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/rl.html) to get started with RL.\n",
        "\n",
        "## Main Features\n",
        "\n",
        "**The performance of each algorithm was tested** (see *Results* section in their respective page),\n",
        "you can take a look at the issues [#48](https://github.com/DLR-RM/stable-baselines3/issues/48) and [#49](https://github.com/DLR-RM/stable-baselines3/issues/49) for more details.\n",
        "\n",
        "We also provide detailed logs and reports on the [OpenRL Benchmark](https://wandb.ai/openrlbenchmark/sb3) platform.\n",
        "\n",
        "\n",
        "| **Features**                | **Stable-Baselines3** |\n",
        "| --------------------------- | ----------------------|\n",
        "| State of the art RL methods | :heavy_check_mark: |\n",
        "| Documentation               | :heavy_check_mark: |\n",
        "| Custom environments         | :heavy_check_mark: |\n",
        "| Custom policies             | :heavy_check_mark: |\n",
        "| Common interface            | :heavy_check_mark: |\n",
        "| `Dict` observation space support  | :heavy_check_mark: |\n",
        "| Ipython / Notebook friendly | :heavy_check_mark: |\n",
        "| Tensorboard support         | :heavy_check_mark: |\n",
        "| PEP8 code style             | :heavy_check_mark: |\n",
        "| Custom callback             | :heavy_check_mark: |\n",
        "| High code coverage          | :heavy_check_mark: |\n",
        "| Type hints                  | :heavy_check_mark: |\n",
        "\n",
        "\n",
        "### Planned features\n",
        "\n",
        "Since most of the features from the [original roadmap](https://github.com/DLR-RM/stable-baselines3/issues/1) have been implemented, there are no major changes planned for SB3, it is now *stable*.\n",
        "If you want to contribute, you can search in the issues for the ones where [help is welcomed](https://github.com/DLR-RM/stable-baselines3/labels/help%20wanted) and the other [proposed enhancements](https://github.com/DLR-RM/stable-baselines3/labels/enhancement).\n",
        "\n",
        "While SB3 development is now focused on bug fixes and maintenance (doc update, user experience, ...), there is more active development going on in the associated repositories:\n",
        "- newer algorithms are regularly added to the [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) repository\n",
        "- faster variants are developed in the [SBX (SB3 + Jax)](https://github.com/araffin/sbx) repository\n",
        "- the training framework for SB3, the RL Zoo, has an active [roadmap](https://github.com/DLR-RM/rl-baselines3-zoo/issues/299)\n",
        "\n",
        "## Migration guide: from Stable-Baselines (SB2) to Stable-Baselines3 (SB3)\n",
        "\n",
        "A migration guide from SB2 to SB3 can be found in the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html).\n",
        "\n",
        "## Documentation\n",
        "\n",
        "Documentation is available online: [https://stable-baselines3.readthedocs.io/](https://stable-baselines3.readthedocs.io/)\n",
        "\n",
        "## Integrations\n",
        "\n",
        "Stable-Baselines3 has some integration with other libraries/services like Weights & Biases for experiment tracking or Hugging Face for storing/sharing trained models. You can find out more in the [dedicated section](https://stable-baselines3.readthedocs.io/en/master/guide/integrations.html) of the documentation.\n",
        "\n",
        "\n",
        "## RL Baselines3 Zoo: A Training Framework for Stable Baselines3 Reinforcement Learning Agents\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL).\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "In addition, it includes a collection of tuned hyperparameters for common environments and RL algorithms, and agents trained with those settings.\n",
        "\n",
        "Goals of this repository:\n",
        "\n",
        "1. Provide a simple interface to train and enjoy RL agents\n",
        "2. Benchmark the different Reinforcement Learning algorithms\n",
        "3. Provide tuned hyperparameters for each environment and RL algorithm\n",
        "4. Have fun with the trained agents!\n",
        "\n",
        "Github repo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "Documentation: https://rl-baselines3-zoo.readthedocs.io/en/master/\n",
        "\n",
        "## SB3-Contrib: Experimental RL Features\n",
        "\n",
        "We implement experimental features in a separate contrib repository: [SB3-Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib)\n",
        "\n",
        "This allows SB3 to maintain a stable and compact core, while still providing the latest features, like Recurrent PPO (PPO LSTM), CrossQ, Truncated Quantile Critics (TQC), Quantile Regression DQN (QR-DQN) or PPO with invalid action masking (Maskable PPO).\n",
        "\n",
        "Documentation is available online: [https://sb3-contrib.readthedocs.io/](https://sb3-contrib.readthedocs.io/)\n",
        "\n",
        "## Stable-Baselines Jax (SBX)\n",
        "\n",
        "[Stable Baselines Jax (SBX)](https://github.com/araffin/sbx) is a proof of concept version of Stable-Baselines3 in Jax, with recent algorithms like DroQ or CrossQ.\n",
        "\n",
        "It provides a minimal number of features compared to SB3 but can be much faster (up to 20x times!): https://twitter.com/araffin2/status/1590714558628253698\n",
        "\n",
        "\n",
        "## Installation\n",
        "\n",
        "**Note:** Stable-Baselines3 supports PyTorch >= 2.3\n",
        "\n",
        "### Prerequisites\n",
        "Stable Baselines3 requires Python 3.9+.\n",
        "\n",
        "#### Windows\n",
        "\n",
        "To install stable-baselines on Windows, please look at the [documentation](https://stable-baselines3.readthedocs.io/en/master/guide/install.html#prerequisites).\n",
        "\n",
        "\n",
        "### Install using pip\n",
        "Install the Stable Baselines3 package:\n",
        "```sh\n",
        "pip install 'stable-baselines3[extra]'\n",
        "```\n",
        "\n",
        "This includes an optional dependencies like Tensorboard, OpenCV or `ale-py` to train on atari games. If you do not need those, you can use:\n",
        "```sh\n",
        "pip install stable-baselines3\n",
        "```\n",
        "\n",
        "Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more details and alternatives (from source, using docker).\n",
        "\n",
        "\n",
        "## Example\n",
        "\n",
        "Most of the code in the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms.\n",
        "\n",
        "Here is a quick example of how to train and run PPO on a cartpole environment:\n",
        "```python\n",
        "import gymnasium as gym\n",
        "\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
        "model.learn(total_timesteps=10_000)\n",
        "\n",
        "vec_env = model.get_env()\n",
        "obs = vec_env.reset()\n",
        "for i in range(1000):\n",
        "    action, _states = model.predict(obs, deterministic=True)\n",
        "    obs, reward, done, info = vec_env.step(action)\n",
        "    vec_env.render()\n",
        "    # VecEnv resets automatically\n",
        "    # if done:\n",
        "    #   obs = env.reset()\n",
        "\n",
        "env.close()\n",
        "```\n",
        "\n",
        "Or just train a model with a one liner if [the environment is registered in Gymnasium](https://gymnasium.farama.org/tutorials/gymnasium_basics/environment_creation/#registering-envs) and if [the policy is registered](https://stable-baselines3.readthedocs.io/en/master/guide/custom_policy.html):\n",
        "\n",
        "```python\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "model = PPO(\"MlpPolicy\", \"CartPole-v1\").learn(10_000)\n",
        "```\n",
        "\n",
        "Please read the [documentation](https://stable-baselines3.readthedocs.io/) for more examples.\n",
        "\n",
        "\n",
        "## Try it online with Colab Notebooks !\n",
        "\n",
        "All the following examples can be executed online using Google Colab notebooks:\n",
        "\n",
        "- [Full Tutorial](https://github.com/araffin/rl-tutorial-jnrr19)\n",
        "- [All Notebooks](https://github.com/Stable-Baselines-Team/rl-colab-notebooks/tree/sb3)\n",
        "- [Getting Started](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/stable_baselines_getting_started.ipynb)\n",
        "- [Training, Saving, Loading](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/saving_loading_dqn.ipynb)\n",
        "- [Multiprocessing](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/multiprocessing_rl.ipynb)\n",
        "- [Monitor Training and Plotting](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/monitor_training.ipynb)\n",
        "- [Atari Games](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/atari_games.ipynb)\n",
        "- [RL Baselines Zoo](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/rl-baselines-zoo.ipynb)\n",
        "- [PyBullet](https://colab.research.google.com/github/Stable-Baselines-Team/rl-colab-notebooks/blob/sb3/pybullet.ipynb)\n",
        "\n",
        "\n",
        "## Implemented Algorithms\n",
        "\n",
        "| **Name**         | **Recurrent**      | `Box`          | `Discrete`     | `MultiDiscrete` | `MultiBinary`  | **Multi Processing**              |\n",
        "| ------------------- | ------------------ | ------------------ | ------------------ | ------------------- | ------------------ | --------------------------------- |\n",
        "| ARS<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |\n",
        "| A2C   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: |\n",
        "| CrossQ<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| DDPG  | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| DQN   | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| HER   | :x: | :heavy_check_mark: | :heavy_check_mark: | :x: | :x: | :heavy_check_mark: |\n",
        "| PPO   | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n",
        "| QR-DQN<sup>[1](#f1)</sup>  | :x: | :x: | :heavy_check_mark: | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| RecurrentPPO<sup>[1](#f1)</sup>   | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n",
        "| SAC   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| TD3   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x:                | :heavy_check_mark: |\n",
        "| TQC<sup>[1](#f1)</sup>   | :x: | :heavy_check_mark: | :x:                | :x:                 | :x: | :heavy_check_mark: |\n",
        "| TRPO<sup>[1](#f1)</sup>  | :x: | :heavy_check_mark: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark: |\n",
        "| Maskable PPO<sup>[1](#f1)</sup>   | :x: | :x: | :heavy_check_mark: | :heavy_check_mark:  | :heavy_check_mark: | :heavy_check_mark:  |\n",
        "\n",
        "<b id=\"f1\">1</b>: Implemented in [SB3 Contrib](https://github.com/Stable-Baselines-Team/stable-baselines3-contrib) GitHub repository.\n",
        "\n",
        "Actions `gymnasium.spaces`:\n",
        " * `Box`: A N-dimensional box that contains every point in the action space.\n",
        " * `Discrete`: A list of possible actions, where each timestep only one of the actions can be used.\n",
        " * `MultiDiscrete`: A list of possible actions, where each timestep only one action of each discrete set can be used.\n",
        " * `MultiBinary`: A list of possible actions, where each timestep any of the actions can be used in any combination.\n",
        "\n",
        "\n",
        "\n",
        "## Testing the installation\n",
        "### Install dependencies\n",
        "```sh\n",
        "pip install -e .[docs,tests,extra]\n",
        "```\n",
        "### Run tests\n",
        "All unit tests in stable baselines3 can be run using `pytest` runner:\n",
        "```sh\n",
        "make pytest\n",
        "```\n",
        "To run a single test file:\n",
        "```sh\n",
        "python3 -m pytest -v tests/test_env_checker.py\n",
        "```\n",
        "To run a single test:\n",
        "```sh\n",
        "python3 -m pytest -v -k 'test_check_env_dict_action'\n",
        "```\n",
        "\n",
        "You can also do a static type check using `mypy`:\n",
        "```sh\n",
        "pip install mypy\n",
        "make type\n",
        "```\n",
        "\n",
        "Codestyle check with `ruff`:\n",
        "```sh\n",
        "pip install ruff\n",
        "make lint\n",
        "```\n",
        "\n",
        "## Projects Using Stable-Baselines3\n",
        "\n",
        "We try to maintain a list of projects using stable-baselines3 in the [documentation](https://stable-baselines3.readthedocs.io/en/master/misc/projects.html),\n",
        "please tell us if you want your project to appear on this page ;)\n",
        "\n",
        "## Citing the Project\n",
        "\n",
        "To cite this repository in publications:\n",
        "\n",
        "```bibtex\n",
        "@article{stable-baselines3,\n",
        "  author  = {Antonin Raffin and Ashley Hill and Adam Gleave and Anssi Kanervisto and Maximilian Ernestus and Noah Dormann},\n",
        "  title   = {Stable-Baselines3: Reliable Reinforcement Learning Implementations},\n",
        "  journal = {Journal of Machine Learning Research},\n",
        "  year    = {2021},\n",
        "  volume  = {22},\n",
        "  number  = {268},\n",
        "  pages   = {1-8},\n",
        "  url     = {http://jmlr.org/papers/v22/20-1364.html}\n",
        "}\n",
        "```\n",
        "\n",
        "Note: If you need to refer to a specific version of SB3, you can also use the [Zenodo DOI](https://doi.org/10.5281/zenodo.8123988).\n",
        "\n",
        "## Maintainers\n",
        "\n",
        "Stable-Baselines3 is currently maintained by [Ashley Hill](https://github.com/hill-a) (aka @hill-a), [Antonin Raffin](https://araffin.github.io/) (aka [@araffin](https://github.com/araffin)), [Maximilian Ernestus](https://github.com/ernestum) (aka @ernestum), [Adam Gleave](https://github.com/adamgleave) (@AdamGleave), [Anssi Kanervisto](https://github.com/Miffyli) (@Miffyli) and [Quentin Galloudec](https://gallouedec.com/) (@qgallouedec).\n",
        "\n",
        "**Important Note: We do not provide technical support, or consulting** and do not answer personal questions via email.\n",
        "Please post your question on the [RL Discord](https://discord.com/invite/xhfNqQv), [Reddit](https://www.reddit.com/r/reinforcementlearning/), or [Stack Overflow](https://stackoverflow.com/) in that case.\n",
        "\n",
        "\n",
        "## How To Contribute\n",
        "\n",
        "To any interested in making the baselines better, there is still some documentation that needs to be done.\n",
        "If you want to contribute, please read [**CONTRIBUTING.md**](./CONTRIBUTING.md) guide first.\n",
        "\n",
        "## Acknowledgments\n",
        "\n",
        "The initial work to develop Stable Baselines3 was partially funded by the project *Reduced Complexity Models* from the *Helmholtz-Gemeinschaft Deutscher Forschungszentren*, and by the EU's Horizon 2020 Research and Innovation Programme under grant number 951992 ([VeriDream](https://www.veridream.eu/)).\n",
        "\n",
        "The original version, Stable Baselines, was created in the [robotics lab U2IS](http://u2is.ensta-paristech.fr/index.php?lang=en) ([INRIA Flowers](https://flowers.inria.fr/) team) at [ENSTA ParisTech](http://www.ensta-paristech.fr/en).\n",
        "\n",
        "\n",
        "Logo credits: [L.M. Tenkes](https://www.instagram.com/lucillehue/)\n"
      ],
      "metadata": {
        "id": "OCGvFl7M7prf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ".gitignore"
      ],
      "metadata": {
        "id": "yPmjRMA_8ACi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ".readthedocs.yml"
      ],
      "metadata": {
        "id": "hzzayour8LaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Docs configuration file\n",
        "# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n",
        "\n",
        "# Required\n",
        "version: 2\n",
        "\n",
        "# Build documentation in the docs/ directory with Sphinx\n",
        "sphinx:\n",
        "  configuration: docs/conf.py\n",
        "\n",
        "# Optionally build your docs in additional formats such as PDF and ePub\n",
        "formats: all\n",
        "\n",
        "# Set requirements using conda env\n",
        "conda:\n",
        "  environment: docs/conda_env.yml\n",
        "\n",
        "build:\n",
        "  os: ubuntu-24.04\n",
        "  tools:\n",
        "    python: \"mambaforge-23.11\""
      ],
      "metadata": {
        "id": "g2e8QlZz8bxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CODE_OF_CONDUCT.md"
      ],
      "metadata": {
        "id": "SOA0zyzF8mEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Contributor Covenant Code of Conduct\n",
        "\n",
        "## Our Pledge\n",
        "\n",
        "We as members, contributors, and leaders pledge to make participation in our\n",
        "community a harassment-free experience for everyone, regardless of age, body\n",
        "size, visible or invisible disability, ethnicity, sex characteristics, gender\n",
        "identity and expression, level of experience, education, socioeconomic status,\n",
        "nationality, personal appearance, race, religion, or sexual identity\n",
        "and orientation.\n",
        "\n",
        "We pledge to act and interact in ways that contribute to an open, welcoming,\n",
        "diverse, inclusive, and healthy community.\n",
        "\n",
        "## Our Standards\n",
        "\n",
        "Examples of behavior that contributes to a positive environment for our\n",
        "community include:\n",
        "\n",
        "* Demonstrating empathy and kindness toward other people\n",
        "* Being respectful of differing opinions, viewpoints, and experiences\n",
        "* Giving and gracefully accepting constructive feedback\n",
        "* Accepting responsibility and apologizing to those affected by our mistakes,\n",
        "  and learning from the experience\n",
        "* Focusing on what is best not just for us as individuals, but for the\n",
        "  overall community\n",
        "\n",
        "Examples of unacceptable behavior include:\n",
        "\n",
        "* The use of sexualized language or imagery, and sexual attention or\n",
        "  advances of any kind\n",
        "* Trolling, insulting or derogatory comments, and personal or political attacks\n",
        "* Public or private harassment\n",
        "* Publishing others' private information, such as a physical or email\n",
        "  address, without their explicit permission\n",
        "* Other conduct which could reasonably be considered inappropriate in a\n",
        "  professional setting\n",
        "\n",
        "## Enforcement Responsibilities\n",
        "\n",
        "Community leaders are responsible for clarifying and enforcing our standards of\n",
        "acceptable behavior and will take appropriate and fair corrective action in\n",
        "response to any behavior that they deem inappropriate, threatening, offensive,\n",
        "or harmful.\n",
        "\n",
        "Community leaders have the right and responsibility to remove, edit, or reject\n",
        "comments, commits, code, wiki edits, issues, and other contributions that are\n",
        "not aligned to this Code of Conduct, and will communicate reasons for moderation\n",
        "decisions when appropriate.\n",
        "\n",
        "## Scope\n",
        "\n",
        "This Code of Conduct applies within all community spaces, and also applies when\n",
        "an individual is officially representing the community in public spaces.\n",
        "Examples of representing our community include using an official e-mail address,\n",
        "posting via an official social media account, or acting as an appointed\n",
        "representative at an online or offline event.\n",
        "\n",
        "## Enforcement\n",
        "\n",
        "Instances of abusive, harassing, or otherwise unacceptable behavior may be\n",
        "reported to the community leaders responsible for enforcement at\n",
        "antonin [dot] raffin [at] dlr [dot] de.\n",
        "All complaints will be reviewed and investigated promptly and fairly.\n",
        "\n",
        "All community leaders are obligated to respect the privacy and security of the\n",
        "reporter of any incident.\n",
        "\n",
        "## Enforcement Guidelines\n",
        "\n",
        "Community leaders will follow these Community Impact Guidelines in determining\n",
        "the consequences for any action they deem in violation of this Code of Conduct:\n",
        "\n",
        "### 1. Correction\n",
        "\n",
        "**Community Impact**: Use of inappropriate language or other behavior deemed\n",
        "unprofessional or unwelcome in the community.\n",
        "\n",
        "**Consequence**: A private, written warning from community leaders, providing\n",
        "clarity around the nature of the violation and an explanation of why the\n",
        "behavior was inappropriate. A public apology may be requested.\n",
        "\n",
        "### 2. Warning\n",
        "\n",
        "**Community Impact**: A violation through a single incident or series\n",
        "of actions.\n",
        "\n",
        "**Consequence**: A warning with consequences for continued behavior. No\n",
        "interaction with the people involved, including unsolicited interaction with\n",
        "those enforcing the Code of Conduct, for a specified period of time. This\n",
        "includes avoiding interactions in community spaces as well as external channels\n",
        "like social media. Violating these terms may lead to a temporary or\n",
        "permanent ban.\n",
        "\n",
        "### 3. Temporary Ban\n",
        "\n",
        "**Community Impact**: A serious violation of community standards, including\n",
        "sustained inappropriate behavior.\n",
        "\n",
        "**Consequence**: A temporary ban from any sort of interaction or public\n",
        "communication with the community for a specified period of time. No public or\n",
        "private interaction with the people involved, including unsolicited interaction\n",
        "with those enforcing the Code of Conduct, is allowed during this period.\n",
        "Violating these terms may lead to a permanent ban.\n",
        "\n",
        "### 4. Permanent Ban\n",
        "\n",
        "**Community Impact**: Demonstrating a pattern of violation of community\n",
        "standards, including sustained inappropriate behavior,  harassment of an\n",
        "individual, or aggression toward or disparagement of classes of individuals.\n",
        "\n",
        "**Consequence**: A permanent ban from any sort of public interaction within\n",
        "the community.\n",
        "\n",
        "## Attribution\n",
        "\n",
        "This Code of Conduct is adapted from the [Contributor Covenant][homepage],\n",
        "version 2.0, available at\n",
        "https://www.contributor-covenant.org/version/2/0/code_of_conduct.html.\n",
        "\n",
        "Community Impact Guidelines were inspired by [Mozilla's code of conduct\n",
        "enforcement ladder](https://github.com/mozilla/diversity).\n",
        "\n",
        "[homepage]: https://www.contributor-covenant.org\n",
        "\n",
        "For answers to common questions about this code of conduct, see the FAQ at\n",
        "https://www.contributor-covenant.org/faq. Translations are available at\n",
        "https://www.contributor-covenant.org/translations."
      ],
      "metadata": {
        "id": "JtNR8S_a9RFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pyproject.toml"
      ],
      "metadata": {
        "id": "sPF7IJV_9vq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "[tool.ruff]\n",
        "# Same as Black.\n",
        "line-length = 127\n",
        "# Assume Python 3.9\n",
        "target-version = \"py39\"\n",
        "\n",
        "[tool.ruff.lint]\n",
        "# See https://beta.ruff.rs/docs/rules/\n",
        "select = [\"E\", \"F\", \"B\", \"UP\", \"C90\", \"RUF\"]\n",
        "# B028: Ignore explicit stacklevel`\n",
        "# RUF013: Too many false positives (implicit optional)\n",
        "ignore = [\"B028\", \"RUF013\"]\n",
        "\n",
        "[tool.ruff.lint.per-file-ignores]\n",
        "# Default implementation in abstract methods\n",
        "\"./stable_baselines3/common/callbacks.py\" = [\"B027\"]\n",
        "\"./stable_baselines3/common/noise.py\" = [\"B027\"]\n",
        "# ClassVar, implicit optional check not needed for tests\n",
        "\"./tests/*.py\" = [\"RUF012\", \"RUF013\"]\n",
        "\n",
        "[tool.ruff.lint.mccabe]\n",
        "# Unlike Flake8, default to a complexity level of 10.\n",
        "max-complexity = 15\n",
        "\n",
        "[tool.black]\n",
        "line-length = 127\n",
        "\n",
        "[tool.mypy]\n",
        "ignore_missing_imports = true\n",
        "follow_imports = \"silent\"\n",
        "show_error_codes = true\n",
        "exclude = \"\"\"(?x)(\n",
        "    tests/test_logger.py$\n",
        "    | tests/test_train_eval_mode.py$\n",
        "  )\"\"\"\n",
        "\n",
        "[tool.pytest.ini_options]\n",
        "# Deterministic ordering for tests; useful for pytest-xdist.\n",
        "env = [\"PYTHONHASHSEED=0\"]\n",
        "\n",
        "filterwarnings = [\n",
        "    # A2C/PPO on GPU\n",
        "    \"ignore:You are trying to run (PPO|A2C) on the GPU\",\n",
        "    # Tensorboard warnings\n",
        "    \"ignore::DeprecationWarning:tensorboard\",\n",
        "    # Gymnasium warnings\n",
        "    \"ignore::UserWarning:gymnasium\",\n",
        "    # tqdm warning about rich being experimental\n",
        "    \"ignore:rich is experimental\",\n",
        "]\n",
        "markers = [\n",
        "    \"expensive: marks tests as expensive (deselect with '-m \\\"not expensive\\\"')\",\n",
        "]\n",
        "\n",
        "[tool.coverage.run]\n",
        "disable_warnings = [\"couldnt-parse\"]\n",
        "branch = false\n",
        "omit = [\n",
        "    \"tests/*\",\n",
        "    \"setup.py\",\n",
        "    # Require graphical interface\n",
        "    \"stable_baselines3/common/results_plotter.py\",\n",
        "    # Require ffmpeg\n",
        "    \"stable_baselines3/common/vec_env/vec_video_recorder.py\",\n",
        "]\n",
        "\n",
        "[tool.coverage.report]\n",
        "exclude_lines = [\n",
        "    \"pragma: no cover\",\n",
        "    \"raise NotImplementedError()\",\n",
        "    \"if typing.TYPE_CHECKING:\",\n",
        "]"
      ],
      "metadata": {
        "id": "0PkQ1ZTv-HbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "setup.py"
      ],
      "metadata": {
        "id": "Xyfmwbcq-SPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from setuptools import find_packages, setup\n",
        "\n",
        "with open(os.path.join(\"stable_baselines3\", \"version.txt\")) as file_handler:\n",
        "    __version__ = file_handler.read().strip()\n",
        "\n",
        "\n",
        "long_description = \"\"\"\n",
        "\n",
        "# Stable Baselines3\n",
        "\n",
        "Stable Baselines3 is a set of reliable implementations of reinforcement learning algorithms in PyTorch. It is the next major version of [Stable Baselines](https://github.com/hill-a/stable-baselines).\n",
        "\n",
        "These algorithms will make it easier for the research community and industry to replicate, refine, and identify new ideas, and will create good baselines to build projects on top of. We expect these tools will be used as a base around which new ideas can be added, and as a tool for comparing a new approach against existing ones. We also hope that the simplicity of these tools will allow beginners to experiment with a more advanced toolset, without being buried in implementation details.\n",
        "\n",
        "\n",
        "## Links\n",
        "\n",
        "Repository:\n",
        "https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Blog post:\n",
        "https://araffin.github.io/post/sb3/\n",
        "\n",
        "Documentation:\n",
        "https://stable-baselines3.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 Zoo:\n",
        "https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "SB3 Contrib:\n",
        "https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
        "\n",
        "## Quick example\n",
        "\n",
        "Most of the library tries to follow a sklearn-like syntax for the Reinforcement Learning algorithms using Gym.\n",
        "\n",
        "Here is a quick example of how to train and run PPO on a cartpole environment:\n"
      ],
      "metadata": {
        "id": "Kprl4Uhy-0I_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Makefile"
      ],
      "metadata": {
        "id": "omeMIwk__KLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SHELL=/bin/bash\n",
        "LINT_PATHS=stable_baselines3/ tests/ docs/conf.py setup.py\n",
        "\n",
        "pytest:\n",
        "\t./scripts/run_tests.sh\n",
        "\n",
        "mypy:\n",
        "\tmypy ${LINT_PATHS}\n",
        "\n",
        "missing-annotations:\n",
        "\tmypy --disallow-untyped-calls --disallow-untyped-defs --ignore-missing-imports stable_baselines3\n",
        "\n",
        "# missing docstrings\n",
        "# pylint -d R,C,W,E -e C0116 stable_baselines3 -j 4\n",
        "\n",
        "type: mypy\n",
        "\n",
        "lint:\n",
        "\t# stop the build if there are Python syntax errors or undefined names\n",
        "\t# see https://www.flake8rules.com/\n",
        "\truff check ${LINT_PATHS} --select=E9,F63,F7,F82 --output-format=full\n",
        "\t# exit-zero treats all errors as warnings.\n",
        "\truff check ${LINT_PATHS} --exit-zero --output-format=concise\n",
        "\n",
        "format:\n",
        "\t# Sort imports\n",
        "\truff check --select I ${LINT_PATHS} --fix\n",
        "\t# Reformat using black\n",
        "\tblack ${LINT_PATHS}\n",
        "\n",
        "check-codestyle:\n",
        "\t# Sort imports\n",
        "\truff check --select I ${LINT_PATHS}\n",
        "\t# Reformat using black\n",
        "\tblack --check ${LINT_PATHS}\n",
        "\n",
        "commit-checks: format type lint\n",
        "\n",
        "doc:\n",
        "\tcd docs && make html\n",
        "\n",
        "spelling:\n",
        "\tcd docs && make spelling\n",
        "\n",
        "clean:\n",
        "\tcd docs && make clean\n",
        "\n",
        "# Build docker images\n",
        "# If you do export RELEASE=True, it will also push them\n",
        "docker: docker-cpu docker-gpu\n",
        "\n",
        "docker-cpu:\n",
        "\t./scripts/build_docker.sh\n",
        "\n",
        "docker-gpu:\n",
        "\tUSE_GPU=True ./scripts/build_docker.sh\n",
        "\n",
        "# PyPi package release\n",
        "release:\n",
        "\tpython -m build\n",
        "\ttwine upload dist/*\n",
        "\n",
        "# Test PyPi package release\n",
        "test-release:\n",
        "\tpython -m build\n",
        "\ttwine upload --repository-url https://test.pypi.org/legacy/ dist/*\n",
        "\n",
        ".PHONY: clean spelling doc lint format check-codestyle commit-checks"
      ],
      "metadata": {
        "id": "r3w_pWTn_Obn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
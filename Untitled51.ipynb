{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNDkswPd85q5/2Z3DTv4amb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/Untitled51.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "id": "G4hldTx2asp3",
        "outputId": "238e77c0-947a-417c-8ad3-0802bb999e0a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 9) (<ipython-input-2-519c8ecc98a0>, line 9)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-519c8ecc98a0>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    Cet exemple montre comment former un DQN (Deep Q Networks) agent sur l'environnement Cartpole en utilisant la bibliothèque TF-agents.\u001b[0m\n\u001b[0m                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 9)\n"
          ]
        }
      ],
      "source": [
        "Former un réseau Deep Q avec des agents TF\n",
        "\n",
        "bookmark_border\n",
        "Copyright 2021 Les auteurs TF-Agents.\n",
        "Exécuter dans Google Colab\n",
        "Voir la source sur GitHub\n",
        "Télécharger le cahier\n",
        "introduction\n",
        "Cet exemple montre comment former un DQN (Deep Q Networks) agent sur l'environnement Cartpole en utilisant la bibliothèque TF-agents.\n",
        "\n",
        "Environnement Cartpole\n",
        "\n",
        "Il vous guidera à travers tous les composants d'un pipeline d'apprentissage par renforcement (RL) pour la formation, l'évaluation et la collecte de données.\n",
        "\n",
        "Pour exécuter ce code en direct, cliquez sur le lien \"Exécuter dans Google Colab\" ci-dessus.\n",
        "\n",
        "Installer\n",
        "Si vous n'avez pas installé les dépendances suivantes, exécutez :\n",
        "\n",
        "\n",
        "sudo apt-get update\n",
        "sudo apt-get install -y xvfb ffmpeg freeglut3-dev\n",
        "pip install 'imageio==2.4.0'\n",
        "pip install pyvirtualdisplay\n",
        "pip install tf-agents[reverb]\n",
        "pip install pyglet\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import base64\n",
        "import imageio\n",
        "import IPython\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import pyvirtualdisplay\n",
        "import reverb\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym\n",
        "from tf_agents.environments import tf_py_environment\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy\n",
        "from tf_agents.policies import random_tf_policy\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer\n",
        "from tf_agents.replay_buffers import reverb_utils\n",
        "from tf_agents.trajectories import trajectory\n",
        "from tf_agents.specs import tensor_spec\n",
        "from tf_agents.utils import common\n",
        "\n",
        "# Set up a virtual display for rendering OpenAI gym environments.\n",
        "display = pyvirtualdisplay.Display(visible=0, size=(1400, 900)).start()\n",
        "\n",
        "tf.version.VERSION\n",
        "\n",
        "'2.6.0'\n",
        "Hyperparamètres\n",
        "\n",
        "num_iterations = 20000 # @param {type:\"integer\"}\n",
        "\n",
        "initial_collect_steps = 100  # @param {type:\"integer\"}\n",
        "collect_steps_per_iteration =   1# @param {type:\"integer\"}\n",
        "replay_buffer_max_length = 100000  # @param {type:\"integer\"}\n",
        "\n",
        "batch_size = 64  # @param {type:\"integer\"}\n",
        "learning_rate = 1e-3  # @param {type:\"number\"}\n",
        "log_interval = 200  # @param {type:\"integer\"}\n",
        "\n",
        "num_eval_episodes = 10  # @param {type:\"integer\"}\n",
        "eval_interval = 1000  # @param {type:\"integer\"}\n",
        "Environnement\n",
        "Dans l'apprentissage par renforcement (RL), un environnement représente la tâche ou le problème à résoudre. Environnements standard peuvent être créés dans TF-agents utilisant tf_agents.environments suites. TF-Agents propose des suites pour le chargement d'environnements à partir de sources telles que OpenAI Gym, Atari et DM Control.\n",
        "\n",
        "Chargez l'environnement CartPole depuis la suite OpenAI Gym.\n",
        "\n",
        "\n",
        "env_name = 'CartPole-v0'\n",
        "env = suite_gym.load(env_name)\n",
        "Vous pouvez rendre cet environnement pour voir à quoi il ressemble. Un poteau à oscillation libre est attaché à un chariot. Le but est de déplacer le chariot à droite ou à gauche afin de garder le poteau pointé vers le haut.\n",
        "\n",
        "\n",
        "env.reset()\n",
        "PIL.Image.fromarray(env.render())\n",
        "png\n",
        "\n",
        "La environment.step méthode prend une action dans l'environnement et renvoie un TimeStep tuple contenant la prochaine observation de l'environnement et la récompense de l'action.\n",
        "\n",
        "Le time_step_spec() méthode retourne la spécification du TimeStep tuple. Son observation attribut montre la forme d'observations, les types de données, et les plages de valeurs autorisées. La reward attribut montre les mêmes détails pour la récompense.\n",
        "\n",
        "\n",
        "print('Observation Spec:')\n",
        "print(env.time_step_spec().observation)\n",
        "\n",
        "Observation Spec:\n",
        "BoundedArraySpec(shape=(4,), dtype=dtype('float32'), name='observation', minimum=[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], maximum=[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38])\n",
        "\n",
        "print('Reward Spec:')\n",
        "print(env.time_step_spec().reward)\n",
        "\n",
        "Reward Spec:\n",
        "ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
        "Le action_spec() méthode retourne la forme, les types de données et valeurs autorisées d'actions valides.\n",
        "\n",
        "\n",
        "print('Action Spec:')\n",
        "print(env.action_spec())\n",
        "\n",
        "Action Spec:\n",
        "BoundedArraySpec(shape=(), dtype=dtype('int64'), name='action', minimum=0, maximum=1)\n",
        "Dans l'environnement Cartpole :\n",
        "\n",
        "observation est un tableau de 4 flotteurs:\n",
        "la position et la vitesse du chariot\n",
        "la position angulaire et la vitesse du pôle\n",
        "reward est une valeur flottante scalaire\n",
        "l' action est un nombre entier scalaire avec seulement deux valeurs possibles:\n",
        "0 - « déplacer vers la gauche »\n",
        "1 - « droit de déplacement »\n",
        "\n",
        "time_step = env.reset()\n",
        "print('Time step:')\n",
        "print(time_step)\n",
        "\n",
        "action = np.array(1, dtype=np.int32)\n",
        "\n",
        "next_time_step = env.step(action)\n",
        "print('Next time step:')\n",
        "print(next_time_step)\n",
        "\n",
        "Time step:\n",
        "TimeStep(\n",
        "{'discount': array(1., dtype=float32),\n",
        " 'observation': array([-0.02109759, -0.00062286,  0.04167245, -0.03825747], dtype=float32),\n",
        " 'reward': array(0., dtype=float32),\n",
        " 'step_type': array(0, dtype=int32)})\n",
        "Next time step:\n",
        "TimeStep(\n",
        "{'discount': array(1., dtype=float32),\n",
        " 'observation': array([-0.02111005,  0.1938775 ,  0.0409073 , -0.31750655], dtype=float32),\n",
        " 'reward': array(1., dtype=float32),\n",
        " 'step_type': array(1, dtype=int32)})\n",
        "Habituellement, deux environnements sont instanciés : un pour la formation et un pour l'évaluation.\n",
        "\n",
        "\n",
        "train_py_env = suite_gym.load(env_name)\n",
        "eval_py_env = suite_gym.load(env_name)\n",
        "L'environnement Cartpole, comme la plupart des environnements, est écrit en Python pur. Ceci est converti en tensorflow en utilisant l' TFPyEnvironment emballage.\n",
        "\n",
        "L'API de l'environnement d'origine utilise des tableaux Numpy. Les TFPyEnvironment convertit en Tensors pour le rendre compatible avec les agents tensorflow et politiques.\n",
        "\n",
        "\n",
        "train_env = tf_py_environment.TFPyEnvironment(train_py_env)\n",
        "eval_env = tf_py_environment.TFPyEnvironment(eval_py_env)\n",
        "Agent\n",
        "L'algorithme utilisé pour résoudre un problème de RL est représenté par un Agent . TF-agents fournit des implémentations standard d'une variété d' Agents , y compris:\n",
        "\n",
        "DQN (utilisé dans ce tutoriel)\n",
        "RENFORCER\n",
        "DDPG\n",
        "TD3\n",
        "OPP\n",
        "SAC\n",
        "L'agent DQN peut être utilisé dans n'importe quel environnement qui a un espace d'action discret.\n",
        "\n",
        "Au cœur d'un agent DQN est un QNetwork , un modèle de réseau de neurones qui peut apprendre à prédire QValues (rendements attendus) pour toutes les actions, compte tenu de l'observation de l'environnement.\n",
        "\n",
        "Nous utiliserons tf_agents.networks. pour créer un QNetwork . Le réseau se compose d'une séquence de tf.keras.layers.Dense couches, la couche finale aura une sortie pour chaque mesure possible.\n",
        "\n",
        "\n",
        "fc_layer_params = (100, 50)\n",
        "action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "# Define a helper function to create Dense layers configured with the right\n",
        "# activation and kernel initializer.\n",
        "def dense_layer(num_units):\n",
        "  return tf.keras.layers.Dense(\n",
        "      num_units,\n",
        "      activation=tf.keras.activations.relu,\n",
        "      kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "          scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "# QNetwork consists of a sequence of Dense layers followed by a dense layer\n",
        "# with `num_actions` units to generate one q_value per available action as\n",
        "# its output.\n",
        "dense_layers = [dense_layer(num_units) for num_units in fc_layer_params]\n",
        "q_values_layer = tf.keras.layers.Dense(\n",
        "    num_actions,\n",
        "    activation=None,\n",
        "    kernel_initializer=tf.keras.initializers.RandomUniform(\n",
        "        minval=-0.03, maxval=0.03),\n",
        "    bias_initializer=tf.keras.initializers.Constant(-0.2))\n",
        "q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "Maintenant , utilisez tf_agents.agents.dqn.dqn_agent pour instancier un DqnAgent . En plus de la time_step_spec , action_spec et le QNetwork, le constructeur de l' agent exige également un optimiseur (dans ce cas, AdamOptimizer ), une fonction de perte, et un compteur de pas entier.\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
        "\n",
        "train_step_counter = tf.Variable(0)\n",
        "\n",
        "agent = dqn_agent.DqnAgent(\n",
        "    train_env.time_step_spec(),\n",
        "    train_env.action_spec(),\n",
        "    q_network=q_net,\n",
        "    optimizer=optimizer,\n",
        "    td_errors_loss_fn=common.element_wise_squared_loss,\n",
        "    train_step_counter=train_step_counter)\n",
        "\n",
        "agent.initialize()\n",
        "Stratégies\n",
        "Une politique définit la manière dont un agent agit dans un environnement. En règle générale, l'objectif de l'apprentissage par renforcement est de former le modèle sous-jacent jusqu'à ce que la politique produise le résultat souhaité.\n",
        "\n",
        "Dans ce tutoriel :\n",
        "\n",
        "Le résultat souhaité est de maintenir le poteau en équilibre au-dessus du chariot.\n",
        "La politique retourne une action (gauche ou droite) pour chaque time_step observation.\n",
        "Les agents contiennent deux stratégies :\n",
        "\n",
        "agent.policy - La politique principale qui est utilisée pour l' évaluation et le déploiement.\n",
        "agent.collect_policy - Une deuxième politique qui est utilisée pour la collecte de données.\n",
        "\n",
        "eval_policy = agent.policy\n",
        "collect_policy = agent.collect_policy\n",
        "Les politiques peuvent être créées indépendamment des agents. Par exemple, utiliser tf_agents.policies.random_tf_policy pour créer une politique qui sélectionnera au hasard une action pour chaque time_step .\n",
        "\n",
        "\n",
        "random_policy = random_tf_policy.RandomTFPolicy(train_env.time_step_spec(),\n",
        "                                                train_env.action_spec())\n",
        "Pour obtenir une action d'une politique, appelez le policy.action(time_step) méthode. Le time_step contient l'observation de l'environnement. Cette méthode renvoie un PolicyStep , qui est un tuple nommé avec trois composantes:\n",
        "\n",
        "l' 0 1 action - l'action à entreprendre (dans ce cas, 0 ou 1 )\n",
        "state - utilisé pour les politiques stateful (qui est, d' après-RNN)\n",
        "info - données auxiliaires, telles que les probabilités de log des actions\n",
        "\n",
        "example_environment = tf_py_environment.TFPyEnvironment(\n",
        "    suite_gym.load('CartPole-v0'))\n",
        "\n",
        "time_step = example_environment.reset()\n",
        "\n",
        "random_policy.action(time_step)\n",
        "\n",
        "PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int64, numpy=array([1])>, state=(), info=())\n",
        "Métriques et évaluation\n",
        "La mesure la plus couramment utilisée pour évaluer une politique est le rendement moyen. Le retour est la somme des récompenses obtenues lors de l'exécution d'une politique dans un environnement pour un épisode. Plusieurs épisodes sont exécutés, créant un rendement moyen.\n",
        "\n",
        "La fonction suivante calcule le rendement moyen d'une politique, compte tenu de la politique, de l'environnement et du nombre d'épisodes.\n",
        "\n",
        "\n",
        "def compute_avg_return(environment, policy, num_episodes=10):\n",
        "\n",
        "  total_return = 0.0\n",
        "  for _ in range(num_episodes):\n",
        "\n",
        "    time_step = environment.reset()\n",
        "    episode_return = 0.0\n",
        "\n",
        "    while not time_step.is_last():\n",
        "      action_step = policy.action(time_step)\n",
        "      time_step = environment.step(action_step.action)\n",
        "      episode_return += time_step.reward\n",
        "    total_return += episode_return\n",
        "\n",
        "  avg_return = total_return / num_episodes\n",
        "  return avg_return.numpy()[0]\n",
        "\n",
        "\n",
        "# See also the metrics module for standard implementations of different metrics.\n",
        "# https://github.com/tensorflow/agents/tree/master/tf_agents/metrics\n",
        "L' exécution de ce calcul sur la random_policy montre une performance de base dans l'environnement.\n",
        "\n",
        "\n",
        "compute_avg_return(eval_env, random_policy, num_eval_episodes)\n",
        "\n",
        "20.7\n",
        "Tampon de relecture\n",
        "Afin de garder une trace des données recueillies à partir de l'environnement, nous allons utiliser la réverbération , un système de lecture efficace, extensible et facile à utiliser par Deepmind. Il stocke les données d'expérience lorsque nous collectons des trajectoires et est consommé pendant l'entraînement.\n",
        "\n",
        "Ce tampon de relecture est construit à l'aide de spécifications décrivant les tenseurs à stocker, qui peuvent être obtenus auprès de l'agent à l'aide de agent.collect_data_spec.\n",
        "\n",
        "\n",
        "table_name = 'uniform_table'\n",
        "replay_buffer_signature = tensor_spec.from_spec(\n",
        "      agent.collect_data_spec)\n",
        "replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "    replay_buffer_signature)\n",
        "\n",
        "table = reverb.Table(\n",
        "    table_name,\n",
        "    max_size=replay_buffer_max_length,\n",
        "    sampler=reverb.selectors.Uniform(),\n",
        "    remover=reverb.selectors.Fifo(),\n",
        "    rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "    signature=replay_buffer_signature)\n",
        "\n",
        "reverb_server = reverb.Server([table])\n",
        "\n",
        "replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "    agent.collect_data_spec,\n",
        "    table_name=table_name,\n",
        "    sequence_length=2,\n",
        "    local_server=reverb_server)\n",
        "\n",
        "rb_observer = reverb_utils.ReverbAddTrajectoryObserver(\n",
        "  replay_buffer.py_client,\n",
        "  table_name,\n",
        "  sequence_length=2)\n",
        "\n",
        "[reverb/cc/platform/tfrecord_checkpointer.cc:150]  Initializing TFRecordCheckpointer in /tmp/tmpcz7e0i7c.\n",
        "[reverb/cc/platform/tfrecord_checkpointer.cc:385] Loading latest checkpoint from /tmp/tmpcz7e0i7c\n",
        "[reverb/cc/platform/default/server.cc:71] Started replay server on port 21909\n",
        "Pour la plupart des agents, collect_data_spec est un tuple nommé appelé Trajectory , contenant les spécifications pour les observations, les actions, les récompenses et autres articles.\n",
        "\n",
        "\n",
        "agent.collect_data_spec\n",
        "\n",
        "Trajectory(\n",
        "{'action': BoundedTensorSpec(shape=(), dtype=tf.int64, name='action', minimum=array(0), maximum=array(1)),\n",
        " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32)),\n",
        " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
        " 'observation': BoundedTensorSpec(shape=(4,), dtype=tf.float32, name='observation', minimum=array([-4.8000002e+00, -3.4028235e+38, -4.1887903e-01, -3.4028235e+38],\n",
        "      dtype=float32), maximum=array([4.8000002e+00, 3.4028235e+38, 4.1887903e-01, 3.4028235e+38],\n",
        "      dtype=float32)),\n",
        " 'policy_info': (),\n",
        " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
        " 'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type')})\n",
        "\n",
        "agent.collect_data_spec._fields\n",
        "\n",
        "('step_type',\n",
        " 'observation',\n",
        " 'action',\n",
        " 'policy_info',\n",
        " 'next_step_type',\n",
        " 'reward',\n",
        " 'discount')\n",
        "Collecte de données\n",
        "Exécutez maintenant la politique aléatoire dans l'environnement pendant quelques étapes, en enregistrant les données dans le tampon de relecture.\n",
        "\n",
        "Ici, nous utilisons 'PyDriver' pour exécuter la boucle de collecte d'expérience. Vous pouvez en apprendre davantage sur le pilote TF Agents dans notre tutoriel pilotes .\n",
        "\n",
        "\n",
        "py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      random_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=initial_collect_steps).run(train_py_env.reset())\n",
        "\n",
        "(TimeStep(\n",
        " {'discount': array(1., dtype=float32),\n",
        "  'observation': array([ 0.04100575,  0.16847703, -0.12718087, -0.6300714 ], dtype=float32),\n",
        "  'reward': array(1., dtype=float32),\n",
        "  'step_type': array(1, dtype=int32)}),\n",
        " ())\n",
        "Le tampon de relecture est maintenant une collection de trajectoires.\n",
        "\n",
        "\n",
        "# For the curious:\n",
        "# Uncomment to peel one of these off and inspect it.\n",
        "# iter(replay_buffer.as_dataset()).next()\n",
        "L'agent a besoin d'accéder au tampon de relecture. Ceci est assuré par la création d' un iterable tf.data.Dataset pipeline qui alimentera les données à l'agent.\n",
        "\n",
        "Chaque ligne du tampon de relecture ne stocke qu'une seule étape d'observation. Mais puisque l'agent DQN a besoin à la fois l'observation courante et à côté de calculer la perte, le pipeline du jeu de données échantillon deux rangées adjacentes pour chaque élément dans le lot ( num_steps=2 ).\n",
        "\n",
        "Cet ensemble de données est également optimisé en exécutant des appels parallèles et en prélevant des données.\n",
        "\n",
        "\n",
        "# Dataset generates trajectories with shape [Bx2x...]\n",
        "dataset = replay_buffer.as_dataset(\n",
        "    num_parallel_calls=3,\n",
        "    sample_batch_size=batch_size,\n",
        "    num_steps=2).prefetch(3)\n",
        "\n",
        "dataset\n",
        "\n",
        "<PrefetchDataset shapes: (Trajectory(\n",
        "{action: (64, 2),\n",
        " discount: (64, 2),\n",
        " next_step_type: (64, 2),\n",
        " observation: (64, 2, 4),\n",
        " policy_info: (),\n",
        " reward: (64, 2),\n",
        " step_type: (64, 2)}), SampleInfo(key=(64, 2), probability=(64, 2), table_size=(64, 2), priority=(64, 2))), types: (Trajectory(\n",
        "{action: tf.int64,\n",
        " discount: tf.float32,\n",
        " next_step_type: tf.int32,\n",
        " observation: tf.float32,\n",
        " policy_info: (),\n",
        " reward: tf.float32,\n",
        " step_type: tf.int32}), SampleInfo(key=tf.uint64, probability=tf.float64, table_size=tf.int64, priority=tf.float64))>\n",
        "\n",
        "iterator = iter(dataset)\n",
        "print(iterator)\n",
        "\n",
        "<tensorflow.python.data.ops.iterator_ops.OwnedIterator object at 0x7f3cec38cd90>\n",
        "\n",
        "# For the curious:\n",
        "# Uncomment to see what the dataset iterator is feeding to the agent.\n",
        "# Compare this representation of replay data\n",
        "# to the collection of individual trajectories shown earlier.\n",
        "\n",
        "# iterator.next()\n",
        "Formation de l'agent\n",
        "Deux choses doivent se produire pendant la boucle d'entraînement :\n",
        "\n",
        "collecter des données sur l'environnement\n",
        "utiliser ces données pour entraîner le ou les réseaux de neurones de l'agent\n",
        "Cet exemple évalue également périodiquement la politique et imprime le score actuel.\n",
        "\n",
        "Ce qui suit prendra environ 5 minutes pour s'exécuter.\n",
        "\n",
        "\n",
        "try:\n",
        "  %%time\n",
        "except:\n",
        "  pass\n",
        "\n",
        "# (Optional) Optimize by wrapping some of the code in a graph using TF function.\n",
        "agent.train = common.function(agent.train)\n",
        "\n",
        "# Reset the train step.\n",
        "agent.train_step_counter.assign(0)\n",
        "\n",
        "# Evaluate the agent's policy once before training.\n",
        "avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "returns = [avg_return]\n",
        "\n",
        "# Reset the environment.\n",
        "time_step = train_py_env.reset()\n",
        "\n",
        "# Create a driver to collect experience.\n",
        "collect_driver = py_driver.PyDriver(\n",
        "    env,\n",
        "    py_tf_eager_policy.PyTFEagerPolicy(\n",
        "      agent.collect_policy, use_tf_function=True),\n",
        "    [rb_observer],\n",
        "    max_steps=collect_steps_per_iteration)\n",
        "\n",
        "for _ in range(num_iterations):\n",
        "\n",
        "  # Collect a few steps and save to the replay buffer.\n",
        "  time_step, _ = collect_driver.run(time_step)\n",
        "\n",
        "  # Sample a batch of data from the buffer and update the agent's network.\n",
        "  experience, unused_info = next(iterator)\n",
        "  train_loss = agent.train(experience).loss\n",
        "\n",
        "  step = agent.train_step_counter.numpy()\n",
        "\n",
        "  if step % log_interval == 0:\n",
        "    print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "\n",
        "  if step % eval_interval == 0:\n",
        "    avg_return = compute_avg_return(eval_env, agent.policy, num_eval_episodes)\n",
        "    print('step = {0}: Average Return = {1}'.format(step, avg_return))\n",
        "    returns.append(avg_return)\n",
        "\n",
        "WARNING:tensorflow:From /tmpfs/src/tf_docs_env/lib/python3.7/site-packages/tensorflow/python/util/dispatch.py:206: calling foldr_v2 (from tensorflow.python.ops.functional_ops) with back_prop=False is deprecated and will be removed in a future version.\n",
        "Instructions for updating:\n",
        "back_prop=False is deprecated. Consider using tf.stop_gradient instead.\n",
        "Instead of:\n",
        "results = tf.foldr(fn, elems, back_prop=False)\n",
        "Use:\n",
        "results = tf.nest.map_structure(tf.stop_gradient, tf.foldr(fn, elems))\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "[reverb/cc/client.cc:163] Sampler and server are owned by the same process (15446) so Table uniform_table is accessed directly without gRPC.\n",
        "step = 200: loss = 27.080341339111328\n",
        "step = 400: loss = 3.0314550399780273\n",
        "step = 600: loss = 470.9187927246094\n",
        "step = 800: loss = 548.7870483398438\n",
        "step = 1000: loss = 4315.17578125\n",
        "step = 1000: Average Return = 48.400001525878906\n",
        "step = 1200: loss = 5297.24853515625\n",
        "step = 1400: loss = 11601.296875\n",
        "step = 1600: loss = 60482.578125\n",
        "step = 1800: loss = 802764.8125\n",
        "step = 2000: loss = 1689283.0\n",
        "step = 2000: Average Return = 63.400001525878906\n",
        "step = 2200: loss = 4928921.0\n",
        "step = 2400: loss = 5508345.0\n",
        "step = 2600: loss = 17888162.0\n",
        "step = 2800: loss = 23993148.0\n",
        "step = 3000: loss = 10192765.0\n",
        "step = 3000: Average Return = 74.0999984741211\n",
        "step = 3200: loss = 88318176.0\n",
        "step = 3400: loss = 77485728.0\n",
        "step = 3600: loss = 3236693504.0\n",
        "step = 3800: loss = 102289840.0\n",
        "step = 4000: loss = 168594496.0\n",
        "step = 4000: Average Return = 73.5999984741211\n",
        "step = 4200: loss = 348990528.0\n",
        "step = 4400: loss = 101819664.0\n",
        "step = 4600: loss = 136486208.0\n",
        "step = 4800: loss = 133454864.0\n",
        "step = 5000: loss = 592934784.0\n",
        "step = 5000: Average Return = 71.5999984741211\n",
        "step = 5200: loss = 216909120.0\n",
        "step = 5400: loss = 181369648.0\n",
        "step = 5600: loss = 600455680.0\n",
        "step = 5800: loss = 551183744.0\n",
        "step = 6000: loss = 368749824.0\n",
        "step = 6000: Average Return = 83.5\n",
        "step = 6200: loss = 1010418176.0\n",
        "step = 6400: loss = 171257856.0\n",
        "step = 6600: loss = 115424904.0\n",
        "step = 6800: loss = 144941152.0\n",
        "step = 7000: loss = 257932752.0\n",
        "step = 7000: Average Return = 107.0\n",
        "step = 7200: loss = 854109248.0\n",
        "step = 7400: loss = 95970128.0\n",
        "step = 7600: loss = 325583744.0\n",
        "step = 7800: loss = 858134016.0\n",
        "step = 8000: loss = 197960128.0\n",
        "step = 8000: Average Return = 124.19999694824219\n",
        "step = 8200: loss = 310187552.0\n",
        "step = 8400: loss = 572293760.0\n",
        "step = 8600: loss = 2338323456.0\n",
        "step = 8800: loss = 384659392.0\n",
        "step = 9000: loss = 676924544.0\n",
        "step = 9000: Average Return = 200.0\n",
        "step = 9200: loss = 946199168.0\n",
        "step = 9400: loss = 605189504.0\n",
        "step = 9600: loss = 768988928.0\n",
        "step = 9800: loss = 508231776.0\n",
        "step = 10000: loss = 526518016.0\n",
        "step = 10000: Average Return = 200.0\n",
        "step = 10200: loss = 1461528704.0\n",
        "step = 10400: loss = 709822016.0\n",
        "step = 10600: loss = 2770553344.0\n",
        "step = 10800: loss = 496421504.0\n",
        "step = 11000: loss = 1822116864.0\n",
        "step = 11000: Average Return = 200.0\n",
        "step = 11200: loss = 744854208.0\n",
        "step = 11400: loss = 778800384.0\n",
        "step = 11600: loss = 667049216.0\n",
        "step = 11800: loss = 586587648.0\n",
        "step = 12000: loss = 2586833920.0\n",
        "step = 12000: Average Return = 200.0\n",
        "step = 12200: loss = 1002041472.0\n",
        "step = 12400: loss = 1526919552.0\n",
        "step = 12600: loss = 1670877056.0\n",
        "step = 12800: loss = 1857608704.0\n",
        "step = 13000: loss = 1040727936.0\n",
        "step = 13000: Average Return = 200.0\n",
        "step = 13200: loss = 1807798656.0\n",
        "step = 13400: loss = 1457996544.0\n",
        "step = 13600: loss = 1322671616.0\n",
        "step = 13800: loss = 22940983296.0\n",
        "step = 14000: loss = 1556422912.0\n",
        "step = 14000: Average Return = 200.0\n",
        "step = 14200: loss = 2488473600.0\n",
        "step = 14400: loss = 46558289920.0\n",
        "step = 14600: loss = 1958968960.0\n",
        "step = 14800: loss = 4677744640.0\n",
        "step = 15000: loss = 1648418304.0\n",
        "step = 15000: Average Return = 200.0\n",
        "step = 15200: loss = 46132723712.0\n",
        "step = 15400: loss = 2189093888.0\n",
        "step = 15600: loss = 1204941056.0\n",
        "step = 15800: loss = 1578462080.0\n",
        "step = 16000: loss = 1695949312.0\n",
        "step = 16000: Average Return = 200.0\n",
        "step = 16200: loss = 19554553856.0\n",
        "step = 16400: loss = 2857277184.0\n",
        "step = 16600: loss = 5782225408.0\n",
        "step = 16800: loss = 2294467072.0\n",
        "step = 17000: loss = 2397877248.0\n",
        "step = 17000: Average Return = 200.0\n",
        "step = 17200: loss = 2910329088.0\n",
        "step = 17400: loss = 6317301760.0\n",
        "step = 17600: loss = 2733602048.0\n",
        "step = 17800: loss = 32502740992.0\n",
        "step = 18000: loss = 6295858688.0\n",
        "step = 18000: Average Return = 200.0\n",
        "step = 18200: loss = 2564860160.0\n",
        "step = 18400: loss = 76450430976.0\n",
        "step = 18600: loss = 6347636736.0\n",
        "step = 18800: loss = 6258629632.0\n",
        "step = 19000: loss = 8091572224.0\n",
        "step = 19000: Average Return = 200.0\n",
        "step = 19200: loss = 3860335616.0\n",
        "step = 19400: loss = 3552561152.0\n",
        "step = 19600: loss = 4175943424.0\n",
        "step = 19800: loss = 5975838720.0\n",
        "step = 20000: loss = 4709884928.0\n",
        "step = 20000: Average Return = 200.0\n",
        "Visualisation\n",
        "Parcelles\n",
        "Utilisez matplotlib.pyplot pour tracer comment la politique améliorée au cours de la formation.\n",
        "\n",
        "Une itération de Cartpole-v0 se compose de 200 pas de temps. L'environnement donne une récompense de +1 pour chaque étape , les séjours pôles, donc le rendement maximal pour un épisode 200. Les tableaux indique le rendement de plus en plus vers ce maximum à chaque fois qu'il est évalué au cours de la formation. (Il peut être un peu instable et ne pas augmenter de manière monotone à chaque fois.)\n",
        "\n",
        "\n",
        "iterations = range(0, num_iterations + 1, eval_interval)\n",
        "plt.plot(iterations, returns)\n",
        "plt.ylabel('Average Return')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylim(top=250)\n",
        "\n",
        "(40.82000160217285, 250.0)\n",
        "png\n",
        "\n",
        "Vidéos\n",
        "Les graphiques sont sympas. Mais le plus excitant est de voir un agent accomplir une tâche dans un environnement.\n",
        "\n",
        "Tout d'abord, créez une fonction pour intégrer des vidéos dans le bloc-notes.\n",
        "\n",
        "\n",
        "def embed_mp4(filename):\n",
        "  \"\"\"Embeds an mp4 file in the notebook.\"\"\"\n",
        "  video = open(filename,'rb').read()\n",
        "  b64 = base64.b64encode(video)\n",
        "  tag = '''\n",
        "  <video width=\"640\" height=\"480\" controls>\n",
        "    <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\">\n",
        "  Your browser does not support the video tag.\n",
        "  </video>'''.format(b64.decode())\n",
        "\n",
        "  return IPython.display.HTML(tag)\n",
        "Parcourez maintenant quelques épisodes du jeu Cartpole avec l'agent. L'environnement Python sous - jacent (celle « à l' intérieur » de l'enveloppe de l' environnement tensorflow) fournit un render() méthode, qui délivre en sortie une image de l'état de l' environnement. Ceux-ci peuvent être rassemblés dans une vidéo.\n",
        "\n",
        "\n",
        "def create_policy_eval_video(policy, filename, num_episodes=5, fps=30):\n",
        "  filename = filename + \".mp4\"\n",
        "  with imageio.get_writer(filename, fps=fps) as video:\n",
        "    for _ in range(num_episodes):\n",
        "      time_step = eval_env.reset()\n",
        "      video.append_data(eval_py_env.render())\n",
        "      while not time_step.is_last():\n",
        "        action_step = policy.action(time_step)\n",
        "        time_step = eval_env.step(action_step.action)\n",
        "        video.append_data(eval_py_env.render())\n",
        "  return embed_mp4(filename)\n",
        "\n",
        "create_policy_eval_video(agent.policy, \"trained-agent\")\n",
        "\n",
        "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n",
        "[swscaler @ 0x55d99fdf83c0] Warning: data is not aligned! This can lead to a speed loss\n",
        "Pour le plaisir, comparez l'agent entraîné (ci-dessus) à un agent se déplaçant au hasard. (Ça ne marche pas aussi bien.)\n",
        "\n",
        "\n",
        "create_policy_eval_video(random_policy, \"random-agent\")\n",
        "\n",
        "WARNING:root:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (400, 600) to (400, 608) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to None (risking incompatibility). You may also see a FFMPEG warning concerning speedloss due to data not being aligned.\n",
        "[swscaler @ 0x55ffa7fe73c0] Warning: data is not aligned! This can lead to a speed loss\n",
        "Ce contenu vous a-t-il été utile ?\n",
        "\n",
        "Sauf indication contraire, le contenu de cette page est régi par une licence Creative Commons Attribution 4.0, et les échantillons de code sont régis par une licence Apache 2.0. Pour en savoir plus, consultez les Règles du site Google Developers. Java est une marque déposée d'Oracle et/ou de ses sociétés affiliées.\n",
        "\n",
        "Dernière mise à jour le 2021/12/08 (UTC).\n",
        "\n",
        "Rester connecté\n",
        "Blog\n",
        "Forum\n",
        "GitHub\n",
        "Twitter\n",
        "YouTube\n",
        "Support\n",
        "Outil de suivi des incidents\n",
        "Notes de version\n",
        "Stack Overflow\n",
        "Consignes relatives à la marque\n",
        "Citer TensorFlow"
      ]
    }
  ]
}
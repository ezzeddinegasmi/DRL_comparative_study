{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOltkeaB9SDwXcAfx2QBfEd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/PPO_Mard_12_h.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZccKITmY-h-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GyYmCpH89Nnb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "714325e4-d4c4-491e-d5f3-ff45abc4e224"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install stable-baselines3[extra] gymnasium wandb numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uumz7N13Xixp",
        "outputId": "e5919627-3f67-4b49-f3da-0d6fe90974bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: stable-baselines3[extra] in /usr/local/lib/python3.11/dist-packages (2.6.0)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.11.0.86)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (0.10.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]) (11.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (4.13.1)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (24.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3[extra]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]) (2.18.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TYc80UY0YCv-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hdYL4Ykr-iG8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "64TFaTtE9Nnc"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from typing import List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " git clone https://github.com/Neilus03/DRL_comparative_study"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "pUBqhnPmWvQa",
        "outputId": "8e1b7f63-9f7f-4d32-a67b-98f656ec0e75"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-2d3d2f31762b>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-2d3d2f31762b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https://github.com/Neilus03/DRL_comparative_study\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO"
      ],
      "metadata": {
        "id": "-fb0BhLBAB8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/README.md"
      ],
      "metadata": {
        "id": "dCKPjR6DYGUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Breakout Reinforcement Learning Implementations\n",
        "\n",
        "Welcome to the Breakout Reinforcement Learning Implementations repository. This project includes various implementations of Reinforcement Learning algorithms applied to the classic Breakout game environment. It aims to provide reproducible results for the research presented in our paper _A COMPARATIVE STUDY OF DEEP REINFORCEMENT\n",
        "LEARNING MODELS: DQN VS PPO VS A2C_.\n",
        "\n",
        "## ðŸ”© Directory Structure\n",
        "\n",
        "The repository is organized into the following directories, each containing a specific approach to solving Breakout using different RL algorithms:\n",
        "\n",
        "- **`BreakOut_base`**:\n",
        "  - *Description*: Base implementation of the Deep Q-Network (DQN) for the Breakout game.\n",
        "\n",
        "- **`BreakOut_sb3_A2C`**:\n",
        "  - Implementation of the Advantage Actor-Critic (A2C) algorithm using Stable Baselines 3.\n",
        "\n",
        "- **`BreakOut_sb3_PPO`**:\n",
        "  - Proximal Policy Optimization (PPO) approach using Stable Baselines 3.\n",
        "- **`Breakout_sb3_DQN`**:\n",
        "  - Implementation of DQN using Stable Baselines 3.\n",
        "\n",
        "*Details*: The README in this folder will guide you through the necessary steps for training and testing.\n",
        "\n",
        "## ðŸ“¦ Installation\n",
        "\n",
        "Ensure Python 3.x is installed along with the following dependencies, which are common across all implementations:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "## ðŸŽ® Usage\n",
        "\n",
        "1. **Clone & Set Up** this repository:\n",
        "    ```bash\n",
        "    git clone https://github.com/Neilus03/DRL_comparative_study\n",
        "    cd DRL_comparative_study\n",
        "    pip install -r requirements.txt\n",
        "    ```\n",
        "\n",
        "2. **Navigate** to the desired implementation directory, e.g., for PPO:\n",
        "    ```bash\n",
        "    cd BreakOut_sb3_PPO\n",
        "    ```\n",
        "\n",
        "3. **Configure** the `config.py` file to adjust the model training, the wandb account, and the saving model options.\n",
        "\n",
        "4. **Execute** the `train.py` to run the model or `test.py` if you already have a pre-trained model to test.\n",
        "\n",
        "## ðŸ‘¥ Contributing\n",
        "\n",
        "We welcome contributions! If you have suggestions or improvements, feel free to create a pull request or open an issue.\n",
        "\n",
        "## ðŸ“§ Contact\n",
        "\n",
        "For any questions or inquiries, please reach out to:\n",
        "\n",
        "[Daniel Vidal](https://www.linkedin.com/in/daniel-alejandro-vidal-guerra-21386b266/)\n",
        "\n",
        "[Neil de la Fuente](https://www.linkedin.com/in/neil-de-la-fuente/)\n",
        "\n",
        "---\n",
        "\n",
        "Thank you for visiting! We hope you find this repository useful for reproducing the results presented in our paper.\n"
      ],
      "metadata": {
        "id": "F1HXe5T-YMEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3JTNENcZ9nl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/config.py"
      ],
      "metadata": {
        "id": "hLKcV2AtaRVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from stable_baselines3.common.utils import get_latest_run_id\n",
        "import torch\n",
        "\n",
        "\n",
        "'''FILE TO STORE ALL THE CONFIGURATION VARIABLES'''\n",
        "\n",
        "#pretrained is a boolean that indicates if a pretrained model will be loaded\n",
        "pretrained = False # Set to True if you want to load a pretrained model\n",
        "\n",
        "#check_freq is the frequency at which the callback is called, in this case, the callback is called every 2000 timesteps\n",
        "check_freq = 2000\n",
        "\n",
        "#save_path is the path where the best model will be saved\n",
        "save_path = \"./breakout_ppo_1M_save_path\"\n",
        "\n",
        "#log_dir is the path where the logs will be saved\n",
        "log_dir = \"./log_dir\"\n",
        "\n",
        "\n",
        "'''\n",
        "Hyperparameters of the model {learning_rate, gamma, device, n_steps, gae_lambda, ent_coef, vf_coef, max_grad_norm, rms_prop_eps, use_rms_prop, use_sde, sde_sample_freq, normalize_advantage}\n",
        "'''\n",
        "#policy is the policy of the model, in this case, the model will use a convolutional neural network\n",
        "policy = \"CnnPolicy\"\n",
        "\n",
        "#learning_rate is the learning rate of the model\n",
        "learning_rate =5e-4  #first trial: 5e-4   #second trial: 1e-4  #third trial: 1e-3  #fourth trial: 5e-5 #fifth trial: 5e-5 gamma = 0.90 #sixth trial: 1e-4 gamma = 0.90 #seventh trial: 5e-4 gamma = 0.90\n",
        "\n",
        "#gamma is the discount factor\n",
        "gamma = 0.99\n",
        "\n",
        "#device is the device where the model will be trained, if cuda is available, the model will be trained in the gpu, otherwise, it will be trained in the cpu\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "#n_steps is the number of steps taken by the model before updating the parameters\n",
        "n_steps = 24\n",
        "\n",
        "#batch_size is the number of samples used in each update\n",
        "batch_size = 96\n",
        "\n",
        "#n_epochs is the number of epochs when optimizing the surrogate loss\n",
        "n_epochs = 6\n",
        "\n",
        "#gae_lambda is the lambda parameter of the generalized advantage estimation, set to 1 to disable it\n",
        "gae_lambda = 0.95\n",
        "\n",
        "#clip_range is the clipping parameter of the surrogate loss\n",
        "clip_range = 0.2\n",
        "\n",
        "#clip_range_vf is the clipping parameter of the value function\n",
        "clip_range_vf = 1\n",
        "\n",
        "#ent_coef is the entropy coefficient, set to 0 to disable it\n",
        "ent_coef = 0.01\n",
        "\n",
        "#vf_coef is the value function coefficient, If we set it to 0.5, then the value function loss will be half the policy loss\n",
        "vf_coef = 0.5\n",
        "\n",
        "#max_grad_norm is the maximum value for the gradient clipping\n",
        "max_grad_norm = 0.5\n",
        "\n",
        "#use_sde is a boolean that indicates if the stochastic differential equation will be used\n",
        "#The stochastic differential equation is a method to add noise to the actions taken by the agent to improve exploration\n",
        "use_sde = False\n",
        "\n",
        "#sde_sample_freq is the frequency at which the noise is added to the actions. If set to -1, the noise will be added every timestep\n",
        "sde_sample_freq = -1\n",
        "\n",
        "#rollout_buffer_class is the class of the rollout buffer, in this case, the model will use the RolloutBuffer class\n",
        "rollout_buffer_class = None\n",
        "\n",
        "#rollout_buffer_kwargs is a dictionary with the keyword arguments for the rollout buffer. If None, it will use the default arguments\n",
        "rollout_buffer_kwargs = None\n",
        "\n",
        "#target_kl is the target value for the KL divergence between the old and updated policy\n",
        "target_kl = 0.5\n",
        "\n",
        "#normalize_advantage is a boolean that indicates if the advantage will be normalized, by normalizing the advantage,\n",
        "# the variance of the advantage is reduced, this is done to improve the training process because the advantage is used to calculate the policy loss\n",
        "normalize_advantage = False\n",
        "\n",
        "#stats_window_size is the size of the window used to calculate the mean and standard deviation of the advantage\n",
        "stats_window_size = 100\n",
        "\n",
        "#tensorboard_log is the path where the tensorboard logs will be saved, in our case, the logs will be saved in the log_dir\n",
        "tensorboard_log = log_dir\n",
        "\n",
        "#policy_kwargs is a dictionary with the keyword arguments for the policy. If None, it will use the default arguments\n",
        "policy_kwargs = None\n",
        "\n",
        "#verbose is the verbosity level: 0 no output, 1 info, 2 debug\n",
        "verbose = 2\n",
        "\n",
        "#seed is the seed for the pseudo random number generator used by the model. It is set to None to use a random seed,\n",
        "# and set to 0 to use a fixed seed for reproducibility\n",
        "seed = None\n",
        "\n",
        "#_init_setup_model is a boolean that indicates if the model will be initialized after being created, set to True to initialize the model\n",
        "_init_setup_model = True\n",
        "\n",
        "#total_timesteps is the total number of timesteps that the model will be trained. In this case, the model will be trained for 1e7 timesteps\n",
        "#Take into account that the number of timesteps is not the number of episodes, in a game like breakout, the agent takes an action every frame,\n",
        "# then the number of timesteps is the number of frames, which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e7 timesteps is 1000 games more or less.\n",
        "total_timesteps = int(3e7)\n",
        "\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "log_interval = 100\n",
        "\n",
        "'''\n",
        "Saved model path\n",
        "'''\n",
        "\n",
        "#for the path to be shorter just put \"./a2c_Breakout_1M.zip\" instead of the full path\n",
        "saved_model_path = \"./PPO_Breakout_30M_lr_5e-4_gamma_90.zip\"\n",
        "unzip_file_path =  \"./PPO_Breakout_30M_lr_5e-4_gamma_90_unzipped\"\n",
        "\n",
        "'''\n",
        "Environment variables\n",
        "'''\n",
        "#n_stack is the number of frames stacked together to form the input to the model\n",
        "n_stack = 4\n",
        "#n_envs is the number of environments that will be run in parallel\n",
        "n_envs = 4\n",
        "\n",
        "'''\n",
        "Wandb configuration\n",
        "'''\n",
        "#log_to_wandb is a boolean that indicates if the training process will be logged to wandb\n",
        "log_to_wandb = False\n",
        "\n",
        "# project is the name of the project in wandb\n",
        "project_train = \"BREAKOUT_SB3_BENCHMARK\"\n",
        "project_test = \"breakout-PPO2-test\"\n",
        "\n",
        "#entity is the name of the team in wandb\n",
        "entity = \"ai42\"\n",
        "\n",
        "#name is the name of the run in wandb\n",
        "name_train = \"PPO_breakout_lr_5e-4_gamma_90\"\n",
        "name_test = \"PPO2_breakout_test\"\n",
        "#notes is a description of the run\n",
        "notes = \"PPO2_breakout with parameters: {}\".format(locals()) #locals() returns a dictionary with all the local variables, in this case, all the variables in this file\n",
        "#sync_tensorboard is a boolean that indicates if the tensorboard logs will be synced to wandb\n",
        "sync_tensorboard = True\n",
        "\n",
        "\n",
        "'''\n",
        "Test configuration\n",
        "'''\n",
        "test_episodes = 100\n"
      ],
      "metadata": {
        "id": "juQ0xTxiaV31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/utils.py"
      ],
      "metadata": {
        "id": "ZTwIv4jvas7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "import os\n",
        "import zipfile\n",
        "from stable_baselines3.common.callbacks import BaseCallback\n",
        "import numpy as np\n",
        "import wandb\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "\n",
        "'''\n",
        "The RewardLogger wrapper is used to log the rewards of each episode to wandb\n",
        "It makes sure that the rewards of each episode are stored in a list and that the current episode reward is reset\n",
        "'''\n",
        "class RewardLogger(gym.Wrapper):\n",
        "    def __init__(self, env):\n",
        "        super(RewardLogger, self).__init__(env)\n",
        "        # Store the rewards of each episode\n",
        "        self.episode_rewards = []\n",
        "        # Store the current episode reward\n",
        "        self.current_episode_reward = 0\n",
        "\n",
        "    # The step function is called every time the agent takes an action in the environment\n",
        "    def step(self, action):\n",
        "        # Call the step function of the environment and store the results\n",
        "        obs, reward, done, truncated, info = self.env.step(action)\n",
        "        # Update the current episode reward\n",
        "        self.current_episode_reward += reward\n",
        "        # If the episode is done, store the episode reward and reset the current episode reward\n",
        "        if done:\n",
        "            self.episode_rewards.append(self.current_episode_reward)\n",
        "            self.current_episode_reward = 0\n",
        "        # Return the results as in a normal step function\n",
        "        return obs, reward, done, truncated, info\n",
        "\n",
        "    # The reset function is called every time the environment is reset (at the beginning of each episode)\n",
        "    def reset(self, **kwargs):\n",
        "        return self.env.reset(**kwargs)\n",
        "\n",
        "    # The get_episode_rewards function returns the rewards of each episode\n",
        "    def get_episode_rewards(self):\n",
        "        return self.episode_rewards\n",
        "\n",
        "'''\n",
        "The CustomWandbCallback is a callback* that logs the mean reward of the last 100 episodes to wandb.\n",
        "A callback is a function that is called at the end of each episode to perform some action,\n",
        "in this case, the action is logging the mean reward of the last 100 episodes to wandb.\n",
        "'''\n",
        "class CustomWandbCallback(BaseCallback):\n",
        "    def __init__(self, check_freq, save_path, verbose=1):\n",
        "        super(CustomWandbCallback, self).__init__(verbose)\n",
        "        # Define the frequency at which the callback is called\n",
        "        self.check_freq = check_freq\n",
        "        # Define the path where the best model will be saved\n",
        "        self.save_path = save_path\n",
        "        # Define the best mean reward as -inf\n",
        "        self.best_mean_reward = -np.inf\n",
        "\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        '''\n",
        "        The _on_step function is called at the end of each episode.\n",
        "        It returns True if the callback should be called again, and False otherwise.\n",
        "        To do this, it checks if the number of calls to the callback is a multiple of the check_freq.\n",
        "        If it is, it computes the mean reward of the last 100 episodes and logs it to wandb.\n",
        "        It also saves the model if the mean reward is greater than the best mean reward.\n",
        "        '''\n",
        "        # Check if the number of calls to the callback is a multiple of the check_freq\n",
        "        if self.n_calls % self.check_freq == 0:\n",
        "            # Gather rewards from all environments, by all environments we mean all the environments in the vectorized environment, usually there is only 1 environment in the vectorized environment\n",
        "            all_rewards = []\n",
        "            for env in self.training_env.envs: # self.training_env is the vectorized environment\n",
        "                # logger_env is the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "                logger_env = env.envs[0] if isinstance(env, DummyVecEnv) else env # env.envs[0] is the AtariWrapper which wraps the environment correctly\n",
        "                #Check if the logger_env is the RewardLogger wrapper\n",
        "                if isinstance(logger_env, RewardLogger):\n",
        "                    # If it is, get the rewards of each episode and store them in all_rewards\n",
        "                    all_rewards.extend(logger_env.get_episode_rewards())#extend is used to add the elements of a list to another list\n",
        "\n",
        "            #If there are rewards in all_rewards, compute the mean reward of the last 100 episodes and log it to wandb\n",
        "            if all_rewards:\n",
        "                # Compute the mean reward of the last 100 episodes\n",
        "                mean_reward = np.mean(all_rewards[-self.check_freq:])\n",
        "                # Log the mean reward of the last 100 episodes to wandb\n",
        "                wandb.log({'mean_reward': mean_reward, 'steps': self.num_timesteps})\n",
        "\n",
        "                # Save the best model\n",
        "                if mean_reward > self.best_mean_reward:\n",
        "                    self.best_mean_reward = mean_reward\n",
        "                    self.model.save(os.path.join(self.save_path, 'best_model'))\n",
        "        # Return True if the callback should be called again, and False otherwise\n",
        "        return True\n",
        "\n",
        "\n",
        "def make_env(env_id, seed=0):\n",
        "    '''\n",
        "    Function for creating the environment with the correct wrappers and rendering.\n",
        "    '''\n",
        "    def _init():\n",
        "        # Create the environment with render mode set to human\n",
        "        env = gym.make(env_id, render_mode='human')\n",
        "        # Set the seed of the environment, this is done to make the results reproducible\n",
        "        env.seed(seed)\n",
        "        # Wrap the environment with the AtariWrapper which wraps the environment correctly\n",
        "        env = AtariWrapper(env)\n",
        "        # Wrap the environment with the RewardLogger wrapper which logs the rewards of each episode to wandb\n",
        "        env = RewardLogger(env)\n",
        "        # Return the environment\n",
        "        return env\n",
        "    # Return the _init function which is used to create the environment\n",
        "    return _init\n",
        "\n",
        "\n",
        "def unzip_file(zip_path, extract_to_folder):\n",
        "    \"\"\"\n",
        "    Unzips a zip file to a specified folder.\n",
        "\n",
        "    Args:\n",
        "    zip_path (str): The path to the zip file.\n",
        "    extract_to_folder (str): The folder to extract the files to.\n",
        "    \"\"\"\n",
        "    # Ensure the target folder exists\n",
        "    os.makedirs(extract_to_folder, exist_ok=True)\n",
        "    # Extract the zip file to the target folder\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_folder)"
      ],
      "metadata": {
        "id": "-6w9HUsHayBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/train_PPO.py"
      ],
      "metadata": {
        "id": "e4gIYfc0blEu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_util import make_atari_env\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "import gymnasium as gym\n",
        "import torch\n",
        "import config\n",
        "import wandb\n",
        "from wandb.integration.sb3 import WandbCallback\n",
        "from utils import make_env, unzip_file, CustomWandbCallback, RewardLogger\n",
        "import os\n",
        "from stable_baselines3.common.utils import get_latest_run_id\n",
        "\n",
        "'''\n",
        "Set up the appropriate directories for logging and saving the model\n",
        "'''\n",
        "os.makedirs(config.log_dir, exist_ok=True)\n",
        "os.makedirs(config.save_path, exist_ok=True)\n",
        "\n",
        "#Create the callback that logs the mean reward of the last 100 episodes to wandb\n",
        "custom_callback = CustomWandbCallback(config.check_freq, config.save_path)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up loging to wandb\n",
        "'''\n",
        "\n",
        "#Set wandb to log the training process\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.project_train, entity = config.entity, name=config.name_train, notes=config.notes, sync_tensorboard=config.sync_tensorboard)\n",
        "    #wandb_callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "    wandb_callback = WandbCallback()\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment\n",
        "'''\n",
        "# Create multiple environments and wrap them correctly\n",
        "env = make_atari_env(\"BreakoutNoFrameskip-v4\", n_envs=config.n_envs, seed=config.seed)\n",
        "env = VecFrameStack(env, n_stack=config.n_stack)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the model\n",
        "'''\n",
        "#Create the model with the parameters specified in config.py, go to config.py to see the meaning of each parameter in detail\n",
        "model = PPO(policy=config.policy\n",
        "            ,env=env\n",
        "            ,learning_rate=config.learning_rate\n",
        "            ,n_steps=config.n_steps\n",
        "            ,batch_size=config.batch_size\n",
        "            ,n_epochs=config.n_epochs\n",
        "            ,gamma=config.gamma\n",
        "            ,gae_lambda=config.gae_lambda\n",
        "            ,clip_range=config.clip_range\n",
        "            ,clip_range_vf=config.clip_range_vf\n",
        "            ,normalize_advantage=config.normalize_advantage\n",
        "            ,ent_coef=config.ent_coef\n",
        "            ,vf_coef=config.vf_coef\n",
        "            ,max_grad_norm=config.max_grad_norm\n",
        "            ,use_sde=config.use_sde\n",
        "            ,sde_sample_freq=config.sde_sample_freq\n",
        "            #,rollout_buffer_class=config.rollout_buffer_class\n",
        "            #,rollout_buffer_kwargs=config.rollout_buffer_kwargs\n",
        "            ,target_kl=config.target_kl\n",
        "            ,stats_window_size=config.stats_window_size\n",
        "            ,tensorboard_log=config.log_dir\n",
        "            ,policy_kwargs=config.policy_kwargs\n",
        "            ,verbose=config.verbose\n",
        "            ,seed=config.seed\n",
        "            ,device=config.device\n",
        "            ,_init_setup_model=config._init_setup_model\n",
        "            )\n",
        "\n",
        "print(\"model in device: \", model.device)\n",
        "\n",
        "#Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    model = PPO.load(config.saved_model_path, env=env, verbose=config.verbose, tensorboard_log=config.log_dir)\n",
        "    #Unzip the file a2c_Breakout_1M.zip and store the unzipped files in the folder a2c_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "Train the model and save it\n",
        "'''\n",
        "#model.learn will train the model for 1e6 timesteps, timestep is the number of actions taken by the agent,\n",
        "# in a game like breakout, the agent takes an action every frame, then the number of timesteps is the number of frames,\n",
        "# which is the number of frames in 1 game multiplied by the number of games played.\n",
        "#The average number of frames in 1 game is 1000, so 1e6 timesteps is 1000 games more or less.\n",
        "#log_interval is the number of timesteps between each log, in this case, the training process will be logged every 100 timesteps.\n",
        "#callback is a callback that logs the training process to wandb, this is done because wandb.watch() does not work with sb3\n",
        "\n",
        "if config.log_to_wandb:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=[wandb_callback, custom_callback], progress_bar=True)\n",
        "else:\n",
        "    model.learn(total_timesteps=config.total_timesteps, log_interval=config.log_interval, callback=custom_callback, progress_bar=True)\n",
        "#Save the model\n",
        "model.save(config.saved_model_path[:-4]) #remove the .zip extension from the path\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "ut6JSCELb16q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/test_PPO.py"
      ],
      "metadata": {
        "id": "ct3akWENcH2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.vec_env import VecFrameStack, DummyVecEnv\n",
        "import torch\n",
        "import wandb\n",
        "import os\n",
        "import tensorboard as tb\n",
        "import config\n",
        "from utils import make_env, unzip_file\n",
        "\n",
        "\n",
        "'''\n",
        "Set up wandb\n",
        "'''\n",
        "if config.log_to_wandb:\n",
        "    wandb.init(project=config.name_test, entity= config.entity, sync_tensorboard=config.sync_tensorboard, name=config.name_test, notes=config.notes)\n",
        "\n",
        "\n",
        "'''\n",
        "Set up the environment and the model to test\n",
        "'''\n",
        "# Unzip the saved model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    #Unzip the file PPO_Breakout_1M.zip and store the unzipped files in the folder PPO_Breakout_unzipped\n",
        "    unzip_file(config.saved_model_path, config.unzip_file_path)\n",
        "\n",
        "#We start with a single environment for Breakout with render mode set to human\n",
        "env = make_env(\"BreakoutNoFrameskip-v4\")\n",
        "#We then wrap the environment with the DummyVecEnv wrapper which converts the environment to a single vectorized environment\n",
        "env = DummyVecEnv([env]) # Output shape: (1, 84, 84)\n",
        "#Finally, we wrap the environment with the VecFrameStack wrapper which stacks the observations over the last 4 frames\n",
        "env = VecFrameStack(env, n_stack=config.n_stack) # Output shape: (4, 84, 84)\n",
        "\n",
        "# Create the model\n",
        "model = PPO(policy = config.policy\n",
        "            ,env = env\n",
        "            ,verbose = config.verbose)\n",
        "\n",
        "# Load the model if config.pretrained is set to True in config.py\n",
        "if config.pretrained:\n",
        "    # Load the model components, including the policy network and the value network\n",
        "    model.policy.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.pth\")))\n",
        "    model.policy.optimizer.load_state_dict(torch.load(os.path.join(config.unzip_file_path, \"policy.optimizer.pth\")))\n",
        "\n",
        "\n",
        "'''\n",
        "Test the model in the environment and log the results to wandb\n",
        "'''\n",
        "# Run the episodes and render the gameplay\n",
        "for episode in range(config.test_episodes):\n",
        "    # Reset the environment and stack the initial state 4 times\n",
        "    obs = env.reset()#obs = np.stack([obs] * 4, axis=0)  # Initial state stack\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        # Take an action in the environment according to the policy of the trained agent\n",
        "        action, _ = model.predict(obs)\n",
        "        # Take the action in the environment and store the results in the variables\n",
        "        obs, reward, done, info = env.step(action) # obs shape: (1, 84, 84), reward shape: (1,), done shape: (1,), info shape: (1,)\n",
        "        # Update the total reward\n",
        "        episode_reward += reward[0]\n",
        "        # Render the environment to visualize the gameplay of the trained agent\n",
        "        env.render()\n",
        "    if config.log_to_wandb:\n",
        "        # Log the total reward of the episode to wandb\n",
        "        wandb.log({'test_episode_reward': episode_reward, 'test_episode': episode})\n",
        "\n",
        "\n",
        "'''\n",
        "Close the environment and finish the logging\n",
        "'''\n",
        "env.close()\n",
        "if config.log_to_wandb:\n",
        "    wandb.finish()"
      ],
      "metadata": {
        "id": "bUPhD7k_cW9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tqiXwrVscvfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut_sb3_PPO/__pycache__"
      ],
      "metadata": {
        "id": "QWV1DXRWcwRy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
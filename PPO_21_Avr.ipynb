{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPa+MWqTT2HwfhIUq03UH1i",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ezzeddinegasmi/DRL_comparative_study/blob/main/PPO_21_Avr.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60uaAvEOEm3H",
        "outputId": "1d5d45d5-ccc4-40bb-d879-a9599095c8ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copMmhVRffM4"
      },
      "source": [
        "## Configuration for Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl_x5UTeffM5",
        "outputId": "a8367ff1-2efe-4f43-9038-4b84fa4adc31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium==1.0.0 in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (4.13.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium==1.0.0) (0.0.4)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "\n",
        "IN_COLAB = \"google.colab\" in sys.modules\n",
        "\n",
        "if IN_COLAB:\n",
        "    !pip install gymnasium==1.0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKgQ5FvjffM6"
      },
      "source": [
        "# 02. PPO\n",
        "\n",
        "- PPO: [J. Schulman et al., \"Proximal Policy Optimization Algorithms.\" arXiv preprint arXiv:1707.06347, 2017.](https://arxiv.org/abs/1707.06347.pdf)\n",
        "- TRPO: [Schulman, John, et al. \"Trust region policy optimization.\" International conference on machine learning. 2015.](http://proceedings.mlr.press/v37/schulman15.pdf)\n",
        "\n",
        "There are two kinds of algorithms of PPO: PPO-Penalty and PPO-Clip. Here, we'll implement PPO-clip version.\n",
        "\n",
        "TRPO computes the gradients with a complex second-order method. On the other hand, PPO tries to solve the problem with a first-order methods that keep new policies close to old. To simplify the surrogate objective, let $r(\\theta)$ denote the probability ratio\n",
        "\n",
        "$$ L^{CPI}(\\theta) = \\hat {\\mathbb{E}}_t \\left [ {\\pi_\\theta(a_t|s_t) \\over \\pi_{\\theta_{old}}(a_t|s_t)} \\hat A_t\\right] = \\hat {\\mathbb{E}}_t \\left [ r_t(\\theta) \\hat A_t \\right ].$$\n",
        "\n",
        "The objective is penalized further away from $r_t(\\theta)$\n",
        "\n",
        "$$ L^{CLIP}(\\theta)=\\hat {\\mathbb{E}}_t \\left [ \\min(r_t(\\theta) \\hat A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat A_t) \\right ] $$\n",
        "\n",
        "If the advantage is positive, the objective will increase. As a result, the action becomes more likely. If advantage is negative, the objective will decrease. AS a result, the action becomes less likely."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "humfBm2yffM6"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "r6YSg5W-ffM6"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from collections import deque\n",
        "from typing import Deque, Dict, List, Tuple\n",
        "\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from IPython.display import clear_output\n",
        "from torch.distributions import Normal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "gsU7wcjhs3RJ",
        "outputId": "5bddea61-a265-4fd7-9e56-6dce32c057b1"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-108-6917a7122764>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-108-6917a7122764>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    git clone https-github.com-Neilus03-DRL_comparative_study-BreakOut_sb3\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "git clone https-github.com-Neilus03-DRL_comparative_study-BreakOut_sb3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "BreakOut sb3 PPO"
      ],
      "metadata": {
        "id": "Iq4UkOI5_Es0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "config.py"
      ],
      "metadata": {
        "id": "p8woq9oR7no6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiTRVsblffM7"
      },
      "source": [
        "## PPO Agent\n",
        "Here is a summary of PPOAgent class.\n",
        "\n",
        "| Method           | Note                                                 |\n",
        "|---               |---                                                   |\n",
        "|select_action     | select an action from the input state.               |\n",
        "|step              | take an action and return the response of the env.   |\n",
        "|update_model      | update the model by gradient descent.                |\n",
        "|train             | train the agent during num_frames.                   |\n",
        "|test              | test the agent (1 episode).                          |\n",
        "|_plot             | plot the training progresses.                        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNvD7ppH9Nnf"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "IrYbor8-ffM8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "cfd5e1ca-a412-4130-8704-8b02f3d39146"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'ActionNormalizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-109-afaae349b558>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pendulum-v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mActionNormalizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'ActionNormalizer' is not defined"
          ]
        }
      ],
      "source": [
        "# environment\n",
        "env = gym.make(\"Pendulum-v1\", render_mode=\"rgb_array\")\n",
        "env = ActionNormalizer(env)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvHO1op6ffM8"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from modular_rl import *\n",
        "\n",
        "class PpoLbfgsUpdater(EzFlat, EzPickle):\n",
        "\n",
        "    options = [\n",
        "        (\"kl_target\", float, 1e-2, \"Desired KL divergence between old and new policy\"),\n",
        "        (\"maxiter\", int, 25, \"Maximum number of iterations\"),\n",
        "        (\"reverse_kl\", int, 0, \"kl[new, old] instead of kl[old, new]\"),\n",
        "        (\"do_split\", int, 0, \"Do train/test split on batches\")\n",
        "    ]\n",
        "\n",
        "    def __init__(self, stochpol, usercfg):\n",
        "        EzPickle.__init__(self, stochpol, usercfg)\n",
        "        cfg = update_default_config(self.options, usercfg)\n",
        "        print \"PPOUpdater\", cfg\n",
        "\n",
        "        self.stochpol = stochpol\n",
        "        self.cfg = cfg\n",
        "        self.kl_coeff = 1.0\n",
        "        kl_cutoff = cfg[\"kl_target\"]*2.0\n",
        "\n",
        "        probtype = stochpol.probtype\n",
        "        params = stochpol.trainable_variables\n",
        "        EzFlat.__init__(self, params)\n",
        "\n",
        "        ob_no = stochpol.input\n",
        "        act_na = probtype.sampled_variable()\n",
        "        adv_n = T.vector(\"adv_n\")\n",
        "        kl_coeff = T.scalar(\"kl_coeff\")\n",
        "\n",
        "        # Probability distribution:\n",
        "        prob_np = stochpol.get_output()\n",
        "        oldprob_np = probtype.prob_variable()\n",
        "\n",
        "        p_n = probtype.likelihood(act_na, prob_np)\n",
        "        oldp_n = probtype.likelihood(act_na, oldprob_np)\n",
        "        N = ob_no.shape[0]\n",
        "\n",
        "        ent = probtype.entropy(prob_np).mean()\n",
        "        if cfg[\"reverse_kl\"]:\n",
        "            kl = probtype.kl(prob_np, oldprob_np).mean()\n",
        "        else:\n",
        "            kl = probtype.kl(oldprob_np, prob_np).mean()\n",
        "\n",
        "\n",
        "        # Policy gradient:\n",
        "        surr = (-1.0 / N) * (p_n / oldp_n).dot(adv_n)\n",
        "        pensurr = surr + kl_coeff*kl + 1000*(kl>kl_cutoff)*T.square(kl-kl_cutoff)\n",
        "        g = flatgrad(pensurr, params)\n",
        "\n",
        "        losses = [surr, kl, ent]\n",
        "        self.loss_names = [\"surr\", \"kl\", \"ent\"]\n",
        "\n",
        "        args = [ob_no, act_na, adv_n, oldprob_np]\n",
        "\n",
        "        self.compute_lossgrad = theano.function([kl_coeff] + args, [pensurr, g], **FNOPTS)\n",
        "        self.compute_losses = theano.function(args, losses, **FNOPTS)\n",
        "\n",
        "    def __call__(self, paths):\n",
        "        cfg = self.cfg\n",
        "        prob_np = concat([path[\"prob\"] for path in paths])\n",
        "        ob_no = concat([path[\"observation\"] for path in paths])\n",
        "        action_na = concat([path[\"action\"] for path in paths])\n",
        "        advantage_n = concat([path[\"advantage\"] for path in paths])\n",
        "\n",
        "        N = ob_no.shape[0]\n",
        "        train_stop = int(0.75 * N) if cfg[\"do_split\"] else N\n",
        "        train_sli = slice(0, train_stop)\n",
        "        test_sli = slice(train_stop, None)\n",
        "\n",
        "        train_args = (ob_no[train_sli], action_na[train_sli], advantage_n[train_sli], prob_np[train_sli])\n",
        "\n",
        "        thprev = self.get_params_flat()\n",
        "        def lossandgrad(th):\n",
        "            self.set_params_flat(th)\n",
        "            l,g = self.compute_lossgrad(self.kl_coeff, *train_args)\n",
        "            g = g.astype('float64')\n",
        "            return (l,g)\n",
        "\n",
        "        train_losses_before = self.compute_losses(*train_args)\n",
        "        if cfg[\"do_split\"]:\n",
        "            test_args = (ob_no[test_sli], action_na[test_sli], advantage_n[test_sli], prob_np[test_sli])\n",
        "            test_losses_before = self.compute_losses(*test_args)\n",
        "\n",
        "        theta, _, opt_info = scipy.optimize.fmin_l_bfgs_b(lossandgrad, thprev, maxiter=cfg[\"maxiter\"])\n",
        "        del opt_info['grad']\n",
        "        print opt_info\n",
        "        self.set_params_flat(theta)\n",
        "        train_losses_after = self.compute_losses(*train_args)\n",
        "        if cfg[\"do_split\"]:\n",
        "            test_losses_after = self.compute_losses(*test_args)\n",
        "        klafter = train_losses_after[self.loss_names.index(\"kl\")]\n",
        "        if klafter > 1.3*self.cfg[\"kl_target\"]:\n",
        "            self.kl_coeff *= 1.5\n",
        "            print \"Got KL=%.3f (target %.3f). Increasing penalty coeff => %.3f.\"%(klafter, self.cfg[\"kl_target\"], self.kl_coeff)\n",
        "        elif klafter < 0.7*self.cfg[\"kl_target\"]:\n",
        "            self.kl_coeff /= 1.5\n",
        "            print \"Got KL=%.3f (target %.3f). Decreasing penalty coeff => %.3f.\"%(klafter, self.cfg[\"kl_target\"], self.kl_coeff)\n",
        "        else:\n",
        "            print \"KL=%.3f is close enough to target %.3f.\"%(klafter, self.cfg[\"kl_target\"])\n",
        "        info = OrderedDict()\n",
        "        for (name,lossbefore, lossafter) in zipsame(self.loss_names, train_losses_before, train_losses_after):\n",
        "            info[name+\"_before\"] = lossbefore\n",
        "            info[name+\"_after\"] = lossafter\n",
        "            info[name+\"_change\"] = lossafter - lossbefore\n",
        "        if cfg[\"do_split\"]:\n",
        "            for (name,lossbefore, lossafter) in zipsame(self.loss_names, test_losses_before, test_losses_after):\n",
        "                info[\"test_\"+name+\"_before\"] = lossbefore\n",
        "                info[\"test_\"+name+\"_after\"] = lossafter\n",
        "                info[\"test_\"+name+\"_change\"] = lossafter - lossbefore\n",
        "\n",
        "        return info\n",
        "\n",
        "\n",
        "class PpoSgdUpdater(EzPickle):\n",
        "\n",
        "    options = [\n",
        "        (\"kl_target\", float, 1e-2, \"\"),\n",
        "        (\"epochs\", int, 10, \"\"),\n",
        "        (\"stepsize\", float, 1e-3, \"\"),\n",
        "        (\"do_split\", int, 0, \"do train/test split\"),\n",
        "        (\"kl_cutoff_coeff\", float, 1000.0, \"\")\n",
        "    ]\n",
        "\n",
        "    def __init__(self, stochpol, usercfg):\n",
        "        EzPickle.__init__(self, stochpol, usercfg)\n",
        "        cfg = update_default_config(self.options, usercfg)\n",
        "        print \"PPOUpdater\", cfg\n",
        "\n",
        "        self.stochpol = stochpol\n",
        "        self.cfg = cfg\n",
        "        self.kl_coeff = 1.0\n",
        "        kl_cutoff = cfg[\"kl_target\"]*2.0\n",
        "\n",
        "        probtype = stochpol.probtype\n",
        "        params = stochpol.trainable_variables\n",
        "        old_params = [theano.shared(v.get_value()) for v in stochpol.trainable_variables]\n",
        "\n",
        "        ob_no = stochpol.input\n",
        "        act_na = probtype.sampled_variable()\n",
        "        adv_n = T.vector(\"adv_n\")\n",
        "        kl_coeff = T.scalar(\"kl_coeff\")\n",
        "\n",
        "        # Probability distribution:\n",
        "        self.loss_names = [\"surr\", \"kl\", \"ent\"]\n",
        "\n",
        "        prob_np = stochpol.get_output()\n",
        "        oldprob_np = theano.clone(stochpol.get_output(), replace=dict(zipsame(params, old_params)))\n",
        "        p_n = probtype.likelihood(act_na, prob_np)\n",
        "        oldp_n = probtype.likelihood(act_na, oldprob_np)\n",
        "        N = ob_no.shape[0]\n",
        "        ent = probtype.entropy(prob_np).mean()\n",
        "        kl = probtype.kl(oldprob_np, prob_np).mean()\n",
        "        # Policy gradient:\n",
        "        surr = (-1.0 / N) * (p_n / oldp_n).dot(adv_n)\n",
        "        train_losses = [surr, kl, ent]\n",
        "\n",
        "        # training\n",
        "        args = [ob_no, act_na, adv_n]\n",
        "        surr,kl = train_losses[:2]\n",
        "        pensurr = surr + kl_coeff*kl + cfg[\"kl_cutoff_coeff\"]*(kl>kl_cutoff)*T.square(kl-kl_cutoff)\n",
        "        self.train = theano.function([kl_coeff]+args, train_losses,\n",
        "            updates=stochpol.get_updates()\n",
        "            + adam_updates(pensurr, params, learning_rate=cfg.stepsize).items(), **FNOPTS)\n",
        "\n",
        "        self.test = theano.function(args, train_losses, **FNOPTS) # XXX\n",
        "        self.update_old_net = theano.function([], [], updates = zip(old_params, params))\n",
        "\n",
        "    def __call__(self, paths):\n",
        "        cfg = self.cfg\n",
        "        ob_no = concat([path[\"observation\"] for path in paths])\n",
        "        action_na = concat([path[\"action\"] for path in paths])\n",
        "        advantage_n = concat([path[\"advantage\"] for path in paths])\n",
        "        args = (ob_no, action_na, advantage_n)\n",
        "\n",
        "        N = args[0].shape[0]\n",
        "        batchsize = 128\n",
        "\n",
        "        self.update_old_net()\n",
        "\n",
        "        if cfg[\"do_split\"]:\n",
        "            train_stop = (int(.75*N)//batchsize) * batchsize\n",
        "            test_losses_before = self.test(*[arg[train_stop:] for arg in args])\n",
        "\n",
        "            print fmt_row(13, [\"epoch\"]\n",
        "                + self.loss_names\n",
        "                + [\"test_\" + name for name in self.loss_names])\n",
        "        else:\n",
        "            train_stop = N\n",
        "            print fmt_row(13, [\"epoch\"]\n",
        "                + self.loss_names)\n",
        "        train_losses_before = self.test(*[arg[:train_stop] for arg in args])\n",
        "\n",
        "        for iepoch in xrange(cfg[\"epochs\"]):\n",
        "            sortinds = np.random.permutation(train_stop)\n",
        "\n",
        "            losses = []\n",
        "            for istart in xrange(0, train_stop, batchsize):\n",
        "                losses.append(  self.train(self.kl_coeff, *[arg[sortinds[istart:istart+batchsize]] for arg in args])  )\n",
        "            train_losses = np.mean(losses, axis=0)\n",
        "            if cfg.do_split:\n",
        "                test_losses = self.test(*[arg[train_stop:] for arg in args])\n",
        "                print fmt_row(13, np.concatenate([[iepoch], train_losses, test_losses]))\n",
        "            else:\n",
        "                print fmt_row(13, np.concatenate([[iepoch], train_losses]))\n",
        "\n",
        "\n",
        "        klafter = train_losses[self.loss_names.index('kl')]\n",
        "        if klafter > 1.3*self.cfg[\"kl_target\"]:\n",
        "            self.kl_coeff *= 1.5\n",
        "            print \"Got KL=%.3f (target %.3f). Increasing penalty coeff => %.3f.\"%(klafter, self.cfg[\"kl_target\"], self.kl_coeff)\n",
        "        elif klafter < 0.7*self.cfg[\"kl_target\"]:\n",
        "            self.kl_coeff /= 1.5\n",
        "            print \"Got KL=%.3f (target %.3f). Decreasing penalty coeff => %.3f.\"%(klafter, self.cfg[\"kl_target\"], self.kl_coeff)\n",
        "        else:\n",
        "            print \"KL=%.3f is close enough to target %.3f.\"%(klafter, self.cfg[\"kl_target\"])\n",
        "\n",
        "        info = {}\n",
        "        for (name,lossbefore, lossafter) in zipsame(self.loss_names, train_losses_before, train_losses):\n",
        "            info[name+\"_before\"] = lossbefore\n",
        "            info[name+\"_after\"] = lossafter\n",
        "            info[name+\"_change\"] = lossafter - lossbefore\n",
        "        if cfg[\"do_split\"]:\n",
        "            for (name,lossbefore, lossafter) in zipsame(self.loss_names, test_losses_before, test_losses):\n",
        "                info[\"test_\"+name+\"_before\"] = lossbefore\n",
        "                info[\"test_\"+name+\"_after\"] = lossafter\n",
        "                info[\"test_\"+name+\"_change\"] = lossafter - lossbefore\n",
        "\n",
        "        return info\n",
        "\n",
        "\n",
        "def adam_updates(loss, params, learning_rate=0.001, beta1=0.9,\n",
        "         beta2=0.999, epsilon=1e-8):\n",
        "\n",
        "    all_grads = T.grad(loss, params)\n",
        "    t_prev = theano.shared(np.array(0,dtype=floatX))\n",
        "    updates = OrderedDict()\n",
        "\n",
        "    t = t_prev + 1\n",
        "    a_t = learning_rate*T.sqrt(1-beta2**t)/(1-beta1**t)\n",
        "\n",
        "    for param, g_t in zip(params, all_grads):\n",
        "        value = param.get_value(borrow=True)\n",
        "        m_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
        "                               broadcastable=param.broadcastable)\n",
        "        v_prev = theano.shared(np.zeros(value.shape, dtype=value.dtype),\n",
        "                               broadcastable=param.broadcastable)\n",
        "\n",
        "        m_t = beta1*m_prev + (1-beta1)*g_t\n",
        "        v_t = beta2*v_prev + (1-beta2)*g_t**2\n",
        "        step = a_t*m_t/(T.sqrt(v_t) + epsilon)\n",
        "\n",
        "        updates[m_prev] = m_t\n",
        "        updates[v_prev] = v_t\n",
        "        updates[param] = param - step\n",
        "\n",
        "    updates[t_prev] = t\n",
        "    return updates"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "l_IcwTsXFp9v",
        "outputId": "f10b82e9-908d-4d0a-874c-4fe2fa7f31b7"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "Missing parentheses in call to 'print'. Did you mean print(...)? (<ipython-input-112-6bf931aa6384>, line 15)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-112-6bf931aa6384>\"\u001b[0;36m, line \u001b[0;32m15\u001b[0m\n\u001b[0;31m    print \"PPOUpdater\", cfg\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(...)?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yyuh81I_ffM9"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "id": "JaGm_BeEffM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "a58e8fcb-0c36-480e-e3ef-1bfbf8a9e96f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-121-4326fee8443c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "agent.train(num_frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rc5lTEZoffM9"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "QW7EppwEffM9",
        "outputId": "ef48229a-3610-4dec-b683-bfef1c97917f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'agent' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-6b7f8f63c996>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mvideo_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"videos/ppo\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'agent' is not defined"
          ]
        }
      ],
      "source": [
        "# test\n",
        "video_folder = \"videos/ppo\"\n",
        "agent.test(video_folder=video_folder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXjioRpmffM9"
      },
      "source": [
        "## Render"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        },
        "id": "2eKCKKDmffM9",
        "outputId": "7ff9fe5f-d952-4ede-e155-4416717e0f65"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "max() arg is an empty sequence",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-127-5a0a30bb4cfb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mlatest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshow_latest_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Played:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-127-5a0a30bb4cfb>\u001b[0m in \u001b[0;36mshow_latest_video\u001b[0;34m(video_folder)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;34m\"\"\"Show the most recently recorded video from video folder.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mlist_of_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvideo_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"*.mp4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mlatest_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_of_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetctime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0mipython_show_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatest_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlatest_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
          ]
        }
      ],
      "source": [
        "import base64\n",
        "import glob\n",
        "import io\n",
        "import os\n",
        "\n",
        "from IPython.display import HTML, display\n",
        "\n",
        "\n",
        "def ipython_show_video(path: str) -> None:\n",
        "    \"\"\"Show a video at `path` within IPython Notebook.\"\"\"\n",
        "    if not os.path.isfile(path):\n",
        "        raise NameError(\"Cannot access: {}\".format(path))\n",
        "\n",
        "    video = io.open(path, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display(\n",
        "        HTML(\n",
        "            data=\"\"\"\n",
        "        <video width=\"320\" height=\"240\" alt=\"test\" controls>\n",
        "        <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\"/>\n",
        "        </video>\n",
        "        \"\"\".format(\n",
        "                encoded.decode(\"ascii\")\n",
        "            )\n",
        "        )\n",
        "    )\n",
        "\n",
        "\n",
        "def show_latest_video(video_folder: str) -> str:\n",
        "    \"\"\"Show the most recently recorded video from video folder.\"\"\"\n",
        "    list_of_files = glob.glob(os.path.join(video_folder, \"*.mp4\"))\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    ipython_show_video(latest_file)\n",
        "    return latest_file\n",
        "\n",
        "\n",
        "latest_file = show_latest_video(video_folder=video_folder)\n",
        "print(\"Played:\", latest_file)"
      ]
    }
  ]
}